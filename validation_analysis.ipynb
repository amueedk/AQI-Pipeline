{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AQI Validation & Source Reliability Analysis\n",
    "\n",
    "**Purpose**: Compare our calculated US AQI (from OpenWeather PM2.5/PM10) with IQAir sensor-based AQI measurements to assess data source reliability and calculation accuracy.\n",
    "\n",
    "**Analysis Structure**:\n",
    "- **Section 1**: Data Loading & Preparation\n",
    "- **Section 2**: Data Source Comparison Analysis  \n",
    "- **Section 3**: AQI Category Analysis\n",
    "- **Section 4**: Temporal Pattern Analysis\n",
    "- **Section 5**: Statistical Reliability Assessment\n",
    "- **Section 6**: Source Reliability Scoring\n",
    "- **Section 7**: Recommendations & Summary\n",
    "\n",
    "**Data Sources**:\n",
    "- **US AQI**: Calculated using EPA formula from OpenWeather PM2.5 & PM10 data\n",
    "- **IQAir AQI**: Direct sensor-based measurements from IQAir API\n",
    "- **OpenWeather AQI**: Categorical scale (1-5) from OpenWeather API\n",
    "\n",
    "**Note**: IQAir data is for validation/comparison only, NOT for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Validation Analysis Environment Setup Complete\n",
      "Analysis Date: 2025-07-17 22:39:56\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro, pearsonr, spearmanr\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üìä Validation Analysis Environment Setup Complete\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading & Preparation\n",
    "\n",
    "Load all validation CSV files and create a consolidated dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 343 validation files\n",
      "Date range: 20250701 to 20250716\n",
      "\n",
      "‚úÖ Successfully loaded 343 validation records\n",
      "üìÖ Time range: 2025-07-01 23:33:58+00:00 to 2025-07-16 22:10:59+00:00\n",
      "‚è±Ô∏è  Duration: 14 days\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 343 entries, 0 to 342\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype              \n",
      "---  ------           --------------  -----              \n",
      " 0   time             343 non-null    datetime64[ns, UTC]\n",
      " 1   openweather_aqi  343 non-null    int64              \n",
      " 2   us_aqi           337 non-null    float64            \n",
      " 3   iqair_aqi        343 non-null    int64              \n",
      " 4   abs_deviation    337 non-null    float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(2), int64(2)\n",
      "memory usage: 13.5 KB\n",
      "None\n",
      "\n",
      "üìä First few records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>openweather_aqi</th>\n",
       "      <th>us_aqi</th>\n",
       "      <th>iqair_aqi</th>\n",
       "      <th>abs_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-01 23:33:58+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>64</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-02 00:52:34+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>59</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-02 02:42:45+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>85</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-02 03:44:55+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-02 04:21:12+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>89</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  openweather_aqi  us_aqi  iqair_aqi  abs_deviation\n",
       "0 2025-07-01 23:33:58+00:00                4    97.0         64           33.0\n",
       "1 2025-07-02 00:52:34+00:00                4    96.0         59           37.0\n",
       "2 2025-07-02 02:42:45+00:00                4    93.0         85            8.0\n",
       "3 2025-07-02 03:44:55+00:00                4    92.0         92            0.0\n",
       "4 2025-07-02 04:21:12+00:00                4    92.0         89            3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all validation CSV files\n",
    "data_dir = 'data/'\n",
    "validation_files = glob.glob(os.path.join(data_dir, 'aqi_validation_current_*.csv'))\n",
    "\n",
    "print(f\"Found {len(validation_files)} validation files\")\n",
    "if validation_files:\n",
    "    print(f\"Date range: {min(validation_files).split('_')[-2]} to {max(validation_files).split('_')[-2]}\")\n",
    "\n",
    "# Load and concatenate all validation data\n",
    "validation_data = []\n",
    "failed_files = []\n",
    "\n",
    "for file in validation_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if not df.empty and len(df.columns) == 5:  # Ensure proper structure\n",
    "            validation_data.append(df)\n",
    "        else:\n",
    "            failed_files.append(file)\n",
    "    except Exception as e:\n",
    "        failed_files.append(file)\n",
    "        print(f\"Failed to load {file}: {e}\")\n",
    "\n",
    "# Combine all data\n",
    "if validation_data:\n",
    "    df_validation = pd.concat(validation_data, ignore_index=True)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df_validation['time'] = pd.to_datetime(df_validation['time'])\n",
    "    \n",
    "    # Sort by time\n",
    "    df_validation = df_validation.sort_values('time').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(df_validation)} validation records\")\n",
    "    print(f\"üìÖ Time range: {df_validation['time'].min()} to {df_validation['time'].max()}\")\n",
    "    print(f\"‚è±Ô∏è  Duration: {(df_validation['time'].max() - df_validation['time'].min()).days} days\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load {len(failed_files)} files\")\n",
    "else:\n",
    "    print(\"‚ùå No validation data loaded successfully\")\n",
    "    raise ValueError(\"Unable to load validation data\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(df_validation.info())\n",
    "print(\"\\nüìä First few records:\")\n",
    "df_validation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Data Quality Assessment:\n",
      "Total records: 343\n",
      "Missing values:\n",
      "time               0\n",
      "openweather_aqi    0\n",
      "us_aqi             6\n",
      "iqair_aqi          0\n",
      "abs_deviation      6\n",
      "dtype: int64\n",
      "\n",
      "Duplicates: 0\n",
      "\n",
      "Data types:\n",
      "time               datetime64[ns, UTC]\n",
      "openweather_aqi                  int64\n",
      "us_aqi                         float64\n",
      "iqair_aqi                        int64\n",
      "abs_deviation                  float64\n",
      "dtype: object\n",
      "\n",
      "üìà Descriptive Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>openweather_aqi</th>\n",
       "      <th>us_aqi</th>\n",
       "      <th>iqair_aqi</th>\n",
       "      <th>abs_deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>343.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.577259</td>\n",
       "      <td>107.311573</td>\n",
       "      <td>76.069971</td>\n",
       "      <td>36.011869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.528992</td>\n",
       "      <td>28.090007</td>\n",
       "      <td>21.947060</td>\n",
       "      <td>27.903637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>131.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       openweather_aqi      us_aqi   iqair_aqi  abs_deviation\n",
       "count       343.000000  337.000000  343.000000     337.000000\n",
       "mean          3.577259  107.311573   76.069971      36.011869\n",
       "std           0.528992   28.090007   21.947060      27.903637\n",
       "min           3.000000   58.000000   28.000000       0.000000\n",
       "25%           3.000000   83.000000   62.000000      13.000000\n",
       "50%           4.000000  106.000000   70.000000      29.000000\n",
       "75%           4.000000  129.000000   87.000000      54.000000\n",
       "max           5.000000  166.000000  160.000000     131.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data quality checks\n",
    "print(\"üîç Data Quality Assessment:\")\n",
    "print(f\"Total records: {len(df_validation)}\")\n",
    "print(f\"Missing values:\")\n",
    "print(df_validation.isnull().sum())\n",
    "print(f\"\\nDuplicates: {df_validation.duplicated().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_validation.dtypes)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìà Descriptive Statistics:\")\n",
    "df_validation.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning for analysis - remove rows with missing values\n",
    "print(\"üßπ Cleaning data for analysis...\")\n",
    "print(f\"Before cleaning: {len(df_validation)} records\")\n",
    "print(\"Missing values by column:\")\n",
    "print(df_validation.isnull().sum())\n",
    "\n",
    "# Create clean dataset without missing values\n",
    "df_clean = df_validation.dropna(subset=['us_aqi', 'iqair_aqi', 'openweather_aqi', 'abs_deviation']).copy()\n",
    "\n",
    "print(f\"\\\\nAfter cleaning: {len(df_clean)} records\")\n",
    "print(f\"Removed: {len(df_validation) - len(df_clean)} records ({((len(df_validation) - len(df_clean))/len(df_validation)*100):.1f}%)\")\n",
    "\n",
    "if len(df_clean) == 0:\n",
    "    raise ValueError(\"‚ùå No complete records available for analysis!\")\n",
    "else:\n",
    "    print(\"‚úÖ Clean dataset ready for analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Source Comparison Analysis\n",
    "\n",
    "Compare US AQI calculations with IQAir sensor measurements to understand differences and patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Correlation Analysis:\n",
      "US AQI vs IQAir AQI: 0.1489\n",
      "OpenWeather AQI vs IQAir AQI: 0.2514\n",
      "US AQI vs OpenWeather AQI: 0.5634\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS AQI vs OpenWeather AQI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation_us_ow\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate performance metrics for US AQI vs IQAir\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m mae_us_iqair \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_validation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miqair_aqi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_validation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mus_aqi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m rmse_us_iqair \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(df_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miqair_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m], df_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     14\u001b[0m r2_us_iqair \u001b[38;5;241m=\u001b[39m r2_score(df_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miqair_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m], df_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus_aqi\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:277\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m0.85...\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    274\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[0;32m    276\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 277\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m )\n\u001b[0;32m    282\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    284\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average(\n\u001b[0;32m    285\u001b[0m     xp\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m    286\u001b[0m )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:198\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mregression task.\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 198\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:106\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    105\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 106\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    109\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Calculate correlation metrics\n",
    "correlation_us_iqair = df_validation['us_aqi'].corr(df_validation['iqair_aqi'])\n",
    "#correlation_ow_iqair = df_validation['openweather_aqi'].corr(df_validation['iqair_aqi'])\n",
    "#correlation_us_ow = df_validation['us_aqi'].corr(df_validation['openweather_aqi'])\n",
    "\n",
    "print(\"üîó Correlation Analysis:\")\n",
    "print(f\"US AQI vs IQAir AQI: {correlation_us_iqair:.4f}\")\n",
    "#print(f\"OpenWeather AQI vs IQAir AQI: {correlation_ow_iqair:.4f}\")\n",
    "#print(f\"US AQI vs OpenWeather AQI: {correlation_us_ow:.4f}\")\n",
    "\n",
    "# Calculate performance metrics for US AQI vs IQAir\n",
    "mae_us_iqair = mean_absolute_error(df_validation['iqair_aqi'], df_validation['us_aqi'])\n",
    "rmse_us_iqair = np.sqrt(mean_squared_error(df_validation['iqair_aqi'], df_validation['us_aqi']))\n",
    "r2_us_iqair = r2_score(df_validation['iqair_aqi'], df_validation['us_aqi'])\n",
    "\n",
    "print(f\"\\nüìä US AQI vs IQAir Performance:\")\n",
    "print(f\"Mean Absolute Error: {mae_us_iqair:.2f}\")\n",
    "print(f\"Root Mean Square Error: {rmse_us_iqair:.2f}\")\n",
    "print(f\"R¬≤ Score: {r2_us_iqair:.4f}\")\n",
    "print(f\"Mean Absolute Deviation: {df_validation['abs_deviation'].mean():.2f}\")\n",
    "print(f\"Median Absolute Deviation: {df_validation['abs_deviation'].median():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('AQI Data Source Comparison Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Scatter plot: US AQI vs IQAir AQI\n",
    "axes[0,0].scatter(df_validation['iqair_aqi'], df_validation['us_aqi'], alpha=0.6, color='blue')\n",
    "max_val = max(df_validation['iqair_aqi'].max(), df_validation['us_aqi'].max())\n",
    "axes[0,0].plot([0, max_val], [0, max_val], 'r--', label='Perfect Agreement')\n",
    "axes[0,0].set_xlabel('IQAir AQI (Sensor)')\n",
    "axes[0,0].set_ylabel('US AQI (Calculated)')\n",
    "axes[0,0].set_title(f'US AQI vs IQAir AQI\\\\n(r = {correlation_us_iqair:.3f})')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Time series comparison\n",
    "axes[0,1].plot(df_validation['time'], df_validation['us_aqi'], label='US AQI', alpha=0.8)\n",
    "axes[0,1].plot(df_validation['time'], df_validation['iqair_aqi'], label='IQAir AQI', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Time')\n",
    "axes[0,1].set_ylabel('AQI Value')\n",
    "axes[0,1].set_title('AQI Time Series Comparison')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Deviation distribution\n",
    "axes[0,2].hist(df_validation['abs_deviation'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0,2].axvline(df_validation['abs_deviation'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: {df_validation[\"abs_deviation\"].mean():.1f}')\n",
    "axes[0,2].axvline(df_validation['abs_deviation'].median(), color='orange', linestyle='--', \n",
    "                  label=f'Median: {df_validation[\"abs_deviation\"].median():.1f}')\n",
    "axes[0,2].set_xlabel('Absolute Deviation')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "axes[0,2].set_title('Deviation Distribution')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Box plot comparison\n",
    "data_for_box = [df_validation['us_aqi'], df_validation['iqair_aqi']]\n",
    "axes[1,0].boxplot(data_for_box, labels=['US AQI', 'IQAir AQI'])\n",
    "axes[1,0].set_ylabel('AQI Value')\n",
    "axes[1,0].set_title('AQI Distribution Comparison')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Deviation over time\n",
    "axes[1,1].plot(df_validation['time'], df_validation['abs_deviation'], color='purple', alpha=0.7)\n",
    "axes[1,1].axhline(df_validation['abs_deviation'].mean(), color='red', linestyle='--', alpha=0.8)\n",
    "axes[1,1].set_xlabel('Time')\n",
    "axes[1,1].set_ylabel('Absolute Deviation')\n",
    "axes[1,1].set_title('Deviation Over Time')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. OpenWeather AQI vs others\n",
    "axes[1,2].scatter(df_validation['openweather_aqi'], df_validation['us_aqi'], \n",
    "                  alpha=0.6, color='orange', label='OW vs US AQI')\n",
    "axes[1,2].scatter(df_validation['openweather_aqi'], df_validation['iqair_aqi'], \n",
    "                  alpha=0.6, color='green', label='OW vs IQAir AQI')\n",
    "axes[1,2].set_xlabel('OpenWeather AQI (1-5 scale)')\n",
    "axes[1,2].set_ylabel('AQI Value')\n",
    "axes[1,2].set_title('OpenWeather AQI Comparison')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: AQI Category Analysis\n",
    "\n",
    "Analyze how well the different AQI measurements agree on air quality categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AQI categories\n",
    "def categorize_aqi(aqi_value):\n",
    "    \"\"\"Convert AQI value to category\"\"\"\n",
    "    if aqi_value <= 50:\n",
    "        return 'Good'\n",
    "    elif aqi_value <= 100:\n",
    "        return 'Moderate'\n",
    "    elif aqi_value <= 150:\n",
    "        return 'Unhealthy for Sensitive'\n",
    "    elif aqi_value <= 200:\n",
    "        return 'Unhealthy'\n",
    "    elif aqi_value <= 300:\n",
    "        return 'Very Unhealthy'\n",
    "    else:\n",
    "        return 'Hazardous'\n",
    "\n",
    "# Apply categorization\n",
    "df_validation['us_aqi_category'] = df_validation['us_aqi'].apply(categorize_aqi)\n",
    "df_validation['iqair_aqi_category'] = df_validation['iqair_aqi'].apply(categorize_aqi)\n",
    "\n",
    "print(\"üìä AQI Category Distribution:\")\n",
    "print(\"\\\\nUS AQI Categories:\")\n",
    "print(df_validation['us_aqi_category'].value_counts())\n",
    "print(\"\\\\nIQAir AQI Categories:\")\n",
    "print(df_validation['iqair_aqi_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category agreement analysis\n",
    "category_agreement = (df_validation['us_aqi_category'] == df_validation['iqair_aqi_category']).mean()\n",
    "print(f\"üéØ Category Agreement (US vs IQAir): {category_agreement:.1%}\")\n",
    "\n",
    "# Get all unique categories\n",
    "all_categories = sorted(set(df_validation['us_aqi_category'].unique()) | \n",
    "                       set(df_validation['iqair_aqi_category'].unique()))\n",
    "\n",
    "cm = confusion_matrix(df_validation['iqair_aqi_category'], df_validation['us_aqi_category'], \n",
    "                      labels=all_categories)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=all_categories, yticklabels=all_categories)\n",
    "plt.title('AQI Category Confusion Matrix\\\\n(IQAir vs US AQI)')\n",
    "plt.xlabel('US AQI Category (Predicted)')\n",
    "plt.ylabel('IQAir AQI Category (Actual)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\\\nüìã Classification Report (US AQI vs IQAir):\")\n",
    "print(classification_report(df_validation['iqair_aqi_category'], df_validation['us_aqi_category']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('AQI Category Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# US AQI categories\n",
    "us_counts = df_validation['us_aqi_category'].value_counts()\n",
    "axes[0].bar(us_counts.index, us_counts.values, color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('US AQI Categories')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# IQAir AQI categories\n",
    "iqair_counts = df_validation['iqair_aqi_category'].value_counts()\n",
    "axes[1].bar(iqair_counts.index, iqair_counts.values, color='lightcoral', alpha=0.8)\n",
    "axes[1].set_title('IQAir AQI Categories')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Temporal Pattern Analysis\n",
    "\n",
    "Analyze how the agreement between data sources varies over time and identify patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporal features\n",
    "df_validation['hour'] = df_validation['time'].dt.hour\n",
    "df_validation['day_of_week'] = df_validation['time'].dt.dayofweek\n",
    "df_validation['date'] = df_validation['time'].dt.date\n",
    "\n",
    "# Calculate daily statistics\n",
    "daily_stats = df_validation.groupby('date').agg({\n",
    "    'us_aqi': ['mean', 'std', 'min', 'max'],\n",
    "    'iqair_aqi': ['mean', 'std', 'min', 'max'],\n",
    "    'abs_deviation': ['mean', 'std', 'min', 'max'],\n",
    "    'time': 'count'\n",
    "}).round(2)\n",
    "\n",
    "daily_stats.columns = ['_'.join(col).strip() for col in daily_stats.columns]\n",
    "daily_stats = daily_stats.rename(columns={'time_count': 'records_per_day'})\n",
    "\n",
    "print(\"üìÖ Daily Statistics Summary:\")\n",
    "print(f\"Average records per day: {daily_stats['records_per_day'].mean():.1f}\")\n",
    "print(f\"Days with data: {len(daily_stats)}\")\n",
    "print(\"\\\\nDaily deviation statistics:\")\n",
    "print(daily_stats[['abs_deviation_mean', 'abs_deviation_std', 'abs_deviation_min', 'abs_deviation_max']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly pattern analysis\n",
    "hourly_stats = df_validation.groupby('hour').agg({\n",
    "    'us_aqi': 'mean',\n",
    "    'iqair_aqi': 'mean',\n",
    "    'abs_deviation': 'mean',\n",
    "    'time': 'count'\n",
    "}).round(2)\n",
    "\n",
    "# Visualization of temporal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Temporal Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Hourly AQI patterns\n",
    "axes[0,0].plot(hourly_stats.index, hourly_stats['us_aqi'], marker='o', label='US AQI', linewidth=2)\n",
    "axes[0,0].plot(hourly_stats.index, hourly_stats['iqair_aqi'], marker='s', label='IQAir AQI', linewidth=2)\n",
    "axes[0,0].set_xlabel('Hour of Day')\n",
    "axes[0,0].set_ylabel('Mean AQI')\n",
    "axes[0,0].set_title('Hourly AQI Patterns')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 2. Hourly deviation pattern\n",
    "axes[0,1].plot(hourly_stats.index, hourly_stats['abs_deviation'], \n",
    "               marker='o', color='red', linewidth=2)\n",
    "axes[0,1].axhline(hourly_stats['abs_deviation'].mean(), color='black', \n",
    "                  linestyle='--', alpha=0.8, label='Overall Mean')\n",
    "axes[0,1].set_xlabel('Hour of Day')\n",
    "axes[0,1].set_ylabel('Mean Absolute Deviation')\n",
    "axes[0,1].set_title('Hourly Deviation Pattern')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 3. Daily deviation trend\n",
    "daily_stats_reset = daily_stats.reset_index()\n",
    "axes[1,0].plot(daily_stats_reset['date'], daily_stats_reset['abs_deviation_mean'], \n",
    "               marker='o', alpha=0.7, linewidth=2)\n",
    "axes[1,0].fill_between(daily_stats_reset['date'], \n",
    "                       daily_stats_reset['abs_deviation_mean'] - daily_stats_reset['abs_deviation_std'],\n",
    "                       daily_stats_reset['abs_deviation_mean'] + daily_stats_reset['abs_deviation_std'],\n",
    "                       alpha=0.3)\n",
    "axes[1,0].set_xlabel('Date')\n",
    "axes[1,0].set_ylabel('Daily Mean Deviation ¬± STD')\n",
    "axes[1,0].set_title('Daily Deviation Trend')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Data availability by hour\n",
    "axes[1,1].bar(hourly_stats.index, hourly_stats['time'], color='green', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Hour of Day')\n",
    "axes[1,1].set_ylabel('Number of Records')\n",
    "axes[1,1].set_title('Data Availability by Hour')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\n‚è∞ Best Agreement Hours: {hourly_stats['abs_deviation'].nsmallest(3).index.tolist()}\")\n",
    "print(f\"üö® Worst Agreement Hours: {hourly_stats['abs_deviation'].nlargest(3).index.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Statistical Reliability Assessment\n",
    "\n",
    "Perform statistical tests to assess the reliability and bias of different data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests\n",
    "print(\"üß™ Statistical Reliability Tests:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Paired t-test for systematic bias\n",
    "tstat, ttest_p = ttest_rel(df_validation['us_aqi'], df_validation['iqair_aqi'])\n",
    "print(f\"\\\\n1. Paired t-test (Systematic Bias):\")\n",
    "print(f\"   t-statistic: {tstat:.4f}\")\n",
    "print(f\"   p-value: {ttest_p:.6f}\")\n",
    "if ttest_p < 0.05:\n",
    "    print(f\"   ‚ùå Significant systematic bias detected (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No significant systematic bias (p ‚â• 0.05)\")\n",
    "\n",
    "# 2. Wilcoxon signed-rank test (non-parametric)\n",
    "wstat, wilcox_p = wilcoxon(df_validation['us_aqi'], df_validation['iqair_aqi'])\n",
    "print(f\"\\\\n2. Wilcoxon Signed-Rank Test:\")\n",
    "print(f\"   W-statistic: {wstat:.0f}\")\n",
    "print(f\"   p-value: {wilcox_p:.6f}\")\n",
    "if wilcox_p < 0.05:\n",
    "    print(f\"   ‚ùå Significant difference in distributions (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No significant difference in distributions (p ‚â• 0.05)\")\n",
    "\n",
    "# 3. Normality tests for residuals\n",
    "residuals = df_validation['us_aqi'] - df_validation['iqair_aqi']\n",
    "shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "print(f\"\\\\n3. Shapiro-Wilk Test (Residuals Normality):\")\n",
    "print(f\"   W-statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"   p-value: {shapiro_p:.6f}\")\n",
    "if shapiro_p < 0.05:\n",
    "    print(f\"   ‚ùå Residuals not normally distributed (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Residuals normally distributed (p ‚â• 0.05)\")\n",
    "\n",
    "# 4. Correlation significance tests\n",
    "pearson_r, pearson_p = pearsonr(df_validation['us_aqi'], df_validation['iqair_aqi'])\n",
    "spearman_r, spearman_p = spearmanr(df_validation['us_aqi'], df_validation['iqair_aqi'])\n",
    "\n",
    "print(f\"\\\\n4. Correlation Tests:\")\n",
    "print(f\"   Pearson r: {pearson_r:.4f} (p = {pearson_p:.6f})\")\n",
    "print(f\"   Spearman œÅ: {spearman_r:.4f} (p = {spearman_p:.6f})\")\n",
    "\n",
    "# 5. Bias and precision metrics\n",
    "bias = (df_validation['us_aqi'] - df_validation['iqair_aqi']).mean()\n",
    "precision = (df_validation['us_aqi'] - df_validation['iqair_aqi']).std()\n",
    "mape = np.mean(np.abs((df_validation['iqair_aqi'] - df_validation['us_aqi']) / df_validation['iqair_aqi'])) * 100\n",
    "\n",
    "print(f\"\\\\n5. Bias & Precision Analysis:\")\n",
    "print(f\"   Mean Bias: {bias:.2f} AQI units\")\n",
    "print(f\"   Precision (SD): {precision:.2f} AQI units\")\n",
    "print(f\"   MAPE: {mape:.1f}%\")\n",
    "\n",
    "if abs(bias) < 5:\n",
    "    print(f\"   ‚úÖ Low bias (|bias| < 5 AQI units)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate bias (|bias| ‚â• 5 AQI units)\")\n",
    "\n",
    "if precision < 15:\n",
    "    print(f\"   ‚úÖ Good precision (SD < 15 AQI units)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate precision (SD ‚â• 15 AQI units)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias analysis visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Statistical Reliability Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Residuals distribution\n",
    "axes[0,0].hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(residuals.mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[0,0].axvline(0, color='green', linestyle='-', label='Perfect Agreement')\n",
    "axes[0,0].set_xlabel('Residuals (US AQI - IQAir AQI)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Residuals Distribution')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q plot for residuals\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
    "axes[0,1].set_title('Q-Q Plot: Residuals vs Normal Distribution')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Bland-Altman plot\n",
    "mean_values = (df_validation['us_aqi'] + df_validation['iqair_aqi']) / 2\n",
    "diff_values = df_validation['us_aqi'] - df_validation['iqair_aqi']\n",
    "\n",
    "axes[1,0].scatter(mean_values, diff_values, alpha=0.6)\n",
    "axes[1,0].axhline(diff_values.mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean Diff: {diff_values.mean():.2f}')\n",
    "axes[1,0].axhline(diff_values.mean() + 1.96*diff_values.std(), color='orange', \n",
    "                  linestyle='--', label=f'+1.96 SD: {diff_values.mean() + 1.96*diff_values.std():.2f}')\n",
    "axes[1,0].axhline(diff_values.mean() - 1.96*diff_values.std(), color='orange', \n",
    "                  linestyle='--', label=f'-1.96 SD: {diff_values.mean() - 1.96*diff_values.std():.2f}')\n",
    "axes[1,0].axhline(0, color='green', linestyle='-', alpha=0.7)\n",
    "axes[1,0].set_xlabel('Mean AQI ((US + IQAir) / 2)')\n",
    "axes[1,0].set_ylabel('Difference (US - IQAir)')\n",
    "axes[1,0].set_title('Bland-Altman Plot')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Rolling bias over time\n",
    "df_validation_sorted = df_validation.sort_values('time').reset_index(drop=True)\n",
    "window_size = min(24, len(df_validation_sorted) // 10)  # Adaptive window size\n",
    "rolling_bias = (df_validation_sorted['us_aqi'] - df_validation_sorted['iqair_aqi']).rolling(window=window_size, center=True).mean()\n",
    "\n",
    "axes[1,1].plot(df_validation_sorted['time'], rolling_bias, color='purple', linewidth=2)\n",
    "axes[1,1].axhline(0, color='green', linestyle='-', alpha=0.7, label='No Bias')\n",
    "axes[1,1].axhline(bias, color='red', linestyle='--', alpha=0.8, label=f'Overall Bias: {bias:.2f}')\n",
    "axes[1,1].set_xlabel('Time')\n",
    "axes[1,1].set_ylabel(f'Rolling Bias (Window={window_size})')\n",
    "axes[1,1].set_title('Temporal Bias Trend')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Source Reliability Scoring\n",
    "\n",
    "Create a comprehensive scoring system to evaluate the reliability of different data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reliability metrics\n",
    "def calculate_reliability_score(mae, rmse, correlation, bias, precision, agreement_rate):\n",
    "    \"\"\"\n",
    "    Calculate reliability score based on multiple metrics\n",
    "    Score ranges from 0-100 (higher is better)\n",
    "    \"\"\"\n",
    "    # Normalize metrics (0-1 scale, higher is better)\n",
    "    \n",
    "    # MAE score (lower is better) - assuming max reasonable MAE is 100\n",
    "    mae_score = max(0, 1 - mae / 100)\n",
    "    \n",
    "    # RMSE score (lower is better) - assuming max reasonable RMSE is 150\n",
    "    rmse_score = max(0, 1 - rmse / 150)\n",
    "    \n",
    "    # Correlation score (higher is better)\n",
    "    corr_score = max(0, correlation)\n",
    "    \n",
    "    # Bias score (lower absolute bias is better) - assuming max reasonable bias is 50\n",
    "    bias_score = max(0, 1 - abs(bias) / 50)\n",
    "    \n",
    "    # Precision score (lower is better) - assuming max reasonable precision is 50\n",
    "    precision_score = max(0, 1 - precision / 50)\n",
    "    \n",
    "    # Agreement rate score\n",
    "    agreement_score = agreement_rate\n",
    "    \n",
    "    # Weighted average (adjust weights based on importance)\n",
    "    weights = {\n",
    "        'mae': 0.2,\n",
    "        'rmse': 0.2,\n",
    "        'correlation': 0.25,\n",
    "        'bias': 0.15,\n",
    "        'precision': 0.1,\n",
    "        'agreement': 0.1\n",
    "    }\n",
    "    \n",
    "    total_score = (\n",
    "        weights['mae'] * mae_score +\n",
    "        weights['rmse'] * rmse_score +\n",
    "        weights['correlation'] * corr_score +\n",
    "        weights['bias'] * bias_score +\n",
    "        weights['precision'] * precision_score +\n",
    "        weights['agreement'] * agreement_score\n",
    "    ) * 100\n",
    "    \n",
    "    return total_score, {\n",
    "        'mae_score': mae_score * 100,\n",
    "        'rmse_score': rmse_score * 100,\n",
    "        'correlation_score': corr_score * 100,\n",
    "        'bias_score': bias_score * 100,\n",
    "        'precision_score': precision_score * 100,\n",
    "        'agreement_score': agreement_score * 100\n",
    "    }\n",
    "\n",
    "# Calculate reliability score for US AQI vs IQAir\n",
    "us_reliability_score, us_components = calculate_reliability_score(\n",
    "    mae=mae_us_iqair,\n",
    "    rmse=rmse_us_iqair,\n",
    "    correlation=correlation_us_iqair,\n",
    "    bias=bias,\n",
    "    precision=precision,\n",
    "    agreement_rate=category_agreement\n",
    ")\n",
    "\n",
    "print(\"üèÜ DATA SOURCE RELIABILITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\\\nüìä US AQI (OpenWeather PM-based) vs IQAir Sensor:\")\n",
    "print(f\"   Overall Reliability Score: {us_reliability_score:.1f}/100\")\n",
    "print(f\"\\\\n   Component Scores:\")\n",
    "for component, score in us_components.items():\n",
    "    print(f\"   ‚Ä¢ {component.replace('_', ' ').title()}: {score:.1f}/100\")\n",
    "\n",
    "# Interpretation\n",
    "if us_reliability_score >= 80:\n",
    "    reliability_grade = \"Excellent (A)\"\n",
    "    reliability_emoji = \"üü¢\"\n",
    "elif us_reliability_score >= 70:\n",
    "    reliability_grade = \"Good (B)\"\n",
    "    reliability_emoji = \"üü°\"\n",
    "elif us_reliability_score >= 60:\n",
    "    reliability_grade = \"Fair (C)\"\n",
    "    reliability_emoji = \"üü†\"\n",
    "else:\n",
    "    reliability_grade = \"Poor (D/F)\"\n",
    "    reliability_emoji = \"üî¥\"\n",
    "\n",
    "print(f\"\\\\n{reliability_emoji} Reliability Grade: {reliability_grade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability score visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Data Source Reliability Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Component scores radar chart\n",
    "components = list(us_components.keys())\n",
    "scores = list(us_components.values())\n",
    "\n",
    "# Create radar chart data\n",
    "angles = np.linspace(0, 2*np.pi, len(components), endpoint=False).tolist()\n",
    "scores_plot = scores + [scores[0]]  # Complete the circle\n",
    "angles_plot = angles + [angles[0]]  # Complete the circle\n",
    "\n",
    "ax_radar = plt.subplot(1, 2, 1, projection='polar')\n",
    "ax_radar.plot(angles_plot, scores_plot, 'o-', linewidth=2, color='blue')\n",
    "ax_radar.fill(angles_plot, scores_plot, alpha=0.25, color='blue')\n",
    "ax_radar.set_xticks(angles)\n",
    "ax_radar.set_xticklabels([comp.replace('_', '\\\\n').title() for comp in components])\n",
    "ax_radar.set_ylim(0, 100)\n",
    "ax_radar.set_title('Reliability Component Scores\\\\n(US AQI vs IQAir)', y=1.08)\n",
    "ax_radar.grid(True)\n",
    "\n",
    "# 2. Overall score gauge\n",
    "ax_gauge = axes[1]\n",
    "theta = np.linspace(0, np.pi, 100)\n",
    "r = np.ones_like(theta)\n",
    "\n",
    "# Create color segments\n",
    "colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "bounds = [0, 40, 60, 70, 80, 100]\n",
    "\n",
    "for i in range(len(colors)):\n",
    "    start_angle = np.pi * (1 - bounds[i+1]/100)\n",
    "    end_angle = np.pi * (1 - bounds[i]/100)\n",
    "    theta_seg = np.linspace(start_angle, end_angle, 20)\n",
    "    r_seg = np.ones_like(theta_seg)\n",
    "    ax_gauge.fill_between(theta_seg, 0, r_seg, color=colors[i], alpha=0.7)\n",
    "\n",
    "# Add score indicator\n",
    "score_angle = np.pi * (1 - us_reliability_score/100)\n",
    "ax_gauge.plot([score_angle, score_angle], [0, 1], 'black', linewidth=4)\n",
    "ax_gauge.plot(score_angle, 1, 'ko', markersize=10)\n",
    "\n",
    "ax_gauge.set_xlim(0, np.pi)\n",
    "ax_gauge.set_ylim(0, 1.2)\n",
    "ax_gauge.set_xticks([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi])\n",
    "ax_gauge.set_xticklabels(['100', '75', '50', '25', '0'])\n",
    "ax_gauge.set_yticks([])\n",
    "ax_gauge.set_title(f'Overall Reliability Score\\\\n{us_reliability_score:.1f}/100 ({reliability_grade})')\n",
    "ax_gauge.text(np.pi/2, 0.5, f'{us_reliability_score:.1f}', \n",
    "              ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Recommendations & Summary\n",
    "\n",
    "Provide actionable recommendations based on the validation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"üìã VALIDATION ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\\\nüìä Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total validation records: {len(df_validation):,}\")\n",
    "print(f\"   ‚Ä¢ Time period: {df_validation['time'].min().strftime('%Y-%m-%d')} to {df_validation['time'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   ‚Ä¢ Duration: {(df_validation['time'].max() - df_validation['time'].min()).days} days\")\n",
    "print(f\"   ‚Ä¢ Data completeness: {(1 - df_validation.isnull().sum().sum() / (len(df_validation) * len(df_validation.columns))) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nüéØ Performance Metrics (US AQI vs IQAir):\")\n",
    "print(f\"   ‚Ä¢ Correlation (Pearson): {correlation_us_iqair:.3f}\")\n",
    "print(f\"   ‚Ä¢ Mean Absolute Error: {mae_us_iqair:.1f} AQI units\")\n",
    "print(f\"   ‚Ä¢ Root Mean Square Error: {rmse_us_iqair:.1f} AQI units\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {r2_us_iqair:.3f}\")\n",
    "print(f\"   ‚Ä¢ Category Agreement: {category_agreement:.1%}\")\n",
    "\n",
    "print(f\"\\\\nüìà Bias Analysis:\")\n",
    "print(f\"   ‚Ä¢ Mean Bias: {bias:.1f} AQI units\")\n",
    "print(f\"   ‚Ä¢ Precision (SD): {precision:.1f} AQI units\")\n",
    "print(f\"   ‚Ä¢ MAPE: {mape:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Median Absolute Deviation: {df_validation['abs_deviation'].median():.1f}\")\n",
    "\n",
    "print(f\"\\\\nüèÜ Overall Assessment:\")\n",
    "print(f\"   ‚Ä¢ Reliability Score: {us_reliability_score:.1f}/100\")\n",
    "print(f\"   ‚Ä¢ Reliability Grade: {reliability_grade}\")\n",
    "print(f\"   ‚Ä¢ Statistical Significance: {'Yes' if ttest_p < 0.05 else 'No'} (p = {ttest_p:.4f})\")\n",
    "\n",
    "# Detailed recommendations\n",
    "print(f\"\\\\nüí° RECOMMENDATIONS:\")\n",
    "print(f\"\\\\n1. üéØ Model Training Approach:\")\n",
    "if us_reliability_score >= 70:\n",
    "    print(f\"   ‚úÖ PROCEED with OpenWeather PM-based AQI calculation for training\")\n",
    "    print(f\"   ‚Ä¢ High reliability score ({us_reliability_score:.1f}/100) indicates good agreement\")\n",
    "    print(f\"   ‚Ä¢ US EPA formula application is working well with OpenWeather PM data\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  CAUTION with OpenWeather PM-based AQI calculation\")\n",
    "    print(f\"   ‚Ä¢ Moderate reliability score ({us_reliability_score:.1f}/100) suggests potential issues\")\n",
    "    print(f\"   ‚Ä¢ Consider additional validation or data source investigation\")\n",
    "\n",
    "print(f\"\\\\n2. üïê Temporal Considerations:\")\n",
    "best_hours = hourly_stats['abs_deviation'].nsmallest(3).index.tolist()\n",
    "worst_hours = hourly_stats['abs_deviation'].nlargest(3).index.tolist()\n",
    "print(f\"   ‚Ä¢ Best agreement hours: {best_hours} (consider weighting these more heavily)\")\n",
    "print(f\"   ‚Ä¢ Worst agreement hours: {worst_hours} (consider additional validation)\")\n",
    "print(f\"   ‚Ä¢ Time-based features may improve model performance\")\n",
    "\n",
    "print(f\"\\\\n3. üé≤ Statistical Insights:\")\n",
    "if abs(bias) < 5:\n",
    "    print(f\"   ‚úÖ Low systematic bias ({bias:.1f} units) - minimal correction needed\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate systematic bias ({bias:.1f} units) - consider bias correction\")\n",
    "\n",
    "if precision < 15:\n",
    "    print(f\"   ‚úÖ Good precision ({precision:.1f} units) - consistent measurements\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate precision ({precision:.1f} units) - consider ensemble methods\")\n",
    "\n",
    "print(f\"\\\\n4. üìä Model Development Strategy:\")\n",
    "print(f\"   ‚Ä¢ Use PM2.5 & PM10 concentrations as primary features (validated approach)\")\n",
    "print(f\"   ‚Ä¢ Include temporal features (hour, day of week) for time-dependent patterns\")\n",
    "print(f\"   ‚Ä¢ Consider bias correction in post-processing if systematic bias detected\")\n",
    "print(f\"   ‚Ä¢ Implement robust validation using time-based splits\")\n",
    "print(f\"   ‚Ä¢ Monitor model performance against IQAir data in production\")\n",
    "\n",
    "print(f\"\\\\n5. üö® Quality Assurance:\")\n",
    "print(f\"   ‚Ä¢ Set up automated validation against IQAir data for ongoing monitoring\")\n",
    "print(f\"   ‚Ä¢ Alert if deviation exceeds {df_validation['abs_deviation'].quantile(0.95):.0f} AQI units (95th percentile)\")\n",
    "print(f\"   ‚Ä¢ Regular recalibration if bias drift is detected\")\n",
    "print(f\"   ‚Ä¢ Maintain separate validation dataset for model evaluation\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ CONCLUSION:\")\n",
    "if us_reliability_score >= 70 and category_agreement >= 0.7:\n",
    "    conclusion = \"OpenWeather PM-based AQI calculation is RELIABLE for model training\"\n",
    "    confidence = \"HIGH\"\n",
    "elif us_reliability_score >= 60 and category_agreement >= 0.6:\n",
    "    conclusion = \"OpenWeather PM-based AQI calculation is ACCEPTABLE with monitoring\"\n",
    "    confidence = \"MODERATE\"\n",
    "else:\n",
    "    conclusion = \"OpenWeather PM-based AQI calculation needs IMPROVEMENT\"\n",
    "    confidence = \"LOW\"\n",
    "\n",
    "print(f\"   {conclusion}\")\n",
    "print(f\"   Confidence Level: {confidence}\")\n",
    "print(f\"   Recommended for 3-day AQI forecasting: {'YES' if confidence != 'LOW' else 'WITH CAUTION'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics for reference\n",
    "summary_stats = {\n",
    "    'validation_records': len(df_validation),\n",
    "    'time_range_start': df_validation['time'].min().isoformat(),\n",
    "    'time_range_end': df_validation['time'].max().isoformat(),\n",
    "    'correlation_us_iqair': float(correlation_us_iqair),\n",
    "    'mae_us_iqair': float(mae_us_iqair),\n",
    "    'rmse_us_iqair': float(rmse_us_iqair),\n",
    "    'r2_score': float(r2_us_iqair),\n",
    "    'category_agreement': float(category_agreement),\n",
    "    'mean_bias': float(bias),\n",
    "    'precision_sd': float(precision),\n",
    "    'mape_percent': float(mape),\n",
    "    'reliability_score': float(us_reliability_score),\n",
    "    'reliability_grade': reliability_grade,\n",
    "    'statistical_significance_p': float(ttest_p),\n",
    "    'best_agreement_hours': best_hours,\n",
    "    'worst_agreement_hours': worst_hours,\n",
    "    'recommended_for_training': confidence != 'LOW'\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('validation_summary_statistics.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\\\nüìÅ Summary statistics exported to: validation_summary_statistics.json\")\n",
    "print(f\"\\\\nüéâ Validation Analysis Complete!\")\n",
    "print(f\"   Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Ready for model development phase: {'YES' if confidence != 'LOW' else 'WITH MONITORING'}\")\n",
    "\n",
    "# Display key takeaways\n",
    "print(f\"\\\\nüîë KEY TAKEAWAYS:\")\n",
    "print(f\"   1. Your US AQI calculation shows {reliability_grade.lower()} agreement with IQAir sensors\")\n",
    "print(f\"   2. Mean deviation is {df_validation['abs_deviation'].mean():.1f} AQI units - {'reasonable' if df_validation['abs_deviation'].mean() < 20 else 'moderate'}\")\n",
    "print(f\"   3. Category agreement is {category_agreement:.1%} - {'good' if category_agreement > 0.7 else 'moderate' if category_agreement > 0.6 else 'needs improvement'}\")\n",
    "print(f\"   4. {'Strong' if correlation_us_iqair > 0.7 else 'Moderate' if correlation_us_iqair > 0.5 else 'Weak'} correlation (r = {correlation_us_iqair:.3f}) indicates {'good' if correlation_us_iqair > 0.7 else 'acceptable' if correlation_us_iqair > 0.5 else 'poor'} linear relationship\")\n",
    "print(f\"   5. This validation supports using OpenWeather PM data for {'reliable' if confidence == 'HIGH' else 'monitored' if confidence == 'MODERATE' else 'cautious'} AQI forecasting\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
