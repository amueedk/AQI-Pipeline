"""
Data Collector for AQI Data Collection & Feature Engineering
Fetches air quality and weather data from OpenWeather API for Multan
"""
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import logging
from typing import Dict, List, Optional, Tuple
import json
import os
from config import DATA_CONFIG, PATHS, IQAIR_CONFIG, OPENWEATHER_CONFIG, OPENWEATHER_HISTORY_WEATHER_URL
import datetime

logger = None

# Ensure logger is always initialized
if logger is None:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(PATHS['logs_dir'], 'data_collector.log')),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

COMMON_POLLUTANTS = [
    "pm2_5", "pm10", "ozone", "nitrogen_dioxide", "sulphur_dioxide", "carbon_monoxide", "us_aqi"
]
COMMON_WEATHER = [
    "temperature", "humidity", "pressure", "wind_speed", "wind_direction"
]

# US EPA AQI breakpoints for each pollutant
AQI_BREAKPOINTS = {
    "pm2_5": [
        (0.0, 12.0, 0, 50),
        (12.1, 35.4, 51, 100),
        (35.5, 55.4, 101, 150),
        (55.5, 150.4, 151, 200),
        (150.5, 250.4, 201, 300),
        (250.5, 350.4, 301, 400),
        (350.5, 500.4, 401, 500),
    ],
    "pm10": [
        (0, 54, 0, 50),
        (55, 154, 51, 100),
        (155, 254, 101, 150),
        (255, 354, 151, 200),
        (355, 424, 201, 300),
        (425, 504, 301, 400),
        (505, 604, 401, 500),
    ],
    "o3_8h": [
        (0, 54, 0, 50),
        (55, 70, 51, 100),
        (71, 85, 101, 150),
        (86, 105, 151, 200),
        (106, 200, 201, 300),
    ],
    "o3_1h": [
        (125, 164, 101, 150),
        (165, 204, 151, 200),
        (205, 404, 201, 300),
        (405, 504, 301, 400),
        (505, 604, 401, 500),
    ],
    "co": [
        (0.0, 4.4, 0, 50),
        (4.5, 9.4, 51, 100),
        (9.5, 12.4, 101, 150),
        (12.5, 15.4, 151, 200),
        (15.5, 30.4, 201, 300),
        (30.5, 40.4, 301, 400),
        (40.5, 50.4, 401, 500),
    ],
    "so2": [
        (0, 35, 0, 50),
        (36, 75, 51, 100),
        (76, 185, 101, 150),
        (186, 304, 151, 200),
        (305, 604, 201, 300),
        (605, 804, 301, 400),
        (805, 1004, 401, 500),
    ],
    "no2": [
        (0, 53, 0, 50),
        (54, 100, 51, 100),
        (101, 360, 101, 150),
        (361, 649, 151, 200),
        (650, 1249, 201, 300),
        (1250, 1649, 301, 400),
        (1650, 2049, 401, 500),
    ],
}

def calc_aqi(conc, breakpoints):
    """Calculate AQI for a given concentration using US EPA breakpoints"""
    for C_low, C_high, I_low, I_high in breakpoints:
        if C_low <= conc <= C_high:
            return round((I_high - I_low) / (C_high - C_low) * (conc - C_low) + I_low)
    return None

def compute_pm2_5_aqi(row):
    """Compute PM2.5 AQI only (since PM2.5 is typically the dominant pollutant)"""
    if not pd.isna(row.get("pm2_5")):
        return calc_aqi(row["pm2_5"], AQI_BREAKPOINTS["pm2_5"])
    return None

class IQAirDataCollector:
    def __init__(self):
        self.api_key = IQAIR_CONFIG["api_key"]
        self.base_url = IQAIR_CONFIG["base_url"]
        self.city = IQAIR_CONFIG["city"]
        self.state = IQAIR_CONFIG["state"]
        self.country = IQAIR_CONFIG["country"]

    def fetch_current_aqi(self) -> pd.DataFrame:
        """
        Fetches the latest AQI (us_aqi) for the configured city from IQAir.
        Returns a DataFrame with a single row: index is time, column is 'iqair_aqi'.
        """
        url = f"{self.base_url}/city"
        params = {
            "city": self.city,
            "state": self.state,
            "country": self.country,
            "key": self.api_key
        }
        try:
            resp = requests.get(url, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            if data["status"] != "success":
                logger.warning(f"IQAir API returned non-success: {data.get('status')}")
                return pd.DataFrame()
            pollution = data["data"]["current"]["pollution"]
            row = {
                "time": pd.to_datetime(pollution["ts"]),
                "iqair_aqi": pollution.get("aqius")
            }
            df = pd.DataFrame([row])
            df.set_index("time", inplace=True)
            return df
        except Exception as e:
            logger.warning(f"IQAir API error: {e}")
            return pd.DataFrame()

def fetch_historic_weather(start_unix, end_unix):
    """
    Fetch hourly weather data from OpenWeather historic endpoint for Multan.
    Returns a DataFrame indexed by UTC timestamp.
    """
    url = f"{OPENWEATHER_HISTORY_WEATHER_URL}?lat={OPENWEATHER_CONFIG['lat']}&lon={OPENWEATHER_CONFIG['lon']}&type=hour&start={start_unix}&end={end_unix}&appid={OPENWEATHER_CONFIG['api_key']}"
    resp = requests.get(url, timeout=20)
    resp.raise_for_status()
    data = resp.json()
    rows = []
    for entry in data.get("list", []):
        dt = pd.to_datetime(entry["dt"], unit="s", utc=True)
        main = entry.get("main", {})
        wind = entry.get("wind", {})
        row = {
            "time": dt,
            "temperature": main.get("temp"),
            "humidity": main.get("humidity"),
            "pressure": main.get("pressure"),
            "wind_speed": wind.get("speed"),
            "wind_direction": wind.get("deg"),
        }
        rows.append(row)
    df = pd.DataFrame(rows)
    df.set_index("time", inplace=True)
    return df

class OpenWeatherDataCollector:
    def __init__(self):
        self.api_key = OPENWEATHER_CONFIG["api_key"]
        self.base_url = OPENWEATHER_CONFIG["base_url"]
        self.lat = OPENWEATHER_CONFIG["lat"]
        self.lon = OPENWEATHER_CONFIG["lon"]

    def fetch_air_pollution(self, start_unix=None, end_unix=None):
        if start_unix and end_unix:
            url = f"{self.base_url}/air_pollution/history?lat={self.lat}&lon={self.lon}&start={start_unix}&end={end_unix}&appid={self.api_key}"
        else:
            url = f"{self.base_url}/air_pollution?lat={self.lat}&lon={self.lon}&appid={self.api_key}"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        rows = []
        for entry in data["list"]:
            row = {"time": pd.to_datetime(entry["dt"], unit="s", utc=True)}
            for k, v in entry["components"].items():
                row[k] = v
            # Extract OpenWeather AQI (1-5 scale)
            if "main" in entry and "aqi" in entry["main"]:
                row["openweather_aqi"] = entry["main"]["aqi"]
            else:
                row["openweather_aqi"] = None
            rows.append(row)
        df = pd.DataFrame(rows)
        df.set_index("time", inplace=True)
        # Calculate PM2.5 AQI
        df["pm2_5_aqi"] = df.apply(compute_pm2_5_aqi, axis=1)
        # Calculate PM10 AQI
        df["pm10_aqi"] = df.apply(lambda row: calc_aqi(row.get('pm10'), AQI_BREAKPOINTS['pm10']) if pd.notna(row.get('pm10')) else None, axis=1)
        # Calculate US AQI as maximum of PM2.5 and PM10 AQI
        df["us_aqi"] = df.apply(lambda row: max(row['pm2_5_aqi'], row['pm10_aqi']) if pd.notna(row.get('pm2_5_aqi')) and pd.notna(row.get('pm10_aqi')) else None, axis=1)
        return df

    def fetch_weather(self, dt_unix=None):
        url = f"{self.base_url}/weather?lat={self.lat}&lon={self.lon}&appid={self.api_key}&units=metric"
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        row = {
            "temperature": data["main"]["temp"],
            "humidity": data["main"]["humidity"],
            "pressure": data["main"]["pressure"],
            "wind_speed": data["wind"]["speed"],
            "wind_direction": data["wind"]["deg"],
        }
        return row

    def collect_historical_data(self, days_back=14):
        end_time = int(time.time())
        start_time = end_time - days_back * 24 * 3600
        df_pollution = self.fetch_air_pollution(start_unix=start_time, end_unix=end_time)
        df_weather = fetch_historic_weather(start_unix=start_time, end_unix=end_time)
        # Merge on timestamp (outer join to keep all pollution records)
        df_merged = df_pollution.join(df_weather, how="outer")
        df_merged["city"] = OPENWEATHER_CONFIG["city"]
        df_merged["latitude"] = self.lat
        df_merged["longitude"] = self.lon
        return df_merged

    def collect_current_data(self):
        """
        Collect current air pollution and weather data (most recent hour).
        Returns a DataFrame with current data from OpenWeather APIs.
        """
        df_pollution = self.fetch_air_pollution()
        weather = self.fetch_weather()
        for k, v in weather.items():
            df_pollution[k] = v
        df_pollution["city"] = OPENWEATHER_CONFIG["city"]
        df_pollution["latitude"] = self.lat
        df_pollution["longitude"] = self.lon
        # Note: CSV saving is handled in collect_current_data_with_iqair()
        return df_pollution

def fetch_iqair_aqi():
    url = f"https://api.airvisual.com/v2/nearest_city?lat={OPENWEATHER_CONFIG['lat']}&lon={OPENWEATHER_CONFIG['lon']}&key={IQAIR_CONFIG['api_key']}"
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        if data["status"] == "success":
            return data["data"]["current"]["pollution"]["aqius"]
    except Exception as e:
        logger.warning(f"IQAir API error: {e}")
    return None

def collect_current_data_with_iqair():
    df = OpenWeatherDataCollector().collect_current_data()
    iqair_aqi = fetch_iqair_aqi()
    if iqair_aqi is not None:
        df["iqair_aqi"] = iqair_aqi
        # Compute absolute deviation
        if "us_aqi" in df.columns:
            df["abs_deviation"] = abs(df["us_aqi"] - iqair_aqi)
    else:
        df["iqair_aqi"] = None
        df["abs_deviation"] = None
    
    # Save only validation data (much smaller files)
    # Reset index to make 'time' a column again
    df_reset = df.reset_index()
    validation_cols = ['time', 'us_aqi', 'iqair_aqi', 'abs_deviation']
    # Only include columns that exist
    available_cols = [col for col in validation_cols if col in df_reset.columns]
    logger.info(f"DEBUG: Available columns in DataFrame: {list(df_reset.columns)}")
    logger.info(f"DEBUG: Validation columns requested: {validation_cols}")
    logger.info(f"DEBUG: Validation columns available: {available_cols}")
    validation_df = df_reset[available_cols].copy()
    
    # Save to monthly validation file
    monthly = datetime.datetime.utcnow().strftime('%Y%m')
    validation_path = f"data/pm2_5_validation_monthly_{monthly}.csv"
    
    # Ensure data directory exists
    os.makedirs("data", exist_ok=True)
    
    if os.path.exists(validation_path):
        # Append to existing monthly validation file
        try:
            existing_df = pd.read_csv(validation_path, index_col=0, parse_dates=True)
            combined_df = pd.concat([existing_df, validation_df])
            # Remove duplicates based on timestamp (keep the latest)
            combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
            # Sort by timestamp
            combined_df = combined_df.sort_index()
        except Exception as e:
            logger.warning(f"Error reading existing validation file: {e}. Creating new file.")
            combined_df = validation_df
    else:
        combined_df = validation_df
    
    combined_df.to_csv(validation_path)
    logger.info(f"Validation data appended to monthly file: {validation_path} (total {len(combined_df)} records)")
    return df

def main():
    """
    Main function to test data collection.
    Can be used for a one-off historical data collection.
    """
    collector = OpenWeatherDataCollector()
    
    # --- For Historical Data Collection ---
    logger.info("--- Starting Historical Data Collection ---")
    historical_df = collector.collect_historical_data(days_back=14)
    if not historical_df.empty:
        logger.info(f"Successfully collected {len(historical_df)} historical records.")
    else:
        logger.error("Historical data collection failed.")

    # --- For Current Data Collection ---
    logger.info("\n--- Starting Current Data Collection ---")
    current_df = collector.collect_current_data()
    if not current_df.empty:
        logger.info(f"Successfully collected {len(current_df)} current records.")
    else:
        logger.warning("No new current data collected.")


if __name__ == "__main__":
    main() 