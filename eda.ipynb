{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA - Multan AQI Features\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Hopsworks Feature Store (multan_aqi_features)\n",
    "- **Records**: 538 observations \n",
    "- **Features**: 127 engineered features\n",
    "- **Time Range**: June 16, 2025 - July 8, 2025\n",
    "\n",
    "## Modeling Approach\n",
    "- **🎯 Goal**: Accurate US AQI prediction for Multan\n",
    "- **🔧 Method**: Train ML model to predict PM2.5 & PM10 → Calculate AQI via EPA formula\n",
    "- **📊 ML Targets**: pm2_5, pm10 concentrations (µg/m³)\n",
    "- **✅ Success Metric**: How well calculated AQI matches actual AQI values\n",
    "\n",
    "## Feature Categories\n",
    "1. **Raw Air Quality**: pm2_5, pm10, co, no2, so2, o3, nh3\n",
    "2. **AQI Calculations**: pm2_5_aqi, pm10_aqi, us_aqi, openweather_aqi\n",
    "3. **Weather Data**: temperature, humidity, pressure, wind_speed, wind_direction\n",
    "4. **Time Features**: Cyclical encodings (hour, day, month, etc.)\n",
    "5. **Lag Features**: 1h-72h historical values\n",
    "6. **Rolling Statistics**: 3h-24h windows (mean, std, min, max)\n",
    "7. **Engineered Features**: Interactions, squared terms, categorical flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store that will be used for modeling.\n",
    "\n",
    "**EDA Focus**: Understanding relationships that help predict PM2.5 and PM10 concentrations accurately, which leads to better AQI predictions.\n",
    "\n",
    "## 1. Data Overview\n",
    "Loading and examining the basic structure of our modeling dataset from Hopsworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import configuration\n",
    "from config import HOPSWORKS_CONFIG\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks and load data\n",
    "print(\"Connecting to Hopsworks...\")\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_CONFIG[\"api_key\"], project=HOPSWORKS_CONFIG[\"project_name\"])\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Loading feature group data...\")\n",
    "fg = fs.get_feature_group(HOPSWORKS_CONFIG[\"feature_group_name\"], version=1)\n",
    "df = fg.read()\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} records from Hopsworks\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column references and prepare data\n",
    "# The actual timestamp column is called 'time' not 'timestamp'\n",
    "print(\"Data preparation and column check...\")\n",
    "print(f\"Time column: {'time' if 'time' in df.columns else 'timestamp not found'}\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "\n",
    "# Ensure time is datetime\n",
    "if df['time'].dtype == 'object':\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "print(\"✓ Data sorted by time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "print()\n",
    "\n",
    "print(\"COLUMNS:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:2d}. {col:<25} {str(df[col].dtype):<15}\")\n",
    "\n",
    "print()\n",
    "print(\"FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'time' in numeric_cols:\n",
    "    numeric_cols.remove('time')\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Datetime features ({len(datetime_cols)}): {datetime_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last few records\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 records:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\nRandom 5 records:\")\n",
    "display(df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric features\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nAIR QUALITY FEATURES SUMMARY:\")\n",
    "aqi_features = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "aqi_present = [col for col in aqi_features if col in df.columns]\n",
    "if aqi_present:\n",
    "    display(df[aqi_present].describe())\n",
    "\n",
    "print(\"\\nWEATHER FEATURES SUMMARY:\")\n",
    "weather_features = ['temperature', 'feels_like', 'humidity', 'pressure', 'visibility', 'wind_speed', 'wind_direction', 'cloud_cover']\n",
    "weather_present = [col for col in weather_features if col in df.columns]\n",
    "if weather_present:\n",
    "    display(df[weather_present].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis\n",
    "\n",
    "Analyzing temporal patterns in PM concentrations (our prediction targets) and derived AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plots for key air quality metrics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# PM2.5 over time\n",
    "axes[0].plot(df['time'], df['pm2_5'], alpha=0.7, color='red')\n",
    "axes[0].set_title('PM2.5 Concentration Over Time')\n",
    "axes[0].set_ylabel('PM2.5 (µg/m³)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PM10 over time  \n",
    "axes[1].plot(df['time'], df['pm10'], alpha=0.7, color='orange')\n",
    "axes[1].set_title('PM10 Concentration Over Time')\n",
    "axes[1].set_ylabel('PM10 (µg/m³)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# US AQI over time\n",
    "axes[2].plot(df['time'], df['us_aqi'], alpha=0.7, color='purple')\n",
    "axes[2].set_title('US AQI Over Time')\n",
    "axes[2].set_ylabel('US AQI')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI category colors as background\n",
    "aqi_levels = [\n",
    "    (0, 50, 'green', 'Good'),\n",
    "    (51, 100, 'yellow', 'Moderate'), \n",
    "    (101, 150, 'orange', 'Unhealthy for Sensitive'),\n",
    "    (151, 200, 'red', 'Unhealthy'),\n",
    "    (201, 300, 'purple', 'Very Unhealthy'),\n",
    "    (301, 500, 'maroon', 'Hazardous')\n",
    "]\n",
    "\n",
    "for min_val, max_val, color, label in aqi_levels:\n",
    "    axes[2].axhspan(min_val, max_val, alpha=0.1, color=color, label=label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series summary - ML TARGETS and GOAL METRIC\n",
    "print(f\"Time Series Summary (ML Targets + Goal Metric):\")\n",
    "print(f\"PM2.5 (ML Target): {df['pm2_5'].min():.1f} - {df['pm2_5'].max():.1f} µg/m³\")\n",
    "print(f\"PM10 (ML Target):  {df['pm10'].min():.1f} - {df['pm10'].max():.1f} µg/m³\") \n",
    "print(f\"US AQI (Goal Metric): {df['us_aqi'].min():.1f} - {df['us_aqi'].max():.1f}\")\n",
    "print(f\"\\nModeling Approach: Predict PM concentrations → Calculate AQI → Evaluate AQI accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis and Correlations\n",
    "\n",
    "Examining relationships between air quality and weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis - ML targets + environmental predictors\n",
    "key_features = [\n",
    "    'pm2_5', 'pm10',  # ML targets\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Weather predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Pollutant predictors\n",
    "]\n",
    "# Note: Excluding us_aqi since it's derived from PM targets\n",
    "\n",
    "# Filter features that exist in our dataset\n",
    "available_features = [col for col in key_features if col in df.columns]\n",
    "print(f\"Analyzing correlations for {len(available_features)} key features:\")\n",
    "print(available_features)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[available_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix (Key Variables)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on ML TARGETS (PM concentrations) - what we need to predict well\n",
    "print(\"\\nStrongest correlations with PM2.5 (ML Target):\")\n",
    "pm25_corr = corr_matrix['pm2_5'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm25_corr.head(8).items():\n",
    "    if feature != 'pm2_5':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nStrongest correlations with PM10 (ML Target):\")\n",
    "pm10_corr = corr_matrix['pm10'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm10_corr.head(8).items():\n",
    "    if feature != 'pm10':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n🎯 CORRELATION FOCUS:\")\n",
    "print(\"Understanding which environmental factors help predict PM concentrations accurately\")\n",
    "print(\"Better PM predictions → More accurate AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ML TARGETS (PM concentrations) - what affects our predictions\n",
    "print(\"=\"*60)\n",
    "print(\"ML TARGET CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_features = ['pm2_5', 'pm10']\n",
    "predictor_features = ['temperature', 'humidity', 'pressure', 'wind_speed', \n",
    "                     'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "\n",
    "available_predictors = [col for col in predictor_features if col in df.columns]\n",
    "\n",
    "for target in target_features:\n",
    "    if target in df.columns:\n",
    "        print(f\"\\nStrongest correlations with {target.upper()} (Target Variable):\")\n",
    "        target_corr = df[[target] + available_predictors].corr()[target].abs().sort_values(ascending=False)\n",
    "        for feature, corr in target_corr.head(6).items():\n",
    "            if feature != target:\n",
    "                print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n[EDA Focus: Understanding what predicts PM concentrations well]\")\n",
    "print(f\"[Goal: Better PM predictions → More accurate AQI calculations]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AQI Dominance Analysis\n",
    "\n",
    "**Key Question**: Which pollutants actually drive AQI values? \n",
    "\n",
    "Since EPA AQI = MAX(individual pollutant AQIs), we need to check if other criteria pollutants sometimes create higher AQI than PM2.5/PM10. This validates our modeling approach of using only PM concentrations.\n",
    "\n",
    "**EPA Criteria Pollutants Analyzed**: PM2.5, PM10, O3, CO, NO2, SO2  \n",
    "*(Only these 6 pollutants have official EPA AQI breakpoints and affect AQI calculations)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA AQI Calculation Functions for All Pollutants\n",
    "def calculate_aqi_from_concentration(concentration, breakpoints):\n",
    "    \"\"\"Calculate AQI from pollutant concentration using EPA breakpoints\"\"\"\n",
    "    if pd.isna(concentration) or concentration < 0:\n",
    "        return 0\n",
    "    \n",
    "    for i, (c_low, c_high, aqi_low, aqi_high) in enumerate(breakpoints):\n",
    "        if c_low <= concentration <= c_high:\n",
    "            # Linear interpolation within the bracket\n",
    "            aqi = ((aqi_high - aqi_low) / (c_high - c_low)) * (concentration - c_low) + aqi_low\n",
    "            return round(aqi)\n",
    "    \n",
    "    # If concentration exceeds highest breakpoint, use hazardous level\n",
    "    return 500\n",
    "\n",
    "# EPA AQI Breakpoints (concentration ranges and corresponding AQI ranges)\n",
    "EPA_BREAKPOINTS = {\n",
    "    'pm2_5': [  # µg/m³, 24-hour average\n",
    "        (0.0, 12.0, 0, 50),\n",
    "        (12.1, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 500.4, 301, 500)\n",
    "    ],\n",
    "    'pm10': [  # µg/m³, 24-hour average  \n",
    "        (0, 54, 0, 50),\n",
    "        (55, 154, 51, 100),\n",
    "        (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200),\n",
    "        (355, 424, 201, 300),\n",
    "        (425, 604, 301, 500)\n",
    "    ],\n",
    "    'ozone': [  # ppb, 8-hour average (converting from µg/m³ if needed)\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 70, 51, 100),\n",
    "        (71, 85, 101, 150),\n",
    "        (86, 105, 151, 200),\n",
    "        (106, 200, 201, 300)\n",
    "    ],\n",
    "    'carbon_monoxide': [  # ppm, 8-hour average\n",
    "        (0.0, 4.4, 0, 50),\n",
    "        (4.5, 9.4, 51, 100),\n",
    "        (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200),\n",
    "        (15.5, 30.4, 201, 300),\n",
    "        (30.5, 50.4, 301, 500)\n",
    "    ],\n",
    "    'nitrogen_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 53, 0, 50),\n",
    "        (54, 100, 51, 100),\n",
    "        (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200),\n",
    "        (650, 1249, 201, 300),\n",
    "        (1250, 2049, 301, 500)\n",
    "    ],\n",
    "    'sulphur_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 35, 0, 50),\n",
    "        (36, 75, 51, 100),\n",
    "        (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200),\n",
    "        (305, 604, 201, 300),\n",
    "        (605, 1004, 301, 500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"EPA AQI Calculation Functions Loaded\")\n",
    "print(f\"EPA Criteria Pollutants (official breakpoints): {list(EPA_BREAKPOINTS.keys())}\")\n",
    "\n",
    "# Check which EPA criteria pollutants we have in our data\n",
    "available_pollutants = []\n",
    "for pollutant in EPA_BREAKPOINTS.keys():\n",
    "    if pollutant in df.columns:\n",
    "        available_pollutants.append(pollutant)\n",
    "        \n",
    "print(f\"\\nEPA criteria pollutants in our dataset: {available_pollutants}\")\n",
    "print(f\"Missing from dataset: {[p for p in EPA_BREAKPOINTS.keys() if p not in df.columns]}\")\n",
    "print(f\"\\nAnalyzing {len(available_pollutants)} pollutants that can affect AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual AQI for each available pollutant\n",
    "aqi_results = df[['time', 'us_aqi']].copy()\n",
    "\n",
    "# Unit conversions (if needed)\n",
    "df_calc = df.copy()\n",
    "\n",
    "# Convert units for certain pollutants if needed\n",
    "# Ozone: µg/m³ to ppb (approximate: µg/m³ * 0.5 ≈ ppb at standard conditions)\n",
    "if 'ozone' in df_calc.columns:\n",
    "    df_calc['ozone_ppb'] = df_calc['ozone'] * 0.5  # Rough conversion\n",
    "    \n",
    "# CO might need conversion from µg/m³ to ppm\n",
    "if 'carbon_monoxide' in df_calc.columns:\n",
    "    df_calc['co_ppm'] = df_calc['carbon_monoxide'] * 0.000873  # Rough conversion\n",
    "\n",
    "print(\"Calculating individual AQI for each pollutant...\")\n",
    "\n",
    "# Calculate AQI for each pollutant\n",
    "for pollutant in available_pollutants:\n",
    "    col_name = f'{pollutant}_individual_aqi'\n",
    "    \n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        concentrations = df_calc['ozone_ppb']\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        concentrations = df_calc['co_ppm']\n",
    "    else:\n",
    "        concentrations = df_calc[pollutant]\n",
    "    \n",
    "    aqi_results[col_name] = concentrations.apply(\n",
    "        lambda x: calculate_aqi_from_concentration(x, EPA_BREAKPOINTS[pollutant])\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ {pollutant}: {aqi_results[col_name].min():.0f} - {aqi_results[col_name].max():.0f} AQI\")\n",
    "\n",
    "# Find controlling pollutant (max AQI) for each timestamp\n",
    "individual_aqi_cols = [col for col in aqi_results.columns if 'individual_aqi' in col]\n",
    "aqi_results['calculated_max_aqi'] = aqi_results[individual_aqi_cols].max(axis=1)\n",
    "aqi_results['controlling_pollutant'] = aqi_results[individual_aqi_cols].idxmax(axis=1)\n",
    "\n",
    "# Clean up pollutant names\n",
    "aqi_results['controlling_pollutant'] = aqi_results['controlling_pollutant'].str.replace('_individual_aqi', '')\n",
    "\n",
    "print(f\"\\nCalculated AQI range: {aqi_results['calculated_max_aqi'].min():.0f} - {aqi_results['calculated_max_aqi'].max():.0f}\")\n",
    "print(f\"Current US AQI range: {aqi_results['us_aqi'].min():.0f} - {aqi_results['us_aqi'].max():.0f}\")\n",
    "print(f\"Difference: {(aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']).describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 1: Pollutant Dominance Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"POLLUTANT DOMINANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count which pollutant controls AQI most often\n",
    "dominance_counts = aqi_results['controlling_pollutant'].value_counts()\n",
    "dominance_pct = (dominance_counts / len(aqi_results) * 100).round(1)\n",
    "\n",
    "print(\"Controlling Pollutant Frequency:\")\n",
    "for pollutant, count in dominance_counts.items():\n",
    "    pct = dominance_pct[pollutant]\n",
    "    print(f\"  {pollutant:<15}: {count:3d} times ({pct:5.1f}%)\")\n",
    "\n",
    "# Pie chart of dominance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(dominance_counts.values, labels=dominance_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Which Pollutant Controls AQI?')\n",
    "\n",
    "# Bar chart for clearer comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(range(len(dominance_counts)), dominance_counts.values, \n",
    "               color=['red' if x in ['pm2_5', 'pm10'] else 'lightcoral' for x in dominance_counts.index])\n",
    "plt.xticks(range(len(dominance_counts)), [p.replace('_', ' ').title() for p in dominance_counts.index], rotation=45)\n",
    "plt.ylabel('Number of Hours')\n",
    "plt.title('Controlling Pollutant Frequency')\n",
    "\n",
    "# Highlight PM2.5 and PM10\n",
    "for i, (pollutant, count) in enumerate(dominance_counts.items()):\n",
    "    color = 'white' if pollutant in ['pm2_5', 'pm10'] else 'black'\n",
    "    plt.text(i, count + 5, f'{count}', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of PM dominance\n",
    "pm_dominance = dominance_counts.get('pm2_5', 0) + dominance_counts.get('pm10', 0)\n",
    "pm_percentage = (pm_dominance / len(aqi_results) * 100)\n",
    "\n",
    "print(f\"\\n🎯 KEY FINDING:\")\n",
    "print(f\"PM2.5 + PM10 control AQI {pm_dominance}/{len(aqi_results)} times ({pm_percentage:.1f}%)\")\n",
    "print(f\"Other pollutants control AQI {len(aqi_results) - pm_dominance}/{len(aqi_results)} times ({100-pm_percentage:.1f}%)\")\n",
    "\n",
    "if pm_percentage >= 80:\n",
    "    print(\"✅ Current PM-only approach captures most AQI variations\")\n",
    "else:\n",
    "    print(\"⚠️  Consider including other pollutants in modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 2: Concentration vs AQI Relationships\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCENTRATION vs AQI ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create scatter plots for EPA criteria pollutants only\n",
    "num_plots = len(available_pollutants)\n",
    "cols = 3\n",
    "rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get concentrations and individual AQI\n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        conc = df_calc['ozone_ppb']\n",
    "        unit = 'ppb'\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        conc = df_calc['co_ppm']\n",
    "        unit = 'ppm'\n",
    "    else:\n",
    "        conc = df_calc[pollutant]\n",
    "        unit = 'µg/m³'\n",
    "    \n",
    "    # All pollutants here are EPA criteria pollutants with AQI calculations\n",
    "    individual_aqi = aqi_results[f'{pollutant}_individual_aqi']\n",
    "    \n",
    "    # Scatter plot with AQI color coding\n",
    "    scatter = ax.scatter(conc, individual_aqi, alpha=0.6, s=20, \n",
    "                        c=individual_aqi, cmap='RdYlGn_r', vmin=0, vmax=150)\n",
    "    \n",
    "    ax.set_xlabel(f'{pollutant.replace(\"_\", \" \").title()} ({unit})')\n",
    "    ax.set_ylabel('Individual AQI')\n",
    "    ax.set_title(f'{pollutant.replace(\"_\", \" \").title()} → AQI')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AQI level lines\n",
    "    ax.axhline(y=50, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=100, color='yellow', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=150, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(available_pollutants), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for EPA criteria pollutants\n",
    "print(\"\\nIndividual AQI Statistics (EPA Criteria Pollutants):\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    stats = aqi_results[aqi_col].describe()\n",
    "    print(f\"{pollutant:<15}: Mean={stats['mean']:5.1f}, Max={stats['max']:5.1f}, >100: {(aqi_results[aqi_col] > 100).sum():3d} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3A: Individual AQI Contributions vs Maximum AQI\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL POLLUTANT AQI CONTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time series showing all individual AQIs vs the maximum\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All individual AQIs over time\n",
    "time_vals = aqi_results['time']\n",
    "max_aqi_vals = aqi_results['calculated_max_aqi']\n",
    "\n",
    "# Plot each individual AQI\n",
    "colors = ['red', 'darkred', 'blue', 'purple', 'orange', 'green']\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        ax1.plot(time_vals, aqi_results[aqi_col], \n",
    "                label=pollutant.replace('_', ' ').title(), \n",
    "                alpha=0.7, linewidth=1.5, color=colors[i % len(colors)])\n",
    "\n",
    "# Plot maximum AQI as thick black line\n",
    "ax1.plot(time_vals, max_aqi_vals, \n",
    "         label='Maximum AQI (Envelope)', \n",
    "         color='black', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('Individual Pollutant AQIs vs Maximum AQI Over Time')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Distribution of individual AQIs \n",
    "aqi_data = []\n",
    "pollutant_names = []\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        aqi_data.append(aqi_results[aqi_col].values)\n",
    "        pollutant_names.append(pollutant.replace('_', ' ').title())\n",
    "\n",
    "# Add maximum AQI for comparison\n",
    "aqi_data.append(max_aqi_vals.values)\n",
    "pollutant_names.append('Maximum AQI')\n",
    "\n",
    "# Create box plot\n",
    "bp = ax2.boxplot(aqi_data, labels=pollutant_names, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "box_colors = colors + ['black']\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('AQI Value')\n",
    "ax2.set_title('AQI Distribution by Pollutant')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level lines\n",
    "for level, color in [(50, 'green'), (100, 'yellow'), (150, 'orange')]:\n",
    "    ax2.axhline(y=level, color=color, linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for contribution analysis\n",
    "print(\"\\nIndividual AQI vs Maximum AQI Analysis:\")\n",
    "max_aqi_mean = max_aqi_vals.mean()\n",
    "print(f\"Maximum AQI - Mean: {max_aqi_mean:.1f}, Range: {max_aqi_vals.min():.0f}-{max_aqi_vals.max():.0f}\")\n",
    "\n",
    "print(\"\\nHow often each pollutant reaches within 90% of maximum AQI:\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        close_to_max = (aqi_results[aqi_col] >= 0.9 * max_aqi_vals).sum()\n",
    "        percentage = (close_to_max / len(aqi_results) * 100)\n",
    "        print(f\"  {pollutant:<15}: {close_to_max:3d}/{len(aqi_results)} times ({percentage:5.1f}%) within 90% of max\")\n",
    "\n",
    "print(\"\\nThis shows which pollutants are the 'runners-up' when they don't control AQI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3: Time Series of Controlling Pollutants\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME SERIES OF AQI CONTROL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a time series showing which pollutant controls AQI\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: AQI comparison over time\n",
    "ax1.plot(aqi_results['time'], aqi_results['us_aqi'], label='Current US AQI (PM only)', alpha=0.7, linewidth=2)\n",
    "ax1.plot(aqi_results['time'], aqi_results['calculated_max_aqi'], label='True Max AQI (All pollutants)', alpha=0.7, linewidth=2)\n",
    "ax1.fill_between(aqi_results['time'], aqi_results['us_aqi'], aqi_results['calculated_max_aqi'], \n",
    "                 alpha=0.3, color='red', label='Missing AQI')\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('AQI Comparison: Current PM-only vs Full EPA Calculation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Controlling pollutant over time\n",
    "pollutant_colors = {\n",
    "    'pm2_5': 'red', 'pm10': 'darkred', 'ozone': 'blue', \n",
    "    'carbon_monoxide': 'purple', 'nitrogen_dioxide': 'orange', 'sulphur_dioxide': 'green'\n",
    "}\n",
    "\n",
    "# Create numerical encoding for pollutants for plotting\n",
    "unique_pollutants = aqi_results['controlling_pollutant'].unique()\n",
    "pollutant_mapping = {p: i for i, p in enumerate(unique_pollutants)}\n",
    "aqi_results['pollutant_num'] = aqi_results['controlling_pollutant'].map(pollutant_mapping)\n",
    "\n",
    "scatter = ax2.scatter(aqi_results['time'], aqi_results['pollutant_num'], \n",
    "                     c=[pollutant_colors.get(p, 'gray') for p in aqi_results['controlling_pollutant']], \n",
    "                     alpha=0.7, s=20)\n",
    "\n",
    "ax2.set_ylabel('Controlling Pollutant')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Which Pollutant Controls AQI Over Time')\n",
    "ax2.set_yticks(range(len(unique_pollutants)))\n",
    "ax2.set_yticklabels([p.replace('_', ' ').title() for p in unique_pollutants])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate impact of missing other pollutants\n",
    "aqi_difference = aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']\n",
    "significant_underestimation = (aqi_difference > 10).sum()\n",
    "\n",
    "print(f\"\\n📊 IMPACT ANALYSIS:\")\n",
    "print(f\"Times when PM-only AQI underestimates by >10 points: {significant_underestimation}/{len(aqi_results)} ({significant_underestimation/len(aqi_results)*100:.1f}%)\")\n",
    "print(f\"Maximum underestimation: {aqi_difference.max():.1f} AQI points\")\n",
    "print(f\"Average difference: {aqi_difference.mean():.1f} AQI points\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n🎯 MODELING RECOMMENDATION:\")\n",
    "if pm_percentage >= 85:\n",
    "    print(\"✅ PM-only approach is SUFFICIENT for Multan AQI prediction\")\n",
    "    print(\"   PM2.5 + PM10 control >85% of AQI variations\")\n",
    "elif pm_percentage >= 70:\n",
    "    print(\"⚠️  PM-only approach is MOSTLY adequate but consider monitoring other pollutants\")\n",
    "    print(\"   PM2.5 + PM10 control 70-85% of AQI variations\")\n",
    "else:\n",
    "    print(\"🚨 PM-only approach MISSES significant AQI drivers\")\n",
    "    print(\"   Consider including other pollutants in prediction model\")\n",
    "\n",
    "print(f\"\\nCurrent focus on PM2.5 and PM10 captures {pm_percentage:.1f}% of AQI control instances.\")\n",
    "\n",
    "print(f\"\\n🎯 FINAL ANALYSIS:\")\n",
    "print(f\"✓ Analyzed all {len(available_pollutants)} EPA criteria pollutants in dataset\")\n",
    "print(f\"✓ PM2.5 and PM10 are responsible for {pm_percentage:.1f}% of AQI determinations\")\n",
    "print(f\"✓ Other criteria pollutants (O3, CO, NO2, SO2) control {100-pm_percentage:.1f}% of AQI\")\n",
    "print(f\"✓ This validates the scope of your PM-focused modeling approach\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lag Features Analysis\n",
    "\n",
    "Examining the importance of historical PM values for predicting current concentrations (which leads to better AQI calculations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features for BOTH ML targets (PM2.5 AND PM10)\n",
    "print(\"=\"*60)\n",
    "print(\"LAG FEATURES ANALYSIS FOR ML TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM10 Lag Analysis (since it's also a target)\n",
    "pm10_lag_features = [col for col in df.columns if 'lag' in col and 'pm10' in col]\n",
    "if pm10_lag_features:\n",
    "    print(f\"\\nFound {len(pm10_lag_features)} PM10 lag features:\")\n",
    "    print(pm10_lag_features[:5], \"...\" if len(pm10_lag_features) > 5 else \"\")\n",
    "    \n",
    "    # Calculate correlations between current PM10 and its lag features\n",
    "    pm10_lag_correlations = df[['pm10'] + pm10_lag_features].corr()['pm10'].drop('pm10')\n",
    "    \n",
    "    # Plot PM10 lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "    pm10_correlations = [pm10_lag_correlations[f'pm10_lag_{h}h'] for h in lag_hours if f'pm10_lag_{h}h' in pm10_lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(pm10_correlations)), pm10_correlations, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM10')\n",
    "    plt.title('PM10 Lag Features Correlation (ML Target)')\n",
    "    plt.xticks(range(len(pm10_correlations)), [f'{h}h' for h in lag_hours[:len(pm10_correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(pm10_correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPM10 lag feature correlations (ML Target):\")\n",
    "    for i, h in enumerate(lag_hours[:len(pm10_correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {pm10_correlations[i]:.3f}\")\n",
    "\n",
    "print(\"\\n[ML Targets: PM2.5 & PM10 concentrations]\")\n",
    "print(\"[Ultimate Goal: Accurate AQI predictions for Multan]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features correlation with current PM2.5\n",
    "lag_features = [col for col in df.columns if 'lag' in col and 'pm2_5' in col]\n",
    "print(f\"Found {len(lag_features)} PM2.5 lag features:\")\n",
    "print(lag_features)\n",
    "\n",
    "if lag_features:\n",
    "    # Calculate correlations between current PM2.5 and its lag features\n",
    "    lag_correlations = df[['pm2_5'] + lag_features].corr()['pm2_5'].drop('pm2_5')\n",
    "    \n",
    "    # Plot lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]  # Expected lag hours\n",
    "    correlations = [lag_correlations[f'pm2_5_lag_{h}h'] for h in lag_hours if f'pm2_5_lag_{h}h' in lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(correlations)), correlations, alpha=0.7)\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM2.5')\n",
    "    plt.title('PM2.5 Lag Features Correlation')\n",
    "    plt.xticks(range(len(correlations)), [f'{h}h' for h in lag_hours[:len(correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLag feature correlations with current PM2.5:\")\n",
    "    for i, h in enumerate(lag_hours[:len(correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {correlations[i]:.3f}\")\n",
    "\n",
    "# Complete Rolling Statistics Analysis for Both ML Targets\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ROLLING STATISTICS ANALYSIS (ML TARGETS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM2.5 Rolling Features Analysis\n",
    "pm25_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm2_5' in col]\n",
    "if pm25_rolling_features:\n",
    "    print(f\"\\nPM2.5 ROLLING FEATURES ({len(pm25_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    rolling_types = ['mean', 'std', 'min', 'max']\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm25_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm2_5'] + stat_features].corr()['pm2_5'].drop('pm2_5')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# PM10 Rolling Features Analysis  \n",
    "pm10_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm10' in col]\n",
    "if pm10_rolling_features:\n",
    "    print(f\"\\nPM10 ROLLING FEATURES ({len(pm10_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm10_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm10'] + stat_features].corr()['pm10'].drop('pm10')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# Change Rate Features Analysis\n",
    "print(f\"\\nCHANGE RATE FEATURES:\")\n",
    "change_features = [col for col in df.columns if 'change_rate' in col]\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    target_change_features = [col for col in change_features if target in col]\n",
    "    if target_change_features:\n",
    "        print(f\"\\n  {target.upper()} change rates:\")\n",
    "        correlations = df[[target] + target_change_features].corr()[target].drop(target)\n",
    "        for feature, corr in correlations.items():\n",
    "            period = feature.split('_')[-1]\n",
    "            print(f\"    {period:<4} change: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"• Rolling features capture trend information over different time windows\")\n",
    "print(f\"• Shorter windows (3h, 6h) typically correlate more strongly with current values\")\n",
    "print(f\"• Change rates show how much PM concentrations are shifting\")\n",
    "print(f\"• These features help models understand pollution persistence and trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Analysis\n",
    "\n",
    "**Focus**: Ensuring data reliability for accurate PM concentration predictions and AQI calculations.\n",
    "\n",
    "### 6.1 Missing Values & Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Missing Values & Null Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES & NULL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic missing value check\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent\n",
    "}).round(2)\n",
    "\n",
    "print(\"EXPLICIT NULL VALUES:\")\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"✓ No explicit null values found!\")\n",
    "else:\n",
    "    print(\"Missing values found:\")\n",
    "    for col in missing_df[missing_df['Missing_Count'] > 0].index:\n",
    "        count = missing_df.loc[col, 'Missing_Count']\n",
    "        percent = missing_df.loc[col, 'Missing_Percentage']\n",
    "        print(f\"  {col:<25} {count:>6} ({percent:>6.2f}%)\")\n",
    "\n",
    "# Check for zero values that might represent missing data\n",
    "print(f\"\\nZERO VALUES ANALYSIS (Potential Missing Data):\")\n",
    "air_quality_cols = ['pm2_5', 'pm10', 'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "zero_issues = {}\n",
    "\n",
    "for col in air_quality_cols:\n",
    "    if col in df.columns:\n",
    "        zero_count = (df[col] == 0.0).sum()\n",
    "        zero_percent = (zero_count / len(df)) * 100\n",
    "        if zero_count > 0:\n",
    "            zero_issues[col] = {'count': zero_count, 'percent': zero_percent}\n",
    "\n",
    "if zero_issues:\n",
    "    print(\"⚠️  ZERO VALUES FOUND (may indicate missing sensors):\")\n",
    "    for col, stats in zero_issues.items():\n",
    "        print(f\"  {col:<20}: {stats['count']:>3d} zeros ({stats['percent']:>5.1f}%)\")\n",
    "        \n",
    "    # Visualize zero patterns over time\n",
    "    fig, axes = plt.subplots(len(zero_issues), 1, figsize=(15, 3*len(zero_issues)))\n",
    "    if len(zero_issues) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (col, stats) in enumerate(zero_issues.items()):\n",
    "        zero_mask = df[col] == 0.0\n",
    "        axes[i].scatter(df[zero_mask]['time'], [col]*zero_mask.sum(), \n",
    "                       alpha=0.7, color='red', s=20, label=f'Zero values ({stats[\"count\"]})')\n",
    "        axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} - Zero Value Timeline')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No zero values in air quality parameters\")\n",
    "\n",
    "# Check for consecutive missing periods (data gaps)\n",
    "print(f\"\\nTIME GAP ANALYSIS:\")\n",
    "df_sorted = df.sort_values('time').reset_index(drop=True)\n",
    "time_diffs = df_sorted['time'].diff()\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]\n",
    "\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"⚠️  Found {len(large_gaps)} time gaps > 2 hours:\")\n",
    "    for idx, gap in large_gaps.items():\n",
    "        gap_start = df_sorted.loc[idx-1, 'time'] if idx > 0 else 'Start'\n",
    "        gap_end = df_sorted.loc[idx, 'time']\n",
    "        print(f\"  Gap: {gap} between {gap_start} and {gap_end}\")\n",
    "else:\n",
    "    print(\"✓ No significant time gaps found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Consistency & Physics Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Data Consistency & Physics Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CONSISTENCY & PHYSICS VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PM2.5 vs PM10 relationship (PM2.5 should generally be ≤ PM10)\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = df[df['pm2_5'] > df['pm10']]\n",
    "    violation_count = len(pm_violations)\n",
    "    violation_percent = (violation_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"PM2.5 vs PM10 CONSISTENCY:\")\n",
    "    if violation_count > 0:\n",
    "        print(f\"⚠️  {violation_count} records ({violation_percent:.1f}%) where PM2.5 > PM10\")\n",
    "        print(f\"   Max violation: PM2.5={pm_violations['pm2_5'].max():.1f} > PM10={pm_violations['pm10'].max():.1f}\")\n",
    "        \n",
    "        # Show violation timeline\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(df['time'], df['pm2_5'], alpha=0.5, label='PM2.5', s=10)\n",
    "        plt.scatter(df['time'], df['pm10'], alpha=0.5, label='PM10', s=10)\n",
    "        plt.scatter(pm_violations['time'], pm_violations['pm2_5'], \n",
    "                   color='red', label=f'PM2.5 > PM10 ({violation_count} cases)', s=30, marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (µg/m³)')\n",
    "        plt.title('PM2.5 vs PM10 Consistency Check')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"✓ PM2.5 ≤ PM10 in all records (physically consistent)\")\n",
    "\n",
    "# Check reasonable ranges for weather parameters\n",
    "print(f\"\\nWEATHER PARAMETER VALIDATION:\")\n",
    "weather_ranges = {\n",
    "    'temperature': (-50, 60, '°C'),  # Extreme but possible range\n",
    "    'humidity': (0, 100, '%'),       # Should be 0-100%\n",
    "    'pressure': (800, 1200, 'hPa'),  # Reasonable atmospheric pressure\n",
    "    'wind_speed': (0, 50, 'm/s'),    # Reasonable wind speeds\n",
    "    'wind_direction': (0, 360, '°')  # Should be 0-360 degrees\n",
    "}\n",
    "\n",
    "weather_violations = {}\n",
    "for param, (min_val, max_val, unit) in weather_ranges.items():\n",
    "    if param in df.columns:\n",
    "        below_min = (df[param] < min_val).sum()\n",
    "        above_max = (df[param] > max_val).sum()\n",
    "        violations = below_min + above_max\n",
    "        \n",
    "        if violations > 0:\n",
    "            weather_violations[param] = {\n",
    "                'below_min': below_min, 'above_max': above_max, \n",
    "                'min_val': min_val, 'max_val': max_val, 'unit': unit,\n",
    "                'actual_min': df[param].min(), 'actual_max': df[param].max()\n",
    "            }\n",
    "\n",
    "if weather_violations:\n",
    "    print(\"⚠️  WEATHER PARAMETER VIOLATIONS:\")\n",
    "    for param, stats in weather_violations.items():\n",
    "        print(f\"  {param:<15}: {stats['below_min']} below {stats['min_val']}{stats['unit']}, \"\n",
    "              f\"{stats['above_max']} above {stats['max_val']}{stats['unit']}\")\n",
    "        print(f\"                   Actual range: {stats['actual_min']:.1f} - {stats['actual_max']:.1f}{stats['unit']}\")\n",
    "else:\n",
    "    print(\"✓ All weather parameters within reasonable ranges\")\n",
    "\n",
    "# Check cyclical feature ranges (sin/cos should be in [-1, 1])\n",
    "print(f\"\\nCYCLICAL FEATURE VALIDATION:\")\n",
    "cyclical_cols = [col for col in df.columns if '_sin' in col or '_cos' in col]\n",
    "cyclical_violations = {}\n",
    "\n",
    "for col in cyclical_cols:\n",
    "    below_neg1 = (df[col] < -1.01).sum()  # Small tolerance for floating point\n",
    "    above_pos1 = (df[col] > 1.01).sum()\n",
    "    violations = below_neg1 + above_pos1\n",
    "    \n",
    "    if violations > 0:\n",
    "        cyclical_violations[col] = {\n",
    "            'below_neg1': below_neg1, 'above_pos1': above_pos1,\n",
    "            'actual_min': df[col].min(), 'actual_max': df[col].max()\n",
    "        }\n",
    "\n",
    "if cyclical_violations:\n",
    "    print(\"⚠️  CYCLICAL FEATURE VIOLATIONS:\")\n",
    "    for col, stats in cyclical_violations.items():\n",
    "        print(f\"  {col:<20}: {stats['below_neg1']} below -1, {stats['above_pos1']} above 1\")\n",
    "        print(f\"                       Actual range: {stats['actual_min']:.6f} - {stats['actual_max']:.6f}\")\n",
    "else:\n",
    "    print(\"✓ All cyclical features within [-1, 1] range\")\n",
    "\n",
    "# Check binary flag consistency\n",
    "print(f\"\\nBINARY FLAG VALIDATION:\")\n",
    "binary_cols = [col for col in df.columns if col.startswith('is_')]\n",
    "binary_violations = {}\n",
    "\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    expected_vals = {0.0, 1.0}\n",
    "    unexpected_vals = set(unique_vals) - expected_vals\n",
    "    \n",
    "    if unexpected_vals:\n",
    "        binary_violations[col] = {\n",
    "            'unexpected': list(unexpected_vals),\n",
    "            'unique_vals': list(unique_vals)\n",
    "        }\n",
    "\n",
    "if binary_violations:\n",
    "    print(\"⚠️  BINARY FLAG VIOLATIONS:\")\n",
    "    for col, stats in binary_violations.items():\n",
    "        print(f\"  {col:<20}: Found {stats['unexpected']} (expected only 0.0, 1.0)\")\n",
    "        print(f\"                       All values: {stats['unique_vals']}\")\n",
    "else:\n",
    "    print(\"✓ All binary flags contain only 0.0 and 1.0 values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Statistical Outlier Detection\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and key environmental features\n",
    "key_features_for_outliers = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for model quality\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Air quality predictors\n",
    "]\n",
    "\n",
    "available_outlier_features = [col for col in key_features_for_outliers if col in df.columns]\n",
    "\n",
    "# Z-score method (|z| > 3 considered outlier)\n",
    "print(\"Z-SCORE OUTLIER DETECTION (|z-score| > 3):\")\n",
    "zscore_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask = z_scores > 3\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        zscore_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'max_zscore': z_scores.max(),\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if zscore_outliers:\n",
    "    print(\"⚠️  Z-SCORE OUTLIERS FOUND:\")\n",
    "    for col, stats in zscore_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%) - Max |z|: {stats['max_zscore']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            print(f\"                       Values: {sorted(stats['outlier_values'])[:5]}...\" if len(stats['outlier_values']) > 5 else f\"                       Values: {sorted(stats['outlier_values'])}\")\n",
    "else:\n",
    "    print(\"✓ No z-score outliers found (|z| > 3)\")\n",
    "\n",
    "# IQR method (values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
    "print(f\"\\nIQR OUTLIER DETECTION (beyond Q1-1.5*IQR, Q3+1.5*IQR):\")\n",
    "iqr_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        iqr_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if iqr_outliers:\n",
    "    print(\"⚠️  IQR OUTLIERS FOUND:\")\n",
    "    for col, stats in iqr_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%)\")\n",
    "        print(f\"                       Expected range: {stats['lower_bound']:.2f} - {stats['upper_bound']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            extreme_values = sorted(stats['outlier_values'])\n",
    "            display_values = extreme_values[:3] + ['...'] + extreme_values[-2:] if len(extreme_values) > 5 else extreme_values\n",
    "            print(f\"                       Outlier values: {display_values}\")\n",
    "else:\n",
    "    print(\"✓ No IQR outliers found\")\n",
    "\n",
    "# Visualize outliers for ML targets\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_present = [col for col in ml_targets if col in df.columns]\n",
    "\n",
    "if ml_targets_present and (zscore_outliers or iqr_outliers):\n",
    "    print(f\"\\nOUTLIER VISUALIZATION FOR ML TARGETS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_present), 2, figsize=(15, 5*len(ml_targets_present)))\n",
    "    if len(ml_targets_present) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_present):\n",
    "        # Box plot\n",
    "        ax1 = axes[i, 0]\n",
    "        bp = ax1.boxplot([df[target].dropna()], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax1.set_title(f'{target.upper()} - Box Plot (IQR Outliers)')\n",
    "        ax1.set_ylabel(f'{target.replace(\"_\", \".\")} (µg/m³)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series with outliers highlighted\n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(df['time'], df[target], alpha=0.7, linewidth=1, label=target.upper())\n",
    "        \n",
    "        # Highlight outliers\n",
    "        if target in zscore_outliers:\n",
    "            z_scores = np.abs((df[target] - df[target].mean()) / df[target].std())\n",
    "            zscore_mask = z_scores > 3\n",
    "            ax2.scatter(df[zscore_mask]['time'], df.loc[zscore_mask, target], \n",
    "                       color='red', s=30, marker='x', label=f'Z-score outliers ({zscore_outliers[target][\"count\"]})')\n",
    "        \n",
    "        if target in iqr_outliers:\n",
    "            Q1 = df[target].quantile(0.25)\n",
    "            Q3 = df[target].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_mask = (df[target] < lower_bound) | (df[target] > upper_bound)\n",
    "            ax2.scatter(df[iqr_mask]['time'], df.loc[iqr_mask, target], \n",
    "                       color='orange', s=20, marker='o', alpha=0.7, label=f'IQR outliers ({iqr_outliers[target][\"count\"]})')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Time Series with Outliers')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(f'{target.replace(\"_\", \".\")} (µg/m³)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary of outlier impact on ML targets\n",
    "print(f\"\\n🎯 OUTLIER IMPACT ON ML TARGETS:\")\n",
    "for target in ml_targets_present:\n",
    "    total_outliers = 0\n",
    "    if target in zscore_outliers:\n",
    "        total_outliers += zscore_outliers[target]['count']\n",
    "    if target in iqr_outliers:\n",
    "        total_outliers += iqr_outliers[target]['count']  # Note: may overlap with z-score\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        outlier_percent = (total_outliers / len(df)) * 100\n",
    "        print(f\"  {target.upper()}: ~{total_outliers} potential outliers ({outlier_percent:.1f}% of data)\")\n",
    "        print(f\"           Consider: Review for sensor errors vs genuine extreme pollution events\")\n",
    "    else:\n",
    "        print(f\"  {target.upper()}: No significant outliers detected\")\n",
    "\n",
    "print(f\"\\n[Quality Note: Outliers in PM concentrations could be genuine pollution spikes or sensor malfunctions]\")\n",
    "print(f\"[Recommendation: Investigate extreme values before removal - they might be real air quality events]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Data Completeness & Summary Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Data Completeness & Summary Quality Report\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA COMPLETENESS & SUMMARY QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"DUPLICATE RECORDS: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"⚠️  Duplicate rows found - consider deduplication\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows\")\n",
    "\n",
    "# Unique values analysis\n",
    "print(f\"\\nUNIQUE VALUES ANALYSIS:\")\n",
    "feature_categories = {\n",
    "    'ML Targets': ['pm2_5', 'pm10'],\n",
    "    'Air Quality': ['carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide', 'nh3'],\n",
    "    'AQI Calculations': ['pm2_5_aqi', 'pm10_aqi', 'us_aqi', 'openweather_aqi'],\n",
    "    'Weather': ['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction'],\n",
    "    'Time Features': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'],\n",
    "    'Binary Flags': [col for col in df.columns if col.startswith('is_')],\n",
    "    'Lag Features': [col for col in df.columns if 'lag' in col][:5],  # Show first 5\n",
    "    'Rolling Features': [col for col in df.columns if 'rolling' in col][:5]  # Show first 5\n",
    "}\n",
    "\n",
    "for category, cols in feature_categories.items():\n",
    "    available_cols = [col for col in cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for col in available_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            unique_ratio = unique_count / len(df)\n",
    "            if unique_ratio < 0.01:  # Very low uniqueness\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ⚠️  Low variation\")\n",
    "            elif unique_ratio > 0.95:  # Very high uniqueness  \n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ✓ High variation\")\n",
    "            else:\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%})\")\n",
    "\n",
    "# Data quality scoring\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL DATA QUALITY SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_score = 100  # Start with perfect score\n",
    "quality_issues = []\n",
    "\n",
    "# Deduct points for various issues\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent_total = (missing_data.sum() / (len(df) * len(df.columns))) * 100\n",
    "    deduction = min(20, missing_percent_total * 4)  # Up to 20 points for missing data\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Missing data: -{deduction:.1f} points ({missing_percent_total:.1f}% of all values)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    dup_percent = (duplicates / len(df)) * 100\n",
    "    deduction = min(10, dup_percent * 2)  # Up to 10 points for duplicates\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Duplicate rows: -{deduction:.1f} points ({dup_percent:.1f}% of records)\")\n",
    "\n",
    "if weather_violations:\n",
    "    deduction = len(weather_violations) * 2  # 2 points per violated weather parameter\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Weather violations: -{deduction} points ({len(weather_violations)} parameters)\")\n",
    "\n",
    "if cyclical_violations:\n",
    "    deduction = len(cyclical_violations) * 3  # 3 points per violated cyclical feature\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Cyclical violations: -{deduction} points ({len(cyclical_violations)} features)\")\n",
    "\n",
    "if binary_violations:\n",
    "    deduction = len(binary_violations) * 2  # 2 points per violated binary flag\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Binary flag violations: -{deduction} points ({len(binary_violations)} flags)\")\n",
    "\n",
    "# Check PM2.5 vs PM10 physics violations\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = (df['pm2_5'] > df['pm10']).sum()\n",
    "    if pm_violations > 0:\n",
    "        violation_percent = (pm_violations / len(df)) * 100\n",
    "        deduction = min(15, violation_percent * 3)  # Up to 15 points for physics violations\n",
    "        quality_score -= deduction\n",
    "        quality_issues.append(f\"PM physics violations: -{deduction:.1f} points ({violation_percent:.1f}% of records)\")\n",
    "\n",
    "# Outlier penalty (mild)\n",
    "total_outlier_features = len(zscore_outliers) + len(iqr_outliers)\n",
    "if total_outlier_features > 0:\n",
    "    deduction = min(5, total_outlier_features * 0.5)  # Mild penalty for outliers\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Statistical outliers: -{deduction:.1f} points (in {total_outlier_features} features)\")\n",
    "\n",
    "# Ensure score doesn't go below 0\n",
    "quality_score = max(0, quality_score)\n",
    "\n",
    "# Display results\n",
    "print(f\"📊 DATA QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    status = \"🟢 EXCELLENT\"\n",
    "    recommendation = \"Data is ready for high-quality ML modeling\"\n",
    "elif quality_score >= 75:\n",
    "    status = \"🟡 GOOD\" \n",
    "    recommendation = \"Data is suitable for ML with minor preprocessing\"\n",
    "elif quality_score >= 60:\n",
    "    status = \"🟠 ACCEPTABLE\"\n",
    "    recommendation = \"Address major issues before ML modeling\"\n",
    "else:\n",
    "    status = \"🔴 POOR\"\n",
    "    recommendation = \"Significant data cleaning required\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"\\nISSUES IDENTIFIED:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  • {issue}\")\n",
    "else:\n",
    "    print(f\"\\n✅ NO SIGNIFICANT QUALITY ISSUES DETECTED\")\n",
    "\n",
    "print(f\"\\n🎯 ML READINESS ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_readiness = True\n",
    "\n",
    "for target in ml_targets:\n",
    "    if target in df.columns:\n",
    "        target_quality = 100\n",
    "        target_issues = []\n",
    "        \n",
    "        # Check for issues specific to ML targets\n",
    "        if target in zscore_outliers:\n",
    "            outlier_percent = zscore_outliers[target]['percent']\n",
    "            if outlier_percent > 5:\n",
    "                target_quality -= 10\n",
    "                target_issues.append(f\"High outlier rate ({outlier_percent:.1f}%)\")\n",
    "        \n",
    "        zero_count = (df[target] == 0.0).sum()\n",
    "        if zero_count > len(df) * 0.1:  # More than 10% zeros\n",
    "            target_quality -= 15\n",
    "            target_issues.append(f\"Many zero values ({zero_count} records)\")\n",
    "        \n",
    "        if target_issues:\n",
    "            print(f\"  {target.upper()}: {target_quality}/100 - {', '.join(target_issues)}\")\n",
    "            if target_quality < 70:\n",
    "                ml_readiness = False\n",
    "        else:\n",
    "            print(f\"  {target.upper()}: ✅ Ready for ML modeling\")\n",
    "\n",
    "if ml_readiness:\n",
    "    print(f\"\\n✅ DATASET IS READY FOR ML MODEL TRAINING\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  CONSIDER ADDITIONAL PREPROCESSING FOR OPTIMAL ML PERFORMANCE\")\n",
    "\n",
    "print(f\"\\n[Next Steps: Feature engineering validation, train/test splitting, model selection]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation checks\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA VALIDATION CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for negative values in air quality parameters (shouldn't be negative)\n",
    "negative_check_cols = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "negative_issues = {}\n",
    "\n",
    "for col in negative_check_cols:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            negative_issues[col] = negative_count\n",
    "\n",
    "if negative_issues:\n",
    "    print(\"⚠️  NEGATIVE VALUES FOUND (unexpected):\")\n",
    "    for col, count in negative_issues.items():\n",
    "        print(f\"  {col}: {count} negative values\")\n",
    "else:\n",
    "    print(\"✓ No unexpected negative values in air quality parameters\")\n",
    "\n",
    "# Check for infinite values\n",
    "infinite_issues = {}\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        infinite_count = np.isinf(df[col]).sum()\n",
    "        if infinite_count > 0:\n",
    "            infinite_issues[col] = infinite_count\n",
    "\n",
    "if infinite_issues:\n",
    "    print(\"\\n⚠️  INFINITE VALUES FOUND:\")\n",
    "    for col, count in infinite_issues.items():\n",
    "        print(f\"  {col}: {count} infinite values\")\n",
    "else:\n",
    "    print(\"\\n✓ No infinite values found\")\n",
    "\n",
    "# Check timestamp continuity\n",
    "print(f\"\\nTIMESTAMP ANALYSIS:\")\n",
    "if 'time' in df.columns:\n",
    "    df_sorted = df.sort_values('time')\n",
    "    time_diffs = df_sorted['time'].diff().dropna()\n",
    "    \n",
    "    print(f\"  Earliest record: {df['time'].min()}\")\n",
    "    print(f\"  Latest record: {df['time'].max()}\")\n",
    "    print(f\"  Total duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "    print(f\"  Most common interval: {time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Records per day average: {len(df) / max(1, (df['time'].max() - df['time'].min()).days):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
