{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA - Multan AQI Features\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Hopsworks Feature Store (multan_aqi_features)\n",
    "- **Records**: 538 observations \n",
    "- **Features**: 127 engineered features\n",
    "- **Time Range**: June 16, 2025 - July 8, 2025\n",
    "\n",
    "## Modeling Approach\n",
    "- **🎯 Goal**: Accurate US AQI prediction for Multan\n",
    "- **🔧 Method**: Train ML model to predict PM2.5 & PM10 → Calculate AQI via EPA formula\n",
    "- **📊 ML Targets**: pm2_5, pm10 concentrations (µg/m³)\n",
    "- **✅ Success Metric**: How well calculated AQI matches actual AQI values\n",
    "\n",
    "## Feature Categories\n",
    "1. **Raw Air Quality**: pm2_5, pm10, co, no2, so2, o3, nh3\n",
    "2. **AQI Calculations**: pm2_5_aqi, pm10_aqi, us_aqi, openweather_aqi\n",
    "3. **Weather Data**: temperature, humidity, pressure, wind_speed, wind_direction\n",
    "4. **Time Features**: Cyclical encodings (hour, day, month, etc.)\n",
    "5. **Lag Features**: 1h-72h historical values\n",
    "6. **Rolling Statistics**: 3h-24h windows (mean, std, min, max)\n",
    "7. **Engineered Features**: Interactions, squared terms, categorical flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store that will be used for modeling.\n",
    "\n",
    "**EDA Focus**: Understanding relationships that help predict PM2.5 and PM10 concentrations accurately, which leads to better AQI predictions.\n",
    "\n",
    "## 1. Data Overview\n",
    "Loading and examining the basic structure of our modeling dataset from Hopsworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import configuration\n",
    "from config import HOPSWORKS_CONFIG\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks and load data\n",
    "print(\"Connecting to Hopsworks...\")\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_CONFIG[\"api_key\"], project=HOPSWORKS_CONFIG[\"project_name\"])\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Loading feature group data...\")\n",
    "fg = fs.get_feature_group(HOPSWORKS_CONFIG[\"feature_group_name\"], version=1)\n",
    "df = fg.read()\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} records from Hopsworks\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column references and prepare data\n",
    "# The actual timestamp column is called 'time' not 'timestamp'\n",
    "print(\"Data preparation and column check...\")\n",
    "print(f\"Time column: {'time' if 'time' in df.columns else 'timestamp not found'}\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "\n",
    "# Ensure time is datetime\n",
    "if df['time'].dtype == 'object':\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "print(\"✓ Data sorted by time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "\n",
    "# Detailed timestamp analysis\n",
    "print(f\"\\nTIMESTAMP ANALYSIS:\")\n",
    "if 'time' in df.columns:\n",
    "    df_sorted = df.sort_values('time')\n",
    "    time_diffs = df_sorted['time'].diff().dropna()\n",
    "    \n",
    "    print(f\"  Earliest record: {df['time'].min()}\")\n",
    "    print(f\"  Latest record: {df['time'].max()}\")\n",
    "    print(f\"  Total duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "    print(f\"  Most common interval: {time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Records per day average: {len(df) / max(1, (df['time'].max() - df['time'].min()).days):.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"COLUMNS:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:2d}. {col:<25} {str(df[col].dtype):<15}\")\n",
    "\n",
    "print()\n",
    "print(\"FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'time' in numeric_cols:\n",
    "    numeric_cols.remove('time')\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Datetime features ({len(datetime_cols)}): {datetime_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last few records\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 records:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\nRandom 5 records:\")\n",
    "display(df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric features\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nAIR QUALITY FEATURES SUMMARY:\")\n",
    "aqi_features = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "aqi_present = [col for col in aqi_features if col in df.columns]\n",
    "if aqi_present:\n",
    "    display(df[aqi_present].describe())\n",
    "\n",
    "print(\"\\nWEATHER FEATURES SUMMARY:\")\n",
    "weather_features = ['temperature', 'feels_like', 'humidity', 'pressure', 'visibility', 'wind_speed', 'wind_direction', 'cloud_cover']\n",
    "weather_present = [col for col in weather_features if col in df.columns]\n",
    "if weather_present:\n",
    "    display(df[weather_present].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis\n",
    "\n",
    "Analyzing temporal patterns in PM concentrations (our prediction targets) and derived AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plots for key air quality metrics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# PM2.5 over time\n",
    "axes[0].plot(df['time'], df['pm2_5'], alpha=0.7, color='red')\n",
    "axes[0].set_title('PM2.5 Concentration Over Time')\n",
    "axes[0].set_ylabel('PM2.5 (µg/m³)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PM10 over time  \n",
    "axes[1].plot(df['time'], df['pm10'], alpha=0.7, color='orange')\n",
    "axes[1].set_title('PM10 Concentration Over Time')\n",
    "axes[1].set_ylabel('PM10 (µg/m³)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# US AQI over time\n",
    "axes[2].plot(df['time'], df['us_aqi'], alpha=0.7, color='purple')\n",
    "axes[2].set_title('US AQI Over Time')\n",
    "axes[2].set_ylabel('US AQI')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI category colors as background\n",
    "aqi_levels = [\n",
    "    (0, 50, 'green', 'Good'),\n",
    "    (51, 100, 'yellow', 'Moderate'), \n",
    "    (101, 150, 'orange', 'Unhealthy for Sensitive'),\n",
    "    (151, 200, 'red', 'Unhealthy'),\n",
    "    (201, 300, 'purple', 'Very Unhealthy'),\n",
    "    (301, 500, 'maroon', 'Hazardous')\n",
    "]\n",
    "\n",
    "for min_val, max_val, color, label in aqi_levels:\n",
    "    axes[2].axhspan(min_val, max_val, alpha=0.1, color=color, label=label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series summary - ML TARGETS and GOAL METRIC\n",
    "print(f\"Time Series Summary (ML Targets + Goal Metric):\")\n",
    "print(f\"PM2.5 (ML Target): {df['pm2_5'].min():.1f} - {df['pm2_5'].max():.1f} µg/m³\")\n",
    "print(f\"PM10 (ML Target):  {df['pm10'].min():.1f} - {df['pm10'].max():.1f} µg/m³\") \n",
    "print(f\"US AQI (Goal Metric): {df['us_aqi'].min():.1f} - {df['us_aqi'].max():.1f}\")\n",
    "print(f\"\\nModeling Approach: Predict PM concentrations → Calculate AQI → Evaluate AQI accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis and Correlations\n",
    "\n",
    "Examining relationships between air quality and weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis - ML targets + environmental predictors\n",
    "key_features = [\n",
    "    'pm2_5', 'pm10',  # ML targets\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Weather predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Pollutant predictors\n",
    "]\n",
    "# Note: Excluding us_aqi since it's derived from PM targets\n",
    "\n",
    "# Filter features that exist in our dataset\n",
    "available_features = [col for col in key_features if col in df.columns]\n",
    "print(f\"Analyzing correlations for {len(available_features)} key features:\")\n",
    "print(available_features)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[available_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix (Key Variables)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on ML TARGETS (PM concentrations) - what we need to predict well\n",
    "print(\"\\nStrongest correlations with PM2.5 (ML Target):\")\n",
    "pm25_corr = corr_matrix['pm2_5'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm25_corr.head(8).items():\n",
    "    if feature != 'pm2_5':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nStrongest correlations with PM10 (ML Target):\")\n",
    "pm10_corr = corr_matrix['pm10'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm10_corr.head(8).items():\n",
    "    if feature != 'pm10':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n🎯 CORRELATION FOCUS:\")\n",
    "print(\"Understanding which environmental factors help predict PM concentrations accurately\")\n",
    "print(\"Better PM predictions → More accurate AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ML TARGETS (PM concentrations) - what affects our predictions\n",
    "print(\"=\"*60)\n",
    "print(\"ML TARGET CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_features = ['pm2_5', 'pm10']\n",
    "predictor_features = ['temperature', 'humidity', 'pressure', 'wind_speed', \n",
    "                     'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "\n",
    "available_predictors = [col for col in predictor_features if col in df.columns]\n",
    "\n",
    "for target in target_features:\n",
    "    if target in df.columns:\n",
    "        print(f\"\\nStrongest correlations with {target.upper()} (Target Variable):\")\n",
    "        target_corr = df[[target] + available_predictors].corr()[target].abs().sort_values(ascending=False)\n",
    "        for feature, corr in target_corr.head(6).items():\n",
    "            if feature != target:\n",
    "                print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n[EDA Focus: Understanding what predicts PM concentrations well]\")\n",
    "print(f\"[Goal: Better PM predictions → More accurate AQI calculations]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AQI Dominance Analysis\n",
    "\n",
    "**Key Question**: Which pollutants actually drive AQI values? \n",
    "\n",
    "Since EPA AQI = MAX(individual pollutant AQIs), we need to check if other criteria pollutants sometimes create higher AQI than PM2.5/PM10. This validates our modeling approach of using only PM concentrations.\n",
    "\n",
    "**EPA Criteria Pollutants Analyzed**: PM2.5, PM10, O3, CO, NO2, SO2  \n",
    "*(Only these 6 pollutants have official EPA AQI breakpoints and affect AQI calculations)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA AQI Calculation Functions for All Pollutants\n",
    "def calculate_aqi_from_concentration(concentration, breakpoints):\n",
    "    \"\"\"Calculate AQI from pollutant concentration using EPA breakpoints - FIXED VERSION\"\"\"\n",
    "    if pd.isna(concentration) or concentration < 0:\n",
    "        return 0\n",
    "    \n",
    "    for C_low, C_high, I_low, I_high in breakpoints:\n",
    "        if C_low <= concentration <= C_high:\n",
    "            # Linear interpolation within the bracket (same formula as feature_engineering.py)\n",
    "            aqi = round((I_high - I_low) / (C_high - C_low) * (concentration - C_low) + I_low)\n",
    "            return aqi\n",
    "    \n",
    "    # ✅ FIXED: Return None instead of 500 when no breakpoint matches\n",
    "    # This indicates data quality issue rather than assuming hazardous level\n",
    "    return None\n",
    "\n",
    "# EPA AQI Breakpoints (concentration ranges and corresponding AQI ranges)\n",
    "EPA_BREAKPOINTS = {\n",
    "    'pm2_5': [\n",
    "        (0.0, 12.0, 0, 50),\n",
    "        (12.1, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 350.4, 301, 400),  # ← ADD THIS MISSING RANGE\n",
    "        (350.5, 500.4, 401, 500),  # ← CORRECT 401-500 RANGE\n",
    "    ],\n",
    "    'pm10': [\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 154, 51, 100),\n",
    "        (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200),\n",
    "        (355, 424, 201, 300),\n",
    "        (425, 504, 301, 400),  # ← ADD THIS MISSING RANGE\n",
    "        (505, 604, 401, 500),  # ← CORRECT 401-500 RANGE\n",
    "    ],\n",
    "    'ozone': [  # ppb, 8-hour average (converting from µg/m³ if needed)\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 70, 51, 100),\n",
    "        (71, 85, 101, 150),\n",
    "        (86, 105, 151, 200),\n",
    "        (106, 200, 201, 300)\n",
    "    ],\n",
    "    'carbon_monoxide': [  # ppm, 8-hour average\n",
    "        (0.0, 4.4, 0, 50),\n",
    "        (4.5, 9.4, 51, 100),\n",
    "        (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200),\n",
    "        (15.5, 30.4, 201, 300),\n",
    "        (30.5, 50.4, 301, 500)\n",
    "    ],\n",
    "    'nitrogen_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 53, 0, 50),\n",
    "        (54, 100, 51, 100),\n",
    "        (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200),\n",
    "        (650, 1249, 201, 300),\n",
    "        (1250, 2049, 301, 500)\n",
    "    ],\n",
    "    'sulphur_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 35, 0, 50),\n",
    "        (36, 75, 51, 100),\n",
    "        (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200),\n",
    "        (305, 604, 201, 300),\n",
    "        (605, 1004, 301, 500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"EPA AQI Calculation Functions Loaded\")\n",
    "print(f\"EPA Criteria Pollutants (official breakpoints): {list(EPA_BREAKPOINTS.keys())}\")\n",
    "\n",
    "# Check which EPA criteria pollutants we have in our data\n",
    "available_pollutants = []\n",
    "for pollutant in EPA_BREAKPOINTS.keys():\n",
    "    if pollutant in df.columns:\n",
    "        available_pollutants.append(pollutant)\n",
    "        \n",
    "print(f\"\\nEPA criteria pollutants in our dataset: {available_pollutants}\")\n",
    "print(f\"Missing from dataset: {[p for p in EPA_BREAKPOINTS.keys() if p not in df.columns]}\")\n",
    "print(f\"\\nAnalyzing {len(available_pollutants)} pollutants that can affect AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual AQI for each available pollutant\n",
    "aqi_results = df[['time', 'us_aqi']].copy()\n",
    "\n",
    "# Unit conversions (if needed)\n",
    "df_calc = df.copy()\n",
    "\n",
    "# Convert units for certain pollutants if needed\n",
    "# Ozone: µg/m³ to ppb (approximate: µg/m³ * 0.5 ≈ ppb at standard conditions)\n",
    "if 'ozone' in df_calc.columns:\n",
    "    df_calc['ozone_ppb'] = df_calc['ozone'] * 0.5  # Rough conversion\n",
    "    \n",
    "# CO might need conversion from µg/m³ to ppm\n",
    "if 'carbon_monoxide' in df_calc.columns:\n",
    "    df_calc['co_ppm'] = df_calc['carbon_monoxide'] * 0.000873  # Rough conversion\n",
    "\n",
    "print(\"Calculating individual AQI for each pollutant...\")\n",
    "\n",
    "# Calculate AQI for each pollutant\n",
    "for pollutant in available_pollutants:\n",
    "    col_name = f'{pollutant}_individual_aqi'\n",
    "    \n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        concentrations = df_calc['ozone_ppb']\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        concentrations = df_calc['co_ppm']\n",
    "    else:\n",
    "        concentrations = df_calc[pollutant]\n",
    "    \n",
    "    aqi_results[col_name] = concentrations.apply(\n",
    "        lambda x: calculate_aqi_from_concentration(x, EPA_BREAKPOINTS[pollutant])\n",
    "    )\n",
    "    \n",
    "    # Handle None values (data outside breakpoint ranges)\n",
    "    null_count = aqi_results[col_name].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"⚠️  {pollutant}: {null_count} values outside breakpoint ranges, replacing with 0\")\n",
    "        aqi_results[col_name] = aqi_results[col_name].fillna(0)\n",
    "    \n",
    "    # Now safe to calculate min/max\n",
    "    valid_values = aqi_results[col_name].dropna()\n",
    "    if len(valid_values) > 0:\n",
    "        print(f\"✓ {pollutant}: {aqi_results[col_name].min():.0f} - {aqi_results[col_name].max():.0f} AQI\")\n",
    "    else:\n",
    "        print(f\"✗ {pollutant}: No valid AQI values calculated\")\n",
    "\n",
    "# Find controlling pollutant (max AQI) for each timestamp\n",
    "individual_aqi_cols = [col for col in aqi_results.columns if 'individual_aqi' in col]\n",
    "aqi_results['calculated_max_aqi'] = aqi_results[individual_aqi_cols].max(axis=1)\n",
    "aqi_results['controlling_pollutant'] = aqi_results[individual_aqi_cols].idxmax(axis=1)\n",
    "\n",
    "# Clean up pollutant names\n",
    "aqi_results['controlling_pollutant'] = aqi_results['controlling_pollutant'].str.replace('_individual_aqi', '')\n",
    "\n",
    "print(f\"\\nCalculated AQI range: {aqi_results['calculated_max_aqi'].min():.0f} - {aqi_results['calculated_max_aqi'].max():.0f}\")\n",
    "print(f\"Current US AQI range: {aqi_results['us_aqi'].min():.0f} - {aqi_results['us_aqi'].max():.0f}\")\n",
    "print(f\"Difference: {(aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']).describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 1: Pollutant Dominance Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"POLLUTANT DOMINANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count which pollutant controls AQI most often\n",
    "dominance_counts = aqi_results['controlling_pollutant'].value_counts()\n",
    "dominance_pct = (dominance_counts / len(aqi_results) * 100).round(1)\n",
    "\n",
    "print(\"Controlling Pollutant Frequency:\")\n",
    "for pollutant, count in dominance_counts.items():\n",
    "    pct = dominance_pct[pollutant]\n",
    "    print(f\"  {pollutant:<15}: {count:3d} times ({pct:5.1f}%)\")\n",
    "\n",
    "# Pie chart of dominance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(dominance_counts.values, labels=dominance_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Which Pollutant Controls AQI?')\n",
    "\n",
    "# Bar chart for clearer comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(range(len(dominance_counts)), dominance_counts.values, \n",
    "               color=['red' if x in ['pm2_5', 'pm10'] else 'lightcoral' for x in dominance_counts.index])\n",
    "plt.xticks(range(len(dominance_counts)), [p.replace('_', ' ').title() for p in dominance_counts.index], rotation=45)\n",
    "plt.ylabel('Number of Hours')\n",
    "plt.title('Controlling Pollutant Frequency')\n",
    "\n",
    "# Highlight PM2.5 and PM10\n",
    "for i, (pollutant, count) in enumerate(dominance_counts.items()):\n",
    "    color = 'white' if pollutant in ['pm2_5', 'pm10'] else 'black'\n",
    "    plt.text(i, count + 5, f'{count}', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of PM dominance\n",
    "pm_dominance = dominance_counts.get('pm2_5', 0) + dominance_counts.get('pm10', 0)\n",
    "pm_percentage = (pm_dominance / len(aqi_results) * 100)\n",
    "\n",
    "print(f\"\\n🎯 KEY FINDING:\")\n",
    "print(f\"PM2.5 + PM10 control AQI {pm_dominance}/{len(aqi_results)} times ({pm_percentage:.1f}%)\")\n",
    "print(f\"Other pollutants control AQI {len(aqi_results) - pm_dominance}/{len(aqi_results)} times ({100-pm_percentage:.1f}%)\")\n",
    "\n",
    "if pm_percentage >= 80:\n",
    "    print(\"✅ Current PM-only approach captures most AQI variations\")\n",
    "else:\n",
    "    print(\"⚠️  Consider including other pollutants in modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MIGHT DELETE ########\n",
    "\n",
    "# VISUALIZATION 2: Concentration vs AQI Relationships\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCENTRATION vs AQI ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create scatter plots for EPA criteria pollutants only\n",
    "num_plots = len(available_pollutants)\n",
    "cols = 3\n",
    "rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get concentrations and individual AQI\n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        conc = df_calc['ozone_ppb']\n",
    "        unit = 'ppb'\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        conc = df_calc['co_ppm']\n",
    "        unit = 'ppm'\n",
    "    else:\n",
    "        conc = df_calc[pollutant]\n",
    "        unit = 'µg/m³'\n",
    "    \n",
    "    # All pollutants here are EPA criteria pollutants with AQI calculations\n",
    "    individual_aqi = aqi_results[f'{pollutant}_individual_aqi']\n",
    "    \n",
    "    # Scatter plot with AQI color coding\n",
    "    scatter = ax.scatter(conc, individual_aqi, alpha=0.6, s=20, \n",
    "                        c=individual_aqi, cmap='RdYlGn_r', vmin=0, vmax=150)\n",
    "    \n",
    "    ax.set_xlabel(f'{pollutant.replace(\"_\", \" \").title()} ({unit})')\n",
    "    ax.set_ylabel('Individual AQI')\n",
    "    ax.set_title(f'{pollutant.replace(\"_\", \" \").title()} → AQI')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AQI level lines\n",
    "    ax.axhline(y=50, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=100, color='yellow', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=150, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(available_pollutants), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for EPA criteria pollutants\n",
    "print(\"\\nIndividual AQI Statistics (EPA Criteria Pollutants):\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    stats = aqi_results[aqi_col].describe()\n",
    "    print(f\"{pollutant:<15}: Mean={stats['mean']:5.1f}, Max={stats['max']:5.1f}, >100: {(aqi_results[aqi_col] > 100).sum():3d} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3A: Individual AQI Contributions vs Maximum AQI\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL POLLUTANT AQI CONTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time series showing all individual AQIs vs the maximum\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All individual AQIs over time\n",
    "time_vals = aqi_results['time']\n",
    "max_aqi_vals = aqi_results['calculated_max_aqi']\n",
    "\n",
    "# Plot each individual AQI\n",
    "colors = ['red', 'darkred', 'blue', 'purple', 'orange', 'green']\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        ax1.plot(time_vals, aqi_results[aqi_col], \n",
    "                label=pollutant.replace('_', ' ').title(), \n",
    "                alpha=0.7, linewidth=1.5, color=colors[i % len(colors)])\n",
    "\n",
    "# Plot maximum AQI as thick black line\n",
    "ax1.plot(time_vals, max_aqi_vals, \n",
    "         label='Maximum AQI (Envelope)', \n",
    "         color='black', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('Individual Pollutant AQIs vs Maximum AQI Over Time')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Distribution of individual AQIs \n",
    "aqi_data = []\n",
    "pollutant_names = []\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        aqi_data.append(aqi_results[aqi_col].values)\n",
    "        pollutant_names.append(pollutant.replace('_', ' ').title())\n",
    "\n",
    "# Add maximum AQI for comparison\n",
    "aqi_data.append(max_aqi_vals.values)\n",
    "pollutant_names.append('Maximum AQI')\n",
    "\n",
    "# Create box plot\n",
    "bp = ax2.boxplot(aqi_data, labels=pollutant_names, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "box_colors = colors + ['black']\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('AQI Value')\n",
    "ax2.set_title('AQI Distribution by Pollutant')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level lines\n",
    "for level, color in [(50, 'green'), (100, 'yellow'), (150, 'orange')]:\n",
    "    ax2.axhline(y=level, color=color, linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for contribution analysis\n",
    "print(\"\\nIndividual AQI vs Maximum AQI Analysis:\")\n",
    "max_aqi_mean = max_aqi_vals.mean()\n",
    "print(f\"Maximum AQI - Mean: {max_aqi_mean:.1f}, Range: {max_aqi_vals.min():.0f}-{max_aqi_vals.max():.0f}\")\n",
    "\n",
    "print(\"\\nHow often each pollutant reaches within 90% of maximum AQI:\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        close_to_max = (aqi_results[aqi_col] >= 0.9 * max_aqi_vals).sum()\n",
    "        percentage = (close_to_max / len(aqi_results) * 100)\n",
    "        print(f\"  {pollutant:<15}: {close_to_max:3d}/{len(aqi_results)} times ({percentage:5.1f}%) within 90% of max\")\n",
    "\n",
    "print(\"\\nThis shows which pollutants are the 'runners-up' when they don't control AQI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3: Time Series of Controlling Pollutants\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME SERIES OF AQI CONTROL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a time series showing which pollutant controls AQI\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: AQI comparison over time\n",
    "ax1.plot(aqi_results['time'], aqi_results['us_aqi'], label='Current US AQI (PM only)', alpha=0.7, linewidth=2)\n",
    "ax1.plot(aqi_results['time'], aqi_results['calculated_max_aqi'], label='True Max AQI (All pollutants)', alpha=0.7, linewidth=2)\n",
    "ax1.fill_between(aqi_results['time'], aqi_results['us_aqi'], aqi_results['calculated_max_aqi'], \n",
    "                 alpha=0.3, color='red', label='Missing AQI')\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('AQI Comparison: Current PM-only vs Full EPA Calculation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Controlling pollutant over time\n",
    "pollutant_colors = {\n",
    "    'pm2_5': 'red', 'pm10': 'darkred', 'ozone': 'blue', \n",
    "    'carbon_monoxide': 'purple', 'nitrogen_dioxide': 'orange', 'sulphur_dioxide': 'green'\n",
    "}\n",
    "\n",
    "# Create numerical encoding for pollutants for plotting\n",
    "unique_pollutants = aqi_results['controlling_pollutant'].unique()\n",
    "pollutant_mapping = {p: i for i, p in enumerate(unique_pollutants)}\n",
    "aqi_results['pollutant_num'] = aqi_results['controlling_pollutant'].map(pollutant_mapping)\n",
    "\n",
    "scatter = ax2.scatter(aqi_results['time'], aqi_results['pollutant_num'], \n",
    "                     c=[pollutant_colors.get(p, 'gray') for p in aqi_results['controlling_pollutant']], \n",
    "                     alpha=0.7, s=20)\n",
    "\n",
    "ax2.set_ylabel('Controlling Pollutant')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Which Pollutant Controls AQI Over Time')\n",
    "ax2.set_yticks(range(len(unique_pollutants)))\n",
    "ax2.set_yticklabels([p.replace('_', ' ').title() for p in unique_pollutants])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate impact of missing other pollutants\n",
    "aqi_difference = aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']\n",
    "significant_underestimation = (aqi_difference > 10).sum()\n",
    "\n",
    "print(f\"\\n📊 IMPACT ANALYSIS:\")\n",
    "print(f\"Times when PM-only AQI underestimates by >10 points: {significant_underestimation}/{len(aqi_results)} ({significant_underestimation/len(aqi_results)*100:.1f}%)\")\n",
    "print(f\"Maximum underestimation: {aqi_difference.max():.1f} AQI points\")\n",
    "print(f\"Average difference: {aqi_difference.mean():.1f} AQI points\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n🎯 MODELING RECOMMENDATION:\")\n",
    "if pm_percentage >= 85:\n",
    "    print(\"✅ PM-only approach is SUFFICIENT for Multan AQI prediction\")\n",
    "    print(\"   PM2.5 + PM10 control >85% of AQI variations\")\n",
    "elif pm_percentage >= 70:\n",
    "    print(\"⚠️  PM-only approach is MOSTLY adequate but consider monitoring other pollutants\")\n",
    "    print(\"   PM2.5 + PM10 control 70-85% of AQI variations\")\n",
    "else:\n",
    "    print(\"🚨 PM-only approach MISSES significant AQI drivers\")\n",
    "    print(\"   Consider including other pollutants in prediction model\")\n",
    "\n",
    "print(f\"\\nCurrent focus on PM2.5 and PM10 captures {pm_percentage:.1f}% of AQI control instances.\")\n",
    "\n",
    "print(f\"\\n🎯 FINAL ANALYSIS:\")\n",
    "print(f\"✓ Analyzed all {len(available_pollutants)} EPA criteria pollutants in dataset\")\n",
    "print(f\"✓ PM2.5 and PM10 are responsible for {pm_percentage:.1f}% of AQI determinations\")\n",
    "print(f\"✓ Other criteria pollutants (O3, CO, NO2, SO2) control {100-pm_percentage:.1f}% of AQI\")\n",
    "print(f\"✓ This validates the scope of your PM-focused modeling approach\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lag Features Analysis\n",
    "\n",
    "Examining the importance of historical PM values for predicting current concentrations (which leads to better AQI calculations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features for BOTH ML targets (PM2.5 AND PM10)\n",
    "print(\"=\"*60)\n",
    "print(\"LAG FEATURES ANALYSIS FOR ML TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM10 Lag Analysis (since it's also a target)\n",
    "pm10_lag_features = [col for col in df.columns if 'lag' in col and 'pm10' in col]\n",
    "if pm10_lag_features:\n",
    "    print(f\"\\nFound {len(pm10_lag_features)} PM10 lag features:\")\n",
    "    print(pm10_lag_features[:5], \"...\" if len(pm10_lag_features) > 5 else \"\")\n",
    "    \n",
    "    # Calculate correlations between current PM10 and its lag features\n",
    "    pm10_lag_correlations = df[['pm10'] + pm10_lag_features].corr()['pm10'].drop('pm10')\n",
    "    \n",
    "    # Plot PM10 lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "    pm10_correlations = [pm10_lag_correlations[f'pm10_lag_{h}h'] for h in lag_hours if f'pm10_lag_{h}h' in pm10_lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(pm10_correlations)), pm10_correlations, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM10')\n",
    "    plt.title('PM10 Lag Features Correlation (ML Target)')\n",
    "    plt.xticks(range(len(pm10_correlations)), [f'{h}h' for h in lag_hours[:len(pm10_correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(pm10_correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPM10 lag feature correlations (ML Target):\")\n",
    "    for i, h in enumerate(lag_hours[:len(pm10_correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {pm10_correlations[i]:.3f}\")\n",
    "\n",
    "print(\"\\n[ML Targets: PM2.5 & PM10 concentrations]\")\n",
    "print(\"[Ultimate Goal: Accurate AQI predictions for Multan]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features correlation with current PM2.5\n",
    "lag_features = [col for col in df.columns if 'lag' in col and 'pm2_5' in col]\n",
    "print(f\"Found {len(lag_features)} PM2.5 lag features:\")\n",
    "print(lag_features)\n",
    "\n",
    "if lag_features:\n",
    "    # Calculate correlations between current PM2.5 and its lag features\n",
    "    lag_correlations = df[['pm2_5'] + lag_features].corr()['pm2_5'].drop('pm2_5')\n",
    "    \n",
    "    # Plot lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]  # Expected lag hours\n",
    "    correlations = [lag_correlations[f'pm2_5_lag_{h}h'] for h in lag_hours if f'pm2_5_lag_{h}h' in lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(correlations)), correlations, alpha=0.7)\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM2.5')\n",
    "    plt.title('PM2.5 Lag Features Correlation')\n",
    "    plt.xticks(range(len(correlations)), [f'{h}h' for h in lag_hours[:len(correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLag feature correlations with current PM2.5:\")\n",
    "    for i, h in enumerate(lag_hours[:len(correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {correlations[i]:.3f}\")\n",
    "\n",
    "# Complete Rolling Statistics Analysis for Both ML Targets\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ROLLING STATISTICS ANALYSIS (ML TARGETS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM2.5 Rolling Features Analysis\n",
    "pm25_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm2_5' in col]\n",
    "if pm25_rolling_features:\n",
    "    print(f\"\\nPM2.5 ROLLING FEATURES ({len(pm25_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    rolling_types = ['mean', 'std', 'min', 'max']\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm25_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm2_5'] + stat_features].corr()['pm2_5'].drop('pm2_5')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# PM10 Rolling Features Analysis  \n",
    "pm10_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm10' in col]\n",
    "if pm10_rolling_features:\n",
    "    print(f\"\\nPM10 ROLLING FEATURES ({len(pm10_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm10_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm10'] + stat_features].corr()['pm10'].drop('pm10')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# Change Rate Features Analysis\n",
    "print(f\"\\nCHANGE RATE FEATURES:\")\n",
    "change_features = [col for col in df.columns if 'change_rate' in col]\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    target_change_features = [col for col in change_features if target in col]\n",
    "    if target_change_features:\n",
    "        print(f\"\\n  {target.upper()} change rates:\")\n",
    "        correlations = df[[target] + target_change_features].corr()[target].drop(target)\n",
    "        for feature, corr in correlations.items():\n",
    "            period = feature.split('_')[-1]\n",
    "            print(f\"    {period:<4} change: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(f\"• Rolling features capture trend information over different time windows\")\n",
    "print(f\"• Shorter windows (3h, 6h) typically correlate more strongly with current values\")\n",
    "print(f\"• Change rates show how much PM concentrations are shifting\")\n",
    "print(f\"• These features help models understand pollution persistence and trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Analysis\n",
    "\n",
    "**Focus**: Ensuring data reliability for accurate PM concentration predictions and AQI calculations.\n",
    "\n",
    "### 6.1 Missing Values & Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Missing Values & Null Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES & NULL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic missing value check\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent\n",
    "}).round(2)\n",
    "\n",
    "print(\"EXPLICIT NULL VALUES:\")\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"✓ No explicit null values found!\")\n",
    "else:\n",
    "    print(\"Missing values found:\")\n",
    "    for col in missing_df[missing_df['Missing_Count'] > 0].index:\n",
    "        count = missing_df.loc[col, 'Missing_Count']\n",
    "        percent = missing_df.loc[col, 'Missing_Percentage']\n",
    "        print(f\"  {col:<25} {count:>6} ({percent:>6.2f}%)\")\n",
    "\n",
    "# Check for zero values that might represent missing data\n",
    "print(f\"\\nZERO VALUES ANALYSIS (Potential Missing Data):\")\n",
    "air_quality_cols = ['pm2_5', 'pm10', 'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "zero_issues = {}\n",
    "\n",
    "for col in air_quality_cols:\n",
    "    if col in df.columns:\n",
    "        zero_count = (df[col] == 0.0).sum()\n",
    "        zero_percent = (zero_count / len(df)) * 100\n",
    "        if zero_count > 0:\n",
    "            zero_issues[col] = {'count': zero_count, 'percent': zero_percent}\n",
    "\n",
    "if zero_issues:\n",
    "    print(\"⚠️  ZERO VALUES FOUND (may indicate missing sensors):\")\n",
    "    for col, stats in zero_issues.items():\n",
    "        print(f\"  {col:<20}: {stats['count']:>3d} zeros ({stats['percent']:>5.1f}%)\")\n",
    "        \n",
    "    # Visualize zero patterns over time\n",
    "    fig, axes = plt.subplots(len(zero_issues), 1, figsize=(15, 3*len(zero_issues)))\n",
    "    if len(zero_issues) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (col, stats) in enumerate(zero_issues.items()):\n",
    "        zero_mask = df[col] == 0.0\n",
    "        axes[i].scatter(df[zero_mask]['time'], [col]*zero_mask.sum(), \n",
    "                       alpha=0.7, color='red', s=20, label=f'Zero values ({stats[\"count\"]})')\n",
    "        axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} - Zero Value Timeline')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✓ No zero values in air quality parameters\")\n",
    "\n",
    "# Check for consecutive missing periods (data gaps)\n",
    "print(f\"\\nTIME GAP ANALYSIS:\")\n",
    "df_sorted = df.sort_values('time').reset_index(drop=True)\n",
    "time_diffs = df_sorted['time'].diff()\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]\n",
    "\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"⚠️  Found {len(large_gaps)} time gaps > 2 hours:\")\n",
    "    for idx, gap in large_gaps.items():\n",
    "        gap_start = df_sorted.loc[idx-1, 'time'] if idx > 0 else 'Start'\n",
    "        gap_end = df_sorted.loc[idx, 'time']\n",
    "        print(f\"  Gap: {gap} between {gap_start} and {gap_end}\")\n",
    "else:\n",
    "    print(\"✓ No significant time gaps found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Consistency & Physics Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Data Consistency & Physics Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CONSISTENCY & PHYSICS VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PM2.5 vs PM10 relationship (PM2.5 should generally be ≤ PM10)\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = df[df['pm2_5'] > df['pm10']]\n",
    "    violation_count = len(pm_violations)\n",
    "    violation_percent = (violation_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"PM2.5 vs PM10 CONSISTENCY:\")\n",
    "    if violation_count > 0:\n",
    "        print(f\"⚠️  {violation_count} records ({violation_percent:.1f}%) where PM2.5 > PM10\")\n",
    "        print(f\"   Max violation: PM2.5={pm_violations['pm2_5'].max():.1f} > PM10={pm_violations['pm10'].max():.1f}\")\n",
    "        \n",
    "        # Show violation timeline\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(df['time'], df['pm2_5'], alpha=0.5, label='PM2.5', s=10)\n",
    "        plt.scatter(df['time'], df['pm10'], alpha=0.5, label='PM10', s=10)\n",
    "        plt.scatter(pm_violations['time'], pm_violations['pm2_5'], \n",
    "                   color='red', label=f'PM2.5 > PM10 ({violation_count} cases)', s=30, marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (µg/m³)')\n",
    "        plt.title('PM2.5 vs PM10 Consistency Check')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"✓ PM2.5 ≤ PM10 in all records (physically consistent)\")\n",
    "\n",
    "# Check reasonable ranges for weather parameters\n",
    "print(f\"\\nWEATHER PARAMETER VALIDATION:\")\n",
    "weather_ranges = {\n",
    "    'temperature': (-50, 60, '°C'),  # Extreme but possible range\n",
    "    'humidity': (0, 100, '%'),       # Should be 0-100%\n",
    "    'pressure': (800, 1200, 'hPa'),  # Reasonable atmospheric pressure\n",
    "    'wind_speed': (0, 50, 'm/s'),    # Reasonable wind speeds\n",
    "    'wind_direction': (0, 360, '°')  # Should be 0-360 degrees\n",
    "}\n",
    "\n",
    "weather_violations = {}\n",
    "for param, (min_val, max_val, unit) in weather_ranges.items():\n",
    "    if param in df.columns:\n",
    "        below_min = (df[param] < min_val).sum()\n",
    "        above_max = (df[param] > max_val).sum()\n",
    "        violations = below_min + above_max\n",
    "        \n",
    "        if violations > 0:\n",
    "            weather_violations[param] = {\n",
    "                'below_min': below_min, 'above_max': above_max, \n",
    "                'min_val': min_val, 'max_val': max_val, 'unit': unit,\n",
    "                'actual_min': df[param].min(), 'actual_max': df[param].max()\n",
    "            }\n",
    "\n",
    "if weather_violations:\n",
    "    print(\"⚠️  WEATHER PARAMETER VIOLATIONS:\")\n",
    "    for param, stats in weather_violations.items():\n",
    "        print(f\"  {param:<15}: {stats['below_min']} below {stats['min_val']}{stats['unit']}, \"\n",
    "              f\"{stats['above_max']} above {stats['max_val']}{stats['unit']}\")\n",
    "        print(f\"                   Actual range: {stats['actual_min']:.1f} - {stats['actual_max']:.1f}{stats['unit']}\")\n",
    "else:\n",
    "    print(\"✓ All weather parameters within reasonable ranges\")\n",
    "\n",
    "# Check cyclical feature ranges (sin/cos should be in [-1, 1])\n",
    "print(f\"\\nCYCLICAL FEATURE VALIDATION:\")\n",
    "cyclical_cols = [col for col in df.columns if '_sin' in col or '_cos' in col]\n",
    "cyclical_violations = {}\n",
    "\n",
    "for col in cyclical_cols:\n",
    "    below_neg1 = (df[col] < -1.01).sum()  # Small tolerance for floating point\n",
    "    above_pos1 = (df[col] > 1.01).sum()\n",
    "    violations = below_neg1 + above_pos1\n",
    "    \n",
    "    if violations > 0:\n",
    "        cyclical_violations[col] = {\n",
    "            'below_neg1': below_neg1, 'above_pos1': above_pos1,\n",
    "            'actual_min': df[col].min(), 'actual_max': df[col].max()\n",
    "        }\n",
    "\n",
    "if cyclical_violations:\n",
    "    print(\"⚠️  CYCLICAL FEATURE VIOLATIONS:\")\n",
    "    for col, stats in cyclical_violations.items():\n",
    "        print(f\"  {col:<20}: {stats['below_neg1']} below -1, {stats['above_pos1']} above 1\")\n",
    "        print(f\"                       Actual range: {stats['actual_min']:.6f} - {stats['actual_max']:.6f}\")\n",
    "else:\n",
    "    print(\"✓ All cyclical features within [-1, 1] range\")\n",
    "\n",
    "# Check binary flag consistency\n",
    "print(f\"\\nBINARY FLAG VALIDATION:\")\n",
    "binary_cols = [col for col in df.columns if col.startswith('is_')]\n",
    "binary_violations = {}\n",
    "\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    expected_vals = {0.0, 1.0}\n",
    "    unexpected_vals = set(unique_vals) - expected_vals\n",
    "    \n",
    "    if unexpected_vals:\n",
    "        binary_violations[col] = {\n",
    "            'unexpected': list(unexpected_vals),\n",
    "            'unique_vals': list(unique_vals)\n",
    "        }\n",
    "\n",
    "if binary_violations:\n",
    "    print(\"⚠️  BINARY FLAG VIOLATIONS:\")\n",
    "    for col, stats in binary_violations.items():\n",
    "        print(f\"  {col:<20}: Found {stats['unexpected']} (expected only 0.0, 1.0)\")\n",
    "        print(f\"                       All values: {stats['unique_vals']}\")\n",
    "else:\n",
    "    print(\"✓ All binary flags contain only 0.0 and 1.0 values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Statistical Outlier Detection\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and key environmental features\n",
    "key_features_for_outliers = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for model quality\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Air quality predictors\n",
    "]\n",
    "\n",
    "available_outlier_features = [col for col in key_features_for_outliers if col in df.columns]\n",
    "\n",
    "# Z-score method (|z| > 3 considered outlier)\n",
    "print(\"Z-SCORE OUTLIER DETECTION (|z-score| > 3):\")\n",
    "zscore_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask = z_scores > 3\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        zscore_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'max_zscore': z_scores.max(),\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if zscore_outliers:\n",
    "    print(\"⚠️  Z-SCORE OUTLIERS FOUND:\")\n",
    "    for col, stats in zscore_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%) - Max |z|: {stats['max_zscore']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            print(f\"                       Values: {sorted(stats['outlier_values'])[:5]}...\" if len(stats['outlier_values']) > 5 else f\"                       Values: {sorted(stats['outlier_values'])}\")\n",
    "else:\n",
    "    print(\"✓ No z-score outliers found (|z| > 3)\")\n",
    "\n",
    "# IQR method (values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
    "print(f\"\\nIQR OUTLIER DETECTION (beyond Q1-1.5*IQR, Q3+1.5*IQR):\")\n",
    "iqr_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        iqr_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if iqr_outliers:\n",
    "    print(\"⚠️  IQR OUTLIERS FOUND:\")\n",
    "    for col, stats in iqr_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%)\")\n",
    "        print(f\"                       Expected range: {stats['lower_bound']:.2f} - {stats['upper_bound']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            extreme_values = sorted(stats['outlier_values'])\n",
    "            display_values = extreme_values[:3] + ['...'] + extreme_values[-2:] if len(extreme_values) > 5 else extreme_values\n",
    "            print(f\"                       Outlier values: {display_values}\")\n",
    "else:\n",
    "    print(\"✓ No IQR outliers found\")\n",
    "\n",
    "# Visualize outliers for ML targets\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_present = [col for col in ml_targets if col in df.columns]\n",
    "\n",
    "if ml_targets_present and (zscore_outliers or iqr_outliers):\n",
    "    print(f\"\\nOUTLIER VISUALIZATION FOR ML TARGETS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_present), 2, figsize=(15, 5*len(ml_targets_present)))\n",
    "    if len(ml_targets_present) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_present):\n",
    "        # Box plot\n",
    "        ax1 = axes[i, 0]\n",
    "        bp = ax1.boxplot([df[target].dropna()], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax1.set_title(f'{target.upper()} - Box Plot (IQR Outliers)')\n",
    "        ax1.set_ylabel(f'{target.replace(\"_\", \".\")} (µg/m³)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series with outliers highlighted\n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(df['time'], df[target], alpha=0.7, linewidth=1, label=target.upper())\n",
    "        \n",
    "        # Highlight outliers\n",
    "        if target in zscore_outliers:\n",
    "            z_scores = np.abs((df[target] - df[target].mean()) / df[target].std())\n",
    "            zscore_mask = z_scores > 3\n",
    "            ax2.scatter(df[zscore_mask]['time'], df.loc[zscore_mask, target], \n",
    "                       color='red', s=30, marker='x', label=f'Z-score outliers ({zscore_outliers[target][\"count\"]})')\n",
    "        \n",
    "        if target in iqr_outliers:\n",
    "            Q1 = df[target].quantile(0.25)\n",
    "            Q3 = df[target].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_mask = (df[target] < lower_bound) | (df[target] > upper_bound)\n",
    "            ax2.scatter(df[iqr_mask]['time'], df.loc[iqr_mask, target], \n",
    "                       color='orange', s=20, marker='o', alpha=0.7, label=f'IQR outliers ({iqr_outliers[target][\"count\"]})')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Time Series with Outliers')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(f'{target.replace(\"_\", \".\")} (µg/m³)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary of outlier impact on ML targets\n",
    "print(f\"\\n🎯 OUTLIER IMPACT ON ML TARGETS:\")\n",
    "for target in ml_targets_present:\n",
    "    total_outliers = 0\n",
    "    if target in zscore_outliers:\n",
    "        total_outliers += zscore_outliers[target]['count']\n",
    "    if target in iqr_outliers:\n",
    "        total_outliers += iqr_outliers[target]['count']  # Note: may overlap with z-score\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        outlier_percent = (total_outliers / len(df)) * 100\n",
    "        print(f\"  {target.upper()}: ~{total_outliers} potential outliers ({outlier_percent:.1f}% of data)\")\n",
    "        print(f\"           Consider: Review for sensor errors vs genuine extreme pollution events\")\n",
    "    else:\n",
    "        print(f\"  {target.upper()}: No significant outliers detected\")\n",
    "\n",
    "print(f\"\\n[Quality Note: Outliers in PM concentrations could be genuine pollution spikes or sensor malfunctions]\")\n",
    "print(f\"[Recommendation: Investigate extreme values before removal - they might be real air quality events]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Data Completeness & Summary Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Data Completeness & Summary Quality Report\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA COMPLETENESS & SUMMARY QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"DUPLICATE RECORDS: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"⚠️  Duplicate rows found - consider deduplication\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows\")\n",
    "\n",
    "# Unique values analysis\n",
    "print(f\"\\nUNIQUE VALUES ANALYSIS:\")\n",
    "feature_categories = {\n",
    "    'ML Targets': ['pm2_5', 'pm10'],\n",
    "    'Air Quality': ['carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide', 'nh3'],\n",
    "    'AQI Calculations': ['pm2_5_aqi', 'pm10_aqi', 'us_aqi', 'openweather_aqi'],\n",
    "    'Weather': ['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction'],\n",
    "    'Time Features': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'],\n",
    "    'Binary Flags': [col for col in df.columns if col.startswith('is_')],\n",
    "    'Lag Features': [col for col in df.columns if 'lag' in col][:5],  # Show first 5\n",
    "    'Rolling Features': [col for col in df.columns if 'rolling' in col][:5]  # Show first 5\n",
    "}\n",
    "\n",
    "for category, cols in feature_categories.items():\n",
    "    available_cols = [col for col in cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for col in available_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            unique_ratio = unique_count / len(df)\n",
    "            if unique_ratio < 0.01:  # Very low uniqueness\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ⚠️  Low variation\")\n",
    "            elif unique_ratio > 0.95:  # Very high uniqueness  \n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ✓ High variation\")\n",
    "            else:\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%})\")\n",
    "\n",
    "# Data quality scoring\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL DATA QUALITY SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_score = 100  # Start with perfect score\n",
    "quality_issues = []\n",
    "\n",
    "# Deduct points for various issues\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent_total = (missing_data.sum() / (len(df) * len(df.columns))) * 100\n",
    "    deduction = min(20, missing_percent_total * 4)  # Up to 20 points for missing data\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Missing data: -{deduction:.1f} points ({missing_percent_total:.1f}% of all values)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    dup_percent = (duplicates / len(df)) * 100\n",
    "    deduction = min(10, dup_percent * 2)  # Up to 10 points for duplicates\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Duplicate rows: -{deduction:.1f} points ({dup_percent:.1f}% of records)\")\n",
    "\n",
    "if weather_violations:\n",
    "    deduction = len(weather_violations) * 2  # 2 points per violated weather parameter\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Weather violations: -{deduction} points ({len(weather_violations)} parameters)\")\n",
    "\n",
    "if cyclical_violations:\n",
    "    deduction = len(cyclical_violations) * 3  # 3 points per violated cyclical feature\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Cyclical violations: -{deduction} points ({len(cyclical_violations)} features)\")\n",
    "\n",
    "if binary_violations:\n",
    "    deduction = len(binary_violations) * 2  # 2 points per violated binary flag\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Binary flag violations: -{deduction} points ({len(binary_violations)} flags)\")\n",
    "\n",
    "# Check PM2.5 vs PM10 physics violations\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = (df['pm2_5'] > df['pm10']).sum()\n",
    "    if pm_violations > 0:\n",
    "        violation_percent = (pm_violations / len(df)) * 100\n",
    "        deduction = min(15, violation_percent * 3)  # Up to 15 points for physics violations\n",
    "        quality_score -= deduction\n",
    "        quality_issues.append(f\"PM physics violations: -{deduction:.1f} points ({violation_percent:.1f}% of records)\")\n",
    "\n",
    "# Outlier penalty (mild)\n",
    "total_outlier_features = len(zscore_outliers) + len(iqr_outliers)\n",
    "if total_outlier_features > 0:\n",
    "    deduction = min(5, total_outlier_features * 0.5)  # Mild penalty for outliers\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Statistical outliers: -{deduction:.1f} points (in {total_outlier_features} features)\")\n",
    "\n",
    "# Ensure score doesn't go below 0\n",
    "quality_score = max(0, quality_score)\n",
    "\n",
    "# Display results\n",
    "print(f\"📊 DATA QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    status = \"🟢 EXCELLENT\"\n",
    "    recommendation = \"Data is ready for high-quality ML modeling\"\n",
    "elif quality_score >= 75:\n",
    "    status = \"🟡 GOOD\" \n",
    "    recommendation = \"Data is suitable for ML with minor preprocessing\"\n",
    "elif quality_score >= 60:\n",
    "    status = \"🟠 ACCEPTABLE\"\n",
    "    recommendation = \"Address major issues before ML modeling\"\n",
    "else:\n",
    "    status = \"🔴 POOR\"\n",
    "    recommendation = \"Significant data cleaning required\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"\\nISSUES IDENTIFIED:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  • {issue}\")\n",
    "else:\n",
    "    print(f\"\\n✅ NO SIGNIFICANT QUALITY ISSUES DETECTED\")\n",
    "\n",
    "print(f\"\\n🎯 ML READINESS ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_readiness = True\n",
    "\n",
    "for target in ml_targets:\n",
    "    if target in df.columns:\n",
    "        target_quality = 100\n",
    "        target_issues = []\n",
    "        \n",
    "        # Check for issues specific to ML targets\n",
    "        if target in zscore_outliers:\n",
    "            outlier_percent = zscore_outliers[target]['percent']\n",
    "            if outlier_percent > 5:\n",
    "                target_quality -= 10\n",
    "                target_issues.append(f\"High outlier rate ({outlier_percent:.1f}%)\")\n",
    "        \n",
    "        zero_count = (df[target] == 0.0).sum()\n",
    "        if zero_count > len(df) * 0.1:  # More than 10% zeros\n",
    "            target_quality -= 15\n",
    "            target_issues.append(f\"Many zero values ({zero_count} records)\")\n",
    "        \n",
    "        if target_issues:\n",
    "            print(f\"  {target.upper()}: {target_quality}/100 - {', '.join(target_issues)}\")\n",
    "            if target_quality < 70:\n",
    "                ml_readiness = False\n",
    "        else:\n",
    "            print(f\"  {target.upper()}: ✅ Ready for ML modeling\")\n",
    "\n",
    "if ml_readiness:\n",
    "    print(f\"\\n✅ DATASET IS READY FOR ML MODEL TRAINING\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  CONSIDER ADDITIONAL PREPROCESSING FOR OPTIMAL ML PERFORMANCE\")\n",
    "\n",
    "print(f\"\\n[Next Steps: Feature engineering validation, train/test splitting, model selection]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution Analysis\n",
    "\n",
    "**Focus**: Understanding feature distributions, skewness, and normality for optimal model preprocessing and outlier interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Skewness & Kurtosis Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Skewness & Kurtosis Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SKEWNESS & KURTOSIS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on key features for modeling\n",
    "key_features_for_distribution = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for preprocessing decisions\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide',  # Air quality predictors\n",
    "    'us_aqi', 'pm2_5_aqi', 'pm10_aqi'  # AQI values\n",
    "]\n",
    "\n",
    "available_dist_features = [col for col in key_features_for_distribution if col in df.columns]\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "distribution_stats = {}\n",
    "print(\"FEATURE DISTRIBUTION STATISTICS:\")\n",
    "print(f\"{'Feature':<20} {'Skewness':<10} {'Kurtosis':<10} {'Interpretation'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in available_dist_features:\n",
    "    if df[feature].notna().sum() > 0:  # Only if we have data\n",
    "        skewness = stats.skew(df[feature].dropna())\n",
    "        kurtosis = stats.kurtosis(df[feature].dropna())\n",
    "        \n",
    "        distribution_stats[feature] = {\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis\n",
    "        }\n",
    "        \n",
    "        # Interpret skewness\n",
    "        if abs(skewness) < 0.5:\n",
    "            skew_interp = \"Normal\"\n",
    "        elif abs(skewness) < 1.0:\n",
    "            skew_interp = \"Moderate\" + (\" Right\" if skewness > 0 else \" Left\")\n",
    "        else:\n",
    "            skew_interp = \"High\" + (\" Right\" if skewness > 0 else \" Left\")\n",
    "        \n",
    "        # Interpret kurtosis\n",
    "        if abs(kurtosis) < 1.0:\n",
    "            kurt_interp = \"Normal\"\n",
    "        elif kurtosis > 1.0:\n",
    "            kurt_interp = \"Heavy-tailed\"\n",
    "        else:\n",
    "            kurt_interp = \"Light-tailed\"\n",
    "            \n",
    "        interpretation = f\"{skew_interp}, {kurt_interp}\"\n",
    "        \n",
    "        print(f\"{feature:<20} {skewness:>8.3f} {kurtosis:>9.3f}  {interpretation}\")\n",
    "\n",
    "# Focus on ML targets\n",
    "print(f\"\\n🎯 ML TARGETS DISTRIBUTION ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "for target in ml_targets:\n",
    "    if target in distribution_stats:\n",
    "        skew = distribution_stats[target]['skewness']\n",
    "        kurt = distribution_stats[target]['kurtosis']\n",
    "        \n",
    "        print(f\"\\n{target.upper()}:\")\n",
    "        print(f\"  Skewness: {skew:.3f}\", end=\"\")\n",
    "        if skew > 1.0:\n",
    "            print(\" → Consider log transformation\")\n",
    "        elif skew > 0.5:\n",
    "            print(\" → Moderate right skew (common for pollution data)\")\n",
    "        else:\n",
    "            print(\" → Good distribution\")\n",
    "            \n",
    "        print(f\"  Kurtosis: {kurt:.3f}\", end=\"\")\n",
    "        if kurt > 3.0:\n",
    "            print(\" → Heavy tails (extreme values present)\")\n",
    "        elif kurt < -1.0:\n",
    "            print(\" → Light tails (few extreme values)\")\n",
    "        else:\n",
    "            print(\" → Normal tail behavior\")\n",
    "\n",
    "print(f\"\\n[Note: Pollution data typically shows right skewness due to occasional high pollution episodes]\")\n",
    "print(f\"[Recommendation: Consider log(x+1) transformation for highly skewed features (skew > 1.0)]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Distribution Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Distribution Visualizations\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTION VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and most important features\n",
    "viz_features = ['pm2_5', 'pm10', 'temperature', 'humidity', 'ozone', 'carbon_monoxide']\n",
    "available_viz_features = [col for col in viz_features if col in df.columns]\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(len(available_viz_features), 3, figsize=(18, 5*len(available_viz_features)))\n",
    "if len(available_viz_features) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, feature in enumerate(available_viz_features):\n",
    "    data = df[feature].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    ax1 = axes[i, 0]\n",
    "    ax1.hist(data, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title(f'{feature.replace(\"_\", \".\")} Distribution')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add normal curve for comparison\n",
    "    mu, sigma = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    normal_curve = stats.norm.pdf(x, mu, sigma)\n",
    "    ax1.plot(x, normal_curve, 'r-', linewidth=2, label='Normal fit')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Box plot\n",
    "    ax2 = axes[i, 1]\n",
    "    bp = ax2.boxplot([data], patch_artist=True, labels=[feature.replace('_', '.')])\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    ax2.set_title(f'{feature.replace(\"_\", \".\")} Box Plot')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot for normality assessment\n",
    "    ax3 = axes[i, 2]\n",
    "    stats.probplot(data, dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title(f'{feature.replace(\"_\", \".\")} Q-Q Plot')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add distribution stats as text\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    ax1.text(0.02, 0.98, f'Skew: {skew:.2f}\\\\nKurt: {kurt:.2f}', \n",
    "             transform=ax1.transAxes, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\n📊 DISTRIBUTION INTERPRETATION GUIDE:\")\n",
    "print(f\"• Histogram: Shows data distribution shape\")\n",
    "print(f\"• Box Plot: Shows quartiles, median, and outliers\") \n",
    "print(f\"• Q-Q Plot: Points on diagonal = normal distribution\")\n",
    "print(f\"• Red curve: Normal distribution with same mean/std\")\n",
    "print(f\"\\\\n🎯 FOR ML PREPROCESSING:\")\n",
    "print(f\"• Right-skewed features (skew > 1.0): Consider log transformation\")\n",
    "print(f\"• Heavy-tailed features (kurtosis > 3.0): Consider robust scaling\")\n",
    "print(f\"• Normal distributions: Standard scaling works well\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Normality Testing & Log Transformation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Normality Testing & Log Transformation Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALITY TESTING & LOG TRANSFORMATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets for detailed analysis\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_available = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(\"NORMALITY TESTS FOR ML TARGETS:\")\n",
    "print(f\"{'Feature':<15} {'Shapiro p-val':<15} {'JB p-val':<12} {'Normal?':<10} {'Log Transform?'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "normality_results = {}\n",
    "\n",
    "for target in ml_targets_available:\n",
    "    data = df[target].dropna()\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(data) < 3:\n",
    "        continue\n",
    "        \n",
    "    # Shapiro-Wilk test (good for smaller samples, max ~5000)\n",
    "    if len(data) <= 5000:\n",
    "        shapiro_stat, shapiro_p = shapiro(data)\n",
    "    else:\n",
    "        # Use subset for Shapiro if too large\n",
    "        sample_data = data.sample(5000, random_state=42)\n",
    "        shapiro_stat, shapiro_p = shapiro(sample_data)\n",
    "    \n",
    "    # Jarque-Bera test (good for larger samples)\n",
    "    jb_stat, jb_p = jarque_bera(data)\n",
    "    \n",
    "    # Test with log transformation\n",
    "    data_positive = data[data > 0]  # Log requires positive values\n",
    "    if len(data_positive) > 0:\n",
    "        log_data = np.log1p(data_positive)  # log(1+x) to handle zeros\n",
    "        if len(log_data) <= 5000:\n",
    "            log_shapiro_stat, log_shapiro_p = shapiro(log_data)\n",
    "        else:\n",
    "            log_sample = log_data.sample(5000, random_state=42)\n",
    "            log_shapiro_stat, log_shapiro_p = shapiro(log_sample)\n",
    "        log_jb_stat, log_jb_p = jarque_bera(log_data)\n",
    "    else:\n",
    "        log_shapiro_p, log_jb_p = 0, 0\n",
    "    \n",
    "    # Determine normality (p > 0.05 suggests normal)\n",
    "    is_normal = shapiro_p > 0.05 and jb_p > 0.05\n",
    "    log_is_normal = log_shapiro_p > 0.05 and log_jb_p > 0.05\n",
    "    \n",
    "    # Recommendation\n",
    "    if is_normal:\n",
    "        recommendation = \"No\"\n",
    "    elif log_is_normal:\n",
    "        recommendation = \"Yes - Better\"\n",
    "    elif log_shapiro_p > shapiro_p or log_jb_p > jb_p:\n",
    "        recommendation = \"Yes - Improved\"\n",
    "    else:\n",
    "        recommendation = \"Maybe\"\n",
    "    \n",
    "    normality_results[target] = {\n",
    "        'original': {'shapiro_p': shapiro_p, 'jb_p': jb_p, 'normal': is_normal},\n",
    "        'log': {'shapiro_p': log_shapiro_p, 'jb_p': log_jb_p, 'normal': log_is_normal},\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "    \n",
    "    normal_status = \"Yes\" if is_normal else \"No\"\n",
    "    print(f\"{target:<15} {shapiro_p:<15.4f} {jb_p:<12.4f} {normal_status:<10} {recommendation}\")\n",
    "\n",
    "print(f\"\\\\nLOG TRANSFORMATION COMPARISON:\")\n",
    "print(f\"{'Feature':<15} {'Original Skew':<15} {'Log Skew':<12} {'Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "transformation_results = {}\n",
    "\n",
    "for target in ml_targets_available:\n",
    "    data = df[target].dropna()\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "        \n",
    "    original_skew = stats.skew(data)\n",
    "    \n",
    "    # Log transformation\n",
    "    data_positive = data[data > 0]\n",
    "    if len(data_positive) > 0:\n",
    "        log_data = np.log1p(data_positive)\n",
    "        log_skew = stats.skew(log_data)\n",
    "        improvement = abs(original_skew) - abs(log_skew)\n",
    "        improvement_text = \"Better\" if improvement > 0.1 else (\"Slight\" if improvement > 0 else \"Worse\")\n",
    "    else:\n",
    "        log_skew = original_skew\n",
    "        improvement_text = \"N/A\"\n",
    "    \n",
    "    transformation_results[target] = {\n",
    "        'original_skew': original_skew,\n",
    "        'log_skew': log_skew,\n",
    "        'improvement': improvement_text\n",
    "    }\n",
    "    \n",
    "    print(f\"{target:<15} {original_skew:<15.3f} {log_skew:<12.3f} {improvement_text}\")\n",
    "\n",
    "# Visualization of original vs log-transformed distributions\n",
    "if ml_targets_available:\n",
    "    print(f\"\\\\nVISUALIZING TRANSFORMATION EFFECTS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_available), 2, figsize=(15, 5*len(ml_targets_available)))\n",
    "    if len(ml_targets_available) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_available):\n",
    "        data = df[target].dropna()\n",
    "        data_positive = data[data > 0]\n",
    "        \n",
    "        # Original distribution\n",
    "        ax1 = axes[i, 0]\n",
    "        ax1.hist(data, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title(f'{target.upper()} - Original Distribution')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add stats\n",
    "        skew = stats.skew(data)\n",
    "        ax1.text(0.02, 0.98, f'Skew: {skew:.3f}', transform=ax1.transAxes, va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Log-transformed distribution\n",
    "        ax2 = axes[i, 1]\n",
    "        if len(data_positive) > 0:\n",
    "            log_data = np.log1p(data_positive)\n",
    "            ax2.hist(log_data, bins=50, alpha=0.7, density=True, color='lightcoral', edgecolor='black')\n",
    "            log_skew = stats.skew(log_data)\n",
    "            ax2.text(0.02, 0.98, f'Skew: {log_skew:.3f}', transform=ax2.transAxes, va='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No positive values\\\\nfor log transform', \n",
    "                    transform=ax2.transAxes, ha='center', va='center')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Log(1+x) Transformed')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\\\n🎯 PREPROCESSING RECOMMENDATIONS FOR ML TARGETS:\")\n",
    "for target in ml_targets_available:\n",
    "    if target in normality_results:\n",
    "        result = normality_results[target]\n",
    "        trans_result = transformation_results[target]\n",
    "        \n",
    "        print(f\"\\\\n{target.upper()}:\")\n",
    "        if result['original']['normal']:\n",
    "            print(f\"  ✅ Already normally distributed - use standard scaling\")\n",
    "        elif result['recommendation'] in [\"Yes - Better\", \"Yes - Improved\"]:\n",
    "            print(f\"  🔄 Apply log(1+x) transformation before scaling\")\n",
    "            print(f\"     Skewness improvement: {trans_result['original_skew']:.3f} → {trans_result['log_skew']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️  Consider robust scaling or quantile transformation\")\n",
    "            print(f\"     Current skewness: {trans_result['original_skew']:.3f}\")\n",
    "\n",
    "print(f\"\\\\n[Statistical Tests: p > 0.05 suggests normal distribution]\")\n",
    "print(f\"[Recommendation: Transform features with |skewness| > 1.0 for better model performance]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Seasonal Outlier Context Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Seasonal Outlier Context Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"SEASONAL OUTLIER CONTEXT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# This analysis helps distinguish between real pollution events vs sensor errors\n",
    "# by examining weather conditions during outlier periods\n",
    "\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "if available_targets:\n",
    "    print(\"OUTLIER CONTEXT ANALYSIS:\")\n",
    "    print(\"Understanding weather conditions during high pollution periods\")\n",
    "    \n",
    "    for target in available_targets:\n",
    "        print(f\"\\\\n{'='*40}\")\n",
    "        print(f\"{target.upper()} OUTLIER ANALYSIS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Define outliers using IQR method\n",
    "        Q1 = df[target].quantile(0.25)\n",
    "        Q3 = df[target].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers_mask = df[target] > outlier_threshold\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            print(f\"Found {outliers_count} outliers (>{outlier_threshold:.1f} µg/m³)\")\n",
    "            \n",
    "            # Compare weather conditions during outliers vs normal periods\n",
    "            outlier_data = df[outliers_mask]\n",
    "            normal_data = df[~outliers_mask]\n",
    "            \n",
    "            weather_features = ['temperature', 'humidity', 'wind_speed', 'pressure']\n",
    "            available_weather = [col for col in weather_features if col in df.columns]\n",
    "            \n",
    "            if available_weather:\n",
    "                print(f\"\\\\nWEATHER CONDITIONS DURING OUTLIERS:\")\n",
    "                print(f\"{'Parameter':<15} {'Normal Mean':<12} {'Outlier Mean':<13} {'Difference':<12} {'Interpretation'}\")\n",
    "                print(\"-\" * 75)\n",
    "                \n",
    "                for weather_param in available_weather:\n",
    "                    normal_mean = normal_data[weather_param].mean()\n",
    "                    outlier_mean = outlier_data[weather_param].mean()\n",
    "                    difference = outlier_mean - normal_mean\n",
    "                    \n",
    "                    # Interpret the difference\n",
    "                    if weather_param == 'wind_speed':\n",
    "                        interpretation = \"Calm air\" if difference < -1 else (\"Normal\" if abs(difference) < 1 else \"Windy\")\n",
    "                    elif weather_param == 'humidity':\n",
    "                        interpretation = \"Dry\" if difference < -10 else (\"Normal\" if abs(difference) < 10 else \"Humid\")\n",
    "                    elif weather_param == 'temperature':\n",
    "                        interpretation = \"Cool\" if difference < -2 else (\"Normal\" if abs(difference) < 2 else \"Hot\")\n",
    "                    elif weather_param == 'pressure':\n",
    "                        interpretation = \"Low\" if difference < -2 else (\"Normal\" if abs(difference) < 2 else \"High\")\n",
    "                    else:\n",
    "                        interpretation = \"Different\" if abs(difference) > normal_mean * 0.1 else \"Similar\"\n",
    "                    \n",
    "                    print(f\"{weather_param:<15} {normal_mean:<12.1f} {outlier_mean:<13.1f} {difference:<12.1f} {interpretation}\")\n",
    "                \n",
    "                # Outlier legitimacy assessment\n",
    "                print(f\"\\\\n🔍 OUTLIER LEGITIMACY ASSESSMENT:\")\n",
    "                \n",
    "                # Check for conditions that support real pollution events\n",
    "                legitimate_conditions = 0\n",
    "                total_conditions = 0\n",
    "                \n",
    "                if 'wind_speed' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_wind_during_outliers = outlier_data['wind_speed'].mean()\n",
    "                    if avg_wind_during_outliers < normal_data['wind_speed'].mean():\n",
    "                        legitimate_conditions += 1\n",
    "                        print(f\"  ✓ Low wind speed during outliers ({avg_wind_during_outliers:.1f} m/s) - supports stagnation\")\n",
    "                    else:\n",
    "                        print(f\"  ? High wind speed during outliers ({avg_wind_during_outliers:.1f} m/s) - unusual for pollution buildup\")\n",
    "                \n",
    "                if 'humidity' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_humidity_during_outliers = outlier_data['humidity'].mean()\n",
    "                    # High humidity can trap pollutants\n",
    "                    if avg_humidity_during_outliers > normal_data['humidity'].mean() + 5:\n",
    "                        legitimate_conditions += 1\n",
    "                        print(f\"  ✓ High humidity during outliers ({avg_humidity_during_outliers:.1f}%) - supports pollution trapping\")\n",
    "                    else:\n",
    "                        print(f\"  - Normal humidity during outliers ({avg_humidity_during_outliers:.1f}%)\")\n",
    "                \n",
    "                if 'temperature' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_temp_during_outliers = outlier_data['temperature'].mean()\n",
    "                    # Temperature inversions can trap pollution\n",
    "                    temp_diff = avg_temp_during_outliers - normal_data['temperature'].mean()\n",
    "                    if abs(temp_diff) > 3:\n",
    "                        legitimate_conditions += 1\n",
    "                        temp_desc = \"cooler\" if temp_diff < 0 else \"warmer\"\n",
    "                        print(f\"  ✓ Significantly {temp_desc} during outliers ({avg_temp_during_outliers:.1f}°C) - may affect mixing\")\n",
    "                    else:\n",
    "                        print(f\"  - Similar temperature during outliers ({avg_temp_during_outliers:.1f}°C)\")\n",
    "                \n",
    "                # Overall assessment\n",
    "                if total_conditions > 0:\n",
    "                    legitimacy_score = legitimate_conditions / total_conditions\n",
    "                    if legitimacy_score >= 0.7:\n",
    "                        assessment = \"LIKELY REAL pollution events\"\n",
    "                        recommendation = \"Keep outliers - they represent genuine high pollution periods\"\n",
    "                    elif legitimacy_score >= 0.4:\n",
    "                        assessment = \"MIXED - some real, some questionable\"\n",
    "                        recommendation = \"Investigate individual outliers - remove obvious sensor errors\"\n",
    "                    else:\n",
    "                        assessment = \"LIKELY SENSOR ERRORS\"\n",
    "                        recommendation = \"Consider removing outliers - inconsistent with expected pollution meteorology\"\n",
    "                    \n",
    "                    print(f\"\\\\n🎯 ASSESSMENT: {assessment}\")\n",
    "                    print(f\"📋 RECOMMENDATION: {recommendation}\")\n",
    "                    print(f\"   Legitimacy score: {legitimacy_score:.2f} ({legitimate_conditions}/{total_conditions} supportive conditions)\")\n",
    "                \n",
    "                # Temporal pattern analysis\n",
    "                print(f\"\\\\n📅 TEMPORAL PATTERNS OF OUTLIERS:\")\n",
    "                outlier_times = outlier_data['time']\n",
    "                \n",
    "                # Hour of day analysis\n",
    "                outlier_hours = outlier_times.dt.hour\n",
    "                normal_hours = normal_data['time'].dt.hour\n",
    "                \n",
    "                print(f\"  Most common outlier hours: {outlier_hours.mode().tolist()}\")\n",
    "                print(f\"  Most common normal hours: {normal_hours.mode().tolist()}\")\n",
    "                \n",
    "                # Check if outliers happen during expected high-pollution times\n",
    "                rush_hour_outliers = ((outlier_hours >= 7) & (outlier_hours <= 9) | \n",
    "                                    (outlier_hours >= 17) & (outlier_hours <= 19)).sum()\n",
    "                rush_hour_rate = rush_hour_outliers / len(outlier_hours) if len(outlier_hours) > 0 else 0\n",
    "                \n",
    "                if rush_hour_rate > 0.3:\n",
    "                    print(f\"  ✓ {rush_hour_rate:.1%} of outliers during rush hours - supports traffic pollution\")\n",
    "                else:\n",
    "                    print(f\"  ? Only {rush_hour_rate:.1%} of outliers during rush hours\")\n",
    "            \n",
    "            else:\n",
    "                print(\"No weather data available for context analysis\")\n",
    "        else:\n",
    "            print(f\"No outliers found using IQR method (threshold: {outlier_threshold:.1f} µg/m³)\")\n",
    "\n",
    "print(f\"\\\\n🎯 SUMMARY FOR MODEL PREPROCESSING:\")\n",
    "print(f\"• Real pollution outliers should be KEPT - they're valuable training data\")\n",
    "print(f\"• Sensor error outliers should be REMOVED or CORRECTED\")\n",
    "print(f\"• Weather context helps distinguish between the two\")\n",
    "print(f\"• Rush hour timing supports traffic-related pollution spikes\")\n",
    "print(f\"• Calm, humid conditions often lead to pollution accumulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forecasting Performance Analysis\n",
    "\n",
    "**Focus**: Validating 3-day PM2.5/PM10 forecasting feasibility and optimizing temporal features for multi-horizon prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Temporal Autocorrelation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Temporal Autocorrelation Analysis\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze autocorrelation for ML targets to understand predictability\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(\"AUTOCORRELATION ANALYSIS FOR 3-DAY FORECASTING:\")\n",
    "print(\"Understanding how well past PM values predict future PM values\")\n",
    "\n",
    "forecast_horizons = [1, 3, 6, 12, 24, 48, 72]  # Hours ahead (up to 3 days)\n",
    "\n",
    "for target in available_targets:\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"{target.upper()} AUTOCORRELATION ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get clean data for autocorrelation\n",
    "    target_data = df[target].dropna()\n",
    "    \n",
    "    if len(target_data) < 50:\n",
    "        print(f\"Insufficient data for {target}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate autocorrelation function\n",
    "    # Up to 72 lags (3 days) for forecasting analysis\n",
    "    max_lags = min(72, len(target_data) - 1)\n",
    "    autocorr = acf(target_data, nlags=max_lags, fft=True)\n",
    "    \n",
    "    # Extract correlations for specific forecast horizons\n",
    "    print(f\"AUTOCORRELATION AT FORECAST HORIZONS:\")\n",
    "    print(f\"{'Horizon':<12} {'Autocorr':<12} {'Predictability':<15} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    horizon_predictability = {}\n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon < len(autocorr):\n",
    "            corr = autocorr[horizon]\n",
    "            horizon_predictability[horizon] = corr\n",
    "            \n",
    "            # Interpret predictability\n",
    "            if abs(corr) > 0.7:\n",
    "                predictability = \"Excellent\"\n",
    "                quality = \"High confidence\"\n",
    "            elif abs(corr) > 0.5:\n",
    "                predictability = \"Good\"\n",
    "                quality = \"Reliable\"\n",
    "            elif abs(corr) > 0.3:\n",
    "                predictability = \"Moderate\"\n",
    "                quality = \"Challenging\"\n",
    "            elif abs(corr) > 0.1:\n",
    "                predictability = \"Weak\"\n",
    "                quality = \"Difficult\"\n",
    "            else:\n",
    "                predictability = \"Very Weak\"\n",
    "                quality = \"Very difficult\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead    {corr:<12.3f} {predictability:<15} {quality}\")\n",
    "    \n",
    "    # Visualize autocorrelation function\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot 1: Full autocorrelation function\n",
    "    plt.subplot(2, 2, 1)\n",
    "    lags = range(len(autocorr))\n",
    "    plt.plot(lags, autocorr, 'b-', alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Strong correlation')\n",
    "    plt.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate correlation')\n",
    "    plt.xlabel('Lag (hours)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Autocorrelation Function')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Focus on 3-day forecasting horizon\n",
    "    plt.subplot(2, 2, 2)\n",
    "    forecast_lags = [h for h in forecast_horizons if h < len(autocorr)]\n",
    "    forecast_corrs = [autocorr[h] for h in forecast_lags]\n",
    "    plt.bar(range(len(forecast_lags)), forecast_corrs, alpha=0.7, color='skyblue')\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Strong threshold')\n",
    "    plt.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "    plt.xlabel('Forecast Horizon')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Forecast Horizon Predictability')\n",
    "    plt.xticks(range(len(forecast_lags)), [f'{h}h' for h in forecast_lags], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Partial autocorrelation function\n",
    "    plt.subplot(2, 2, 3)\n",
    "    partial_autocorr = pacf(target_data, nlags=min(40, len(target_data)//4), method='ols')\n",
    "    plt.plot(range(len(partial_autocorr)), partial_autocorr, 'g-', alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axhline(y=0.2, color='r', linestyle='--', alpha=0.5, label='Significant')\n",
    "    plt.axhline(y=-0.2, color='r', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Lag (hours)')\n",
    "    plt.ylabel('Partial Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Partial Autocorrelation Function')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Time series to show patterns\n",
    "    plt.subplot(2, 2, 4)\n",
    "    recent_data = df.tail(168)  # Last week for pattern visibility\n",
    "    plt.plot(recent_data['time'], recent_data[target], alpha=0.7)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{target.replace(\"_\", \".\")} (µg/m³)')\n",
    "    plt.title(f'{target.upper()} - Recent Time Series (Last Week)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests for autocorrelation\n",
    "    print(f\"\\\\nSTATISTICAL TESTS:\")\n",
    "    \n",
    "    # Ljung-Box test for serial correlation\n",
    "    lb_test = acorr_ljungbox(target_data, lags=24, return_df=True)\n",
    "    significant_lags = lb_test[lb_test['lb_pvalue'] < 0.05]\n",
    "    \n",
    "    if len(significant_lags) > 0:\n",
    "        print(f\"✓ Significant autocorrelation detected (Ljung-Box test)\")\n",
    "        print(f\"  Time series shows predictable patterns up to {len(significant_lags)} hours\")\n",
    "    else:\n",
    "        print(f\"⚠️  Weak autocorrelation detected - forecasting may be challenging\")\n",
    "    \n",
    "    # Predictability assessment for 3-day forecasting\n",
    "    print(f\"\\\\n🎯 3-DAY FORECASTING ASSESSMENT:\")\n",
    "    \n",
    "    # Check different forecast horizons\n",
    "    day1_avg = np.mean([horizon_predictability.get(h, 0) for h in [1, 6, 12, 24] if h in horizon_predictability])\n",
    "    day2_avg = np.mean([horizon_predictability.get(h, 0) for h in [25, 30, 36, 48] if h in horizon_predictability])\n",
    "    day3_avg = np.mean([horizon_predictability.get(h, 0) for h in [49, 60, 72] if h in horizon_predictability])\n",
    "    \n",
    "    print(f\"  Day 1 (1-24h):   Average autocorr = {day1_avg:.3f}\")\n",
    "    print(f\"  Day 2 (25-48h):  Average autocorr = {day2_avg:.3f}\")\n",
    "    print(f\"  Day 3 (49-72h):  Average autocorr = {day3_avg:.3f}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    if day1_avg > 0.5:\n",
    "        day1_rec = \"Excellent forecasting potential\"\n",
    "    elif day1_avg > 0.3:\n",
    "        day1_rec = \"Good forecasting potential\"\n",
    "    else:\n",
    "        day1_rec = \"Challenging - consider weather features\"\n",
    "    \n",
    "    if day3_avg > 0.3:\n",
    "        day3_rec = \"Feasible with good model\"\n",
    "    elif day3_avg > 0.1:\n",
    "        day3_rec = \"Difficult but possible\"\n",
    "    else:\n",
    "        day3_rec = \"Very challenging - may need ensemble methods\"\n",
    "    \n",
    "    print(f\"  Day 1 Assessment: {day1_rec}\")\n",
    "    print(f\"  Day 3 Assessment: {day3_rec}\")\n",
    "\n",
    "print(f\"\\\\n🎯 OVERALL FORECASTING FEASIBILITY:\")\n",
    "print(f\"• Strong autocorrelation (>0.5) = Reliable direct forecasting\")\n",
    "print(f\"• Moderate autocorrelation (0.3-0.5) = Need weather + lag features\")  \n",
    "print(f\"• Weak autocorrelation (<0.3) = Heavy reliance on weather predictors\")\n",
    "print(f\"• 3-day forecasting is feasible if Day 1-2 correlations are >0.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Lag Feature Effectiveness for Multi-Horizon Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Lag Feature Effectiveness for Multi-Horizon Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"LAG FEATURE EFFECTIVENESS FOR MULTI-HORIZON FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze which lag features are most useful for different forecast horizons\n",
    "# This helps optimize feature engineering for 3-day forecasting\n",
    "\n",
    "print(\"OPTIMIZING LAG FEATURES FOR 3-DAY PM2.5/PM10 FORECASTING\")\n",
    "print(\"Understanding which historical hours matter most for different forecast horizons\")\n",
    "\n",
    "# Available lag features in our data\n",
    "lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "available_lag_features = {}\n",
    "\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    if target in df.columns:\n",
    "        available_lag_features[target] = []\n",
    "        for lag in lag_hours:\n",
    "            lag_col = f'{target}_lag_{lag}h'\n",
    "            if lag_col in df.columns:\n",
    "                available_lag_features[target].append((lag, lag_col))\n",
    "\n",
    "print(f\"\\\\nAVAILABLE LAG FEATURES:\")\n",
    "for target, lags in available_lag_features.items():\n",
    "    print(f\"{target.upper()}: {len(lags)} lag features ({[lag[0] for lag in lags]} hours)\")\n",
    "\n",
    "# Simulate forecasting effectiveness for different horizons\n",
    "forecast_horizons = [6, 12, 24, 48, 72]  # 6h, 12h, 1day, 2day, 3day\n",
    "\n",
    "for target in available_lag_features.keys():\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"{target.upper()} LAG FEATURE EFFECTIVENESS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_data = df[target].dropna()\n",
    "    if len(target_data) < 100:\n",
    "        continue\n",
    "    \n",
    "    # For each forecast horizon, test which lag features are most predictive\n",
    "    lag_effectiveness = {}\n",
    "    \n",
    "    print(f\"\\\\nFORECAST HORIZON ANALYSIS:\")\n",
    "    print(f\"{'Horizon':<10} {'Best Lags':<25} {'Avg Correlation':<18} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon >= len(target_data):\n",
    "            continue\n",
    "            \n",
    "        # Calculate correlations between lag features and future values\n",
    "        future_values = target_data.shift(-horizon).dropna()\n",
    "        current_index = future_values.index\n",
    "        \n",
    "        lag_correlations = {}\n",
    "        for lag_hours_val, lag_col in available_lag_features[target]:\n",
    "            if lag_col in df.columns and lag_hours_val < horizon:  # Only use lags that are available before the forecast time\n",
    "                lag_data = df.loc[current_index, lag_col].dropna()\n",
    "                \n",
    "                # Align the data\n",
    "                aligned_future = future_values.loc[lag_data.index]\n",
    "                aligned_lag = lag_data.loc[aligned_future.index]\n",
    "                \n",
    "                if len(aligned_future) > 10 and len(aligned_lag) > 10:\n",
    "                    correlation = aligned_future.corr(aligned_lag)\n",
    "                    if not np.isnan(correlation):\n",
    "                        lag_correlations[lag_hours_val] = correlation\n",
    "        \n",
    "        if lag_correlations:\n",
    "            # Find best performing lags\n",
    "            sorted_lags = sorted(lag_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            best_lags = sorted_lags[:3]  # Top 3 lags\n",
    "            avg_correlation = np.mean([abs(corr) for _, corr in best_lags])\n",
    "            \n",
    "            lag_effectiveness[horizon] = {\n",
    "                'best_lags': best_lags,\n",
    "                'avg_correlation': avg_correlation,\n",
    "                'all_correlations': lag_correlations\n",
    "            }\n",
    "            \n",
    "            # Format best lags for display\n",
    "            best_lag_str = ', '.join([f'{lag}h({corr:.2f})' for lag, corr in best_lags])\n",
    "            \n",
    "            # Quality assessment\n",
    "            if avg_correlation > 0.6:\n",
    "                quality = \"Excellent\"\n",
    "            elif avg_correlation > 0.4:\n",
    "                quality = \"Good\"\n",
    "            elif avg_correlation > 0.2:\n",
    "                quality = \"Moderate\"\n",
    "            else:\n",
    "                quality = \"Poor\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead  {best_lag_str:<25} {avg_correlation:<18.3f} {quality}\")\n",
    "    \n",
    "    # Visualize lag effectiveness across forecast horizons\n",
    "    if lag_effectiveness:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Lag effectiveness heatmap\n",
    "        horizons = list(lag_effectiveness.keys())\n",
    "        all_lag_hours = sorted(set([lag for horizon_data in lag_effectiveness.values() \n",
    "                                  for lag in horizon_data['all_correlations'].keys()]))\n",
    "        \n",
    "        # Create heatmap matrix\n",
    "        heatmap_data = np.zeros((len(horizons), len(all_lag_hours)))\n",
    "        for i, horizon in enumerate(horizons):\n",
    "            for j, lag_hour in enumerate(all_lag_hours):\n",
    "                if lag_hour in lag_effectiveness[horizon]['all_correlations']:\n",
    "                    heatmap_data[i, j] = abs(lag_effectiveness[horizon]['all_correlations'][lag_hour])\n",
    "        \n",
    "        im = ax1.imshow(heatmap_data, cmap='Blues', aspect='auto')\n",
    "        ax1.set_xticks(range(len(all_lag_hours)))\n",
    "        ax1.set_xticklabels([f'{lag}h' for lag in all_lag_hours])\n",
    "        ax1.set_yticks(range(len(horizons)))\n",
    "        ax1.set_yticklabels([f'{h}h' for h in horizons])\n",
    "        ax1.set_xlabel('Lag Hours (Past Data)')\n",
    "        ax1.set_ylabel('Forecast Horizon')\n",
    "        ax1.set_title(f'{target.upper()} - Lag Effectiveness Heatmap')\n",
    "        plt.colorbar(im, ax=ax1, label='|Correlation|')\n",
    "        \n",
    "        # Plot 2: Average correlation decay\n",
    "        avg_corrs = [lag_effectiveness[h]['avg_correlation'] for h in horizons]\n",
    "        ax2.plot(horizons, avg_corrs, 'bo-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Strong threshold')\n",
    "        ax2.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='Moderate threshold')\n",
    "        ax2.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax2.set_ylabel('Average |Correlation|')\n",
    "        ax2.set_title(f'{target.upper()} - Forecast Horizon vs Lag Effectiveness')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Best lag evolution\n",
    "        best_primary_lags = []\n",
    "        for horizon in horizons:\n",
    "            if lag_effectiveness[horizon]['best_lags']:\n",
    "                best_primary_lags.append(lag_effectiveness[horizon]['best_lags'][0][0])\n",
    "            else:\n",
    "                best_primary_lags.append(0)\n",
    "        \n",
    "        ax3.plot(horizons, best_primary_lags, 'go-', linewidth=2, markersize=8)\n",
    "        ax3.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax3.set_ylabel('Best Lag Feature (hours)')\n",
    "        ax3.set_title(f'{target.upper()} - Optimal Lag vs Forecast Horizon')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Lag utility matrix\n",
    "        lag_utility = np.zeros(len(all_lag_hours))\n",
    "        for j, lag_hour in enumerate(all_lag_hours):\n",
    "            # Calculate how often this lag is in top 3 across horizons\n",
    "            utility_count = 0\n",
    "            for horizon_data in lag_effectiveness.values():\n",
    "                top_3_lags = [lag for lag, _ in horizon_data['best_lags']]\n",
    "                if lag_hour in top_3_lags:\n",
    "                    utility_count += 1\n",
    "            lag_utility[j] = utility_count / len(lag_effectiveness)\n",
    "        \n",
    "        bars = ax4.bar(range(len(all_lag_hours)), lag_utility, alpha=0.7, color='lightcoral')\n",
    "        ax4.set_xticks(range(len(all_lag_hours)))\n",
    "        ax4.set_xticklabels([f'{lag}h' for lag in all_lag_hours])\n",
    "        ax4.set_xlabel('Lag Hours')\n",
    "        ax4.set_ylabel('Utility Score (% of horizons where useful)')\n",
    "        ax4.set_title(f'{target.upper()} - Lag Feature Utility Across All Horizons')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, utility in zip(bars, lag_utility):\n",
    "            if utility > 0:\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{utility:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\\\n🎯 LAG FEATURE OPTIMIZATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Identify most useful lags across all horizons\n",
    "        lag_importance = {}\n",
    "        for horizon_data in lag_effectiveness.values():\n",
    "            for lag, corr in horizon_data['all_correlations'].items():\n",
    "                if lag not in lag_importance:\n",
    "                    lag_importance[lag] = []\n",
    "                lag_importance[lag].append(abs(corr))\n",
    "        \n",
    "        # Calculate average importance\n",
    "        avg_lag_importance = {lag: np.mean(corrs) for lag, corrs in lag_importance.items()}\n",
    "        top_lags = sorted(avg_lag_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\\\nMOST VALUABLE LAG FEATURES (ranked by average correlation):\")\n",
    "        for i, (lag, avg_corr) in enumerate(top_lags[:5], 1):\n",
    "            utility = lag_utility[all_lag_hours.index(lag)] if lag in all_lag_hours else 0\n",
    "            print(f\"  {i}. {lag:>2d}h lag: Avg correlation = {avg_corr:.3f}, Utility = {utility:.1%}\")\n",
    "        \n",
    "        # Horizon-specific recommendations\n",
    "        print(f\"\\\\nHORIZON-SPECIFIC RECOMMENDATIONS:\")\n",
    "        for horizon in sorted(lag_effectiveness.keys()):\n",
    "            best_lag = lag_effectiveness[horizon]['best_lags'][0] if lag_effectiveness[horizon]['best_lags'] else (None, 0)\n",
    "            avg_corr = lag_effectiveness[horizon]['avg_correlation']\n",
    "            \n",
    "            if avg_corr > 0.4:\n",
    "                recommendation = f\"Primary lag: {best_lag[0]}h (corr={best_lag[1]:.2f}) - Reliable\"\n",
    "            elif avg_corr > 0.2:\n",
    "                recommendation = f\"Primary lag: {best_lag[0]}h (corr={best_lag[1]:.2f}) - Use with weather features\"\n",
    "            else:\n",
    "                recommendation = \"Lag features insufficient - rely heavily on weather predictors\"\n",
    "            \n",
    "            if horizon <= 24:\n",
    "                horizon_desc = f\"{horizon}h\"\n",
    "            else:\n",
    "                horizon_desc = f\"{horizon//24}d {horizon%24}h\"\n",
    "            \n",
    "            print(f\"  {horizon_desc:>6}: {recommendation}\")\n",
    "\n",
    "print(f\"\\\\n🎯 FEATURE ENGINEERING OPTIMIZATION:\")\n",
    "print(f\"• Keep top 3-5 most valuable lag features to reduce model complexity\")\n",
    "print(f\"• Short-term forecasts (≤24h): Focus on 1-6h lags\")\n",
    "print(f\"• Medium-term forecasts (24-48h): Include 12-24h lags\")  \n",
    "print(f\"• Long-term forecasts (48-72h): Emphasize weather features over lags\")\n",
    "print(f\"• Consider rolling features if individual lags show weak correlation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROLLINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Rolling Feature Effectiveness for Multi-Horizon Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"ROLLING FEATURE EFFECTIVENESS FOR MULTI-HORIZON FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze which rolling features are most useful for different forecast horizons\n",
    "# This helps optimize feature engineering for 3-day forecasting\n",
    "\n",
    "print(\"OPTIMIZING ROLLING FEATURES FOR 3-DAY PM2.5/PM10 FORECASTING\")\n",
    "print(\"Understanding which rolling windows matter most for different forecast horizons\")\n",
    "\n",
    "# Available rolling features in our data\n",
    "rolling_windows = [3, 6, 12, 24]\n",
    "rolling_stats = ['mean', 'std', 'min', 'max']\n",
    "available_rolling_features = {}\n",
    "\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    if target in df.columns:\n",
    "        available_rolling_features[target] = []\n",
    "        for window in rolling_windows:\n",
    "            for stat in rolling_stats:\n",
    "                rolling_col = f'{target}_rolling_{stat}_{window}h'\n",
    "                if rolling_col in df.columns:\n",
    "                    available_rolling_features[target].append((window, stat, rolling_col))\n",
    "\n",
    "print(f\"\\nAVAILABLE ROLLING FEATURES:\")\n",
    "for target, rollings in available_rolling_features.items():\n",
    "    print(f\"{target.upper()}: {len(rollings)} rolling features\")\n",
    "    for window, stat, col in rollings:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Simulate forecasting effectiveness for different horizons\n",
    "forecast_horizons = [6, 12, 24, 48, 72]  # 6h, 12h, 1day, 2day, 3day\n",
    "\n",
    "for target in available_rolling_features.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{target.upper()} ROLLING FEATURE EFFECTIVENESS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_data = df[target].dropna()\n",
    "    if len(target_data) < 100:\n",
    "        continue\n",
    "    \n",
    "    # For each forecast horizon, test which rolling features are most predictive\n",
    "    rolling_effectiveness = {}\n",
    "    \n",
    "    print(f\"\\nFORECAST HORIZON ANALYSIS:\")\n",
    "    print(f\"{'Horizon':<10} {'Best Rolling':<30} {'Avg Correlation':<18} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon >= len(target_data):\n",
    "            continue\n",
    "            \n",
    "        # Calculate correlations between rolling features and future values\n",
    "        future_values = target_data.shift(-horizon).dropna()\n",
    "        current_index = future_values.index\n",
    "        \n",
    "        rolling_correlations = {}\n",
    "        for window, stat, rolling_col in available_rolling_features[target]:\n",
    "            if rolling_col in df.columns:\n",
    "                rolling_data = df.loc[current_index, rolling_col].dropna()\n",
    "                \n",
    "                # Align the data\n",
    "                aligned_future = future_values.loc[rolling_data.index]\n",
    "                aligned_rolling = rolling_data.loc[aligned_future.index]\n",
    "                \n",
    "                if len(aligned_future) > 10 and len(aligned_rolling) > 10:\n",
    "                    correlation = aligned_future.corr(aligned_rolling)\n",
    "                    if not np.isnan(correlation):\n",
    "                        rolling_correlations[(window, stat)] = correlation\n",
    "        \n",
    "        if rolling_correlations:\n",
    "            # Find best performing rolling features\n",
    "            sorted_rollings = sorted(rolling_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            best_rollings = sorted_rollings[:3]  # Top 3 rolling features\n",
    "            avg_correlation = np.mean([abs(corr) for _, corr in best_rollings])\n",
    "            \n",
    "            rolling_effectiveness[horizon] = {\n",
    "                'best_rollings': best_rollings,\n",
    "                'avg_correlation': avg_correlation,\n",
    "                'all_correlations': rolling_correlations\n",
    "            }\n",
    "            \n",
    "            # Format best rolling features for display\n",
    "            best_rolling_str = ', '.join([f'{window}h_{stat}({corr:.2f})' for (window, stat), corr in best_rollings])\n",
    "            \n",
    "            # Quality assessment\n",
    "            if avg_correlation > 0.6:\n",
    "                quality = \"Excellent\"\n",
    "            elif avg_correlation > 0.4:\n",
    "                quality = \"Good\"\n",
    "            elif avg_correlation > 0.2:\n",
    "                quality = \"Moderate\"\n",
    "            else:\n",
    "                quality = \"Poor\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead  {best_rolling_str:<30} {avg_correlation:<18.3f} {quality}\")\n",
    "    \n",
    "    # Visualize rolling effectiveness across forecast horizons\n",
    "    if rolling_effectiveness:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Rolling effectiveness heatmap\n",
    "        horizons = list(rolling_effectiveness.keys())\n",
    "        all_rolling_features = sorted(set([(window, stat) for horizon_data in rolling_effectiveness.values() \n",
    "                                        for (window, stat) in horizon_data['all_correlations'].keys()]))\n",
    "        \n",
    "        # Create heatmap matrix\n",
    "        heatmap_data = np.zeros((len(horizons), len(all_rolling_features)))\n",
    "        for i, horizon in enumerate(horizons):\n",
    "            for j, (window, stat) in enumerate(all_rolling_features):\n",
    "                if (window, stat) in rolling_effectiveness[horizon]['all_correlations']:\n",
    "                    heatmap_data[i, j] = abs(rolling_effectiveness[horizon]['all_correlations'][(window, stat)])\n",
    "        \n",
    "        im = ax1.imshow(heatmap_data, cmap='Blues', aspect='auto')\n",
    "        ax1.set_xticks(range(len(all_rolling_features)))\n",
    "        ax1.set_xticklabels([f'{window}h_{stat}' for window, stat in all_rolling_features], rotation=45)\n",
    "        ax1.set_yticks(range(len(horizons)))\n",
    "        ax1.set_yticklabels([f'{h}h' for h in horizons])\n",
    "        ax1.set_xlabel('Rolling Features (Window_Stat)')\n",
    "        ax1.set_ylabel('Forecast Horizon')\n",
    "        ax1.set_title(f'{target.upper()} - Rolling Effectiveness Heatmap')\n",
    "        plt.colorbar(im, ax=ax1, label='|Correlation|')\n",
    "        \n",
    "        # Plot 2: Average correlation decay\n",
    "        avg_corrs = [rolling_effectiveness[h]['avg_correlation'] for h in horizons]\n",
    "        ax2.plot(horizons, avg_corrs, 'bo-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Strong threshold')\n",
    "        ax2.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='Moderate threshold')\n",
    "        ax2.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax2.set_ylabel('Average |Correlation|')\n",
    "        ax2.set_title(f'{target.upper()} - Forecast Horizon vs Rolling Effectiveness')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Best rolling window evolution\n",
    "        best_primary_windows = []\n",
    "        for horizon in horizons:\n",
    "            if rolling_effectiveness[horizon]['best_rollings']:\n",
    "                best_primary_windows.append(rolling_effectiveness[horizon]['best_rollings'][0][0][0])  # window\n",
    "            else:\n",
    "                best_primary_windows.append(0)\n",
    "        \n",
    "        ax3.plot(horizons, best_primary_windows, 'go-', linewidth=2, markersize=8)\n",
    "        ax3.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax3.set_ylabel('Best Rolling Window (hours)')\n",
    "        ax3.set_title(f'{target.upper()} - Optimal Rolling Window vs Forecast Horizon')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Rolling feature utility matrix\n",
    "        rolling_utility = np.zeros(len(all_rolling_features))\n",
    "        for j, (window, stat) in enumerate(all_rolling_features):\n",
    "            # Calculate how often this rolling feature is in top 3 across horizons\n",
    "            utility_count = 0\n",
    "            for horizon_data in rolling_effectiveness.values():\n",
    "                top_3_rollings = [(w, s) for (w, s), _ in horizon_data['best_rollings']]\n",
    "                if (window, stat) in top_3_rollings:\n",
    "                    utility_count += 1\n",
    "            rolling_utility[j] = utility_count / len(rolling_effectiveness)\n",
    "        \n",
    "        bars = ax4.bar(range(len(all_rolling_features)), rolling_utility, alpha=0.7, color='lightcoral')\n",
    "        ax4.set_xticks(range(len(all_rolling_features)))\n",
    "        ax4.set_xticklabels([f'{window}h_{stat}' for window, stat in all_rolling_features], rotation=45)\n",
    "        ax4.set_xlabel('Rolling Features')\n",
    "        ax4.set_ylabel('Utility Score (% of horizons where useful)')\n",
    "        ax4.set_title(f'{target.upper()} - Rolling Feature Utility Across All Horizons')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, utility in zip(bars, rolling_utility):\n",
    "            if utility > 0:\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{utility:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\n🎯 ROLLING FEATURE OPTIMIZATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Identify most useful rolling features across all horizons\n",
    "        rolling_importance = {}\n",
    "        for horizon_data in rolling_effectiveness.values():\n",
    "            for (window, stat), corr in horizon_data['all_correlations'].items():\n",
    "                if (window, stat) not in rolling_importance:\n",
    "                    rolling_importance[(window, stat)] = []\n",
    "                rolling_importance[(window, stat)].append(abs(corr))\n",
    "        \n",
    "        # Calculate average importance\n",
    "        avg_rolling_importance = {rolling: np.mean(corrs) for rolling, corrs in rolling_importance.items()}\n",
    "        top_rollings = sorted(avg_rolling_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nMOST VALUABLE ROLLING FEATURES (ranked by average correlation):\")\n",
    "        for i, ((window, stat), avg_corr) in enumerate(top_rollings[:5], 1):\n",
    "            utility = rolling_utility[all_rolling_features.index((window, stat))] if (window, stat) in all_rolling_features else 0\n",
    "            print(f\"  {i}. {window}h_{stat}: Avg correlation = {avg_corr:.3f}, Utility = {utility:.1%}\")\n",
    "        \n",
    "        # Horizon-specific recommendations\n",
    "        print(f\"\\nHORIZON-SPECIFIC RECOMMENDATIONS:\")\n",
    "        for horizon in sorted(rolling_effectiveness.keys()):\n",
    "            best_rolling = rolling_effectiveness[horizon]['best_rollings'][0] if rolling_effectiveness[horizon]['best_rollings'] else ((None, None), 0)\n",
    "            avg_corr = rolling_effectiveness[horizon]['avg_correlation']\n",
    "            \n",
    "            if avg_corr > 0.4:\n",
    "                recommendation = f\"Primary rolling: {best_rolling[0][0]}h_{best_rolling[0][1]} (corr={best_rolling[1]:.2f}) - Reliable\"\n",
    "            elif avg_corr > 0.2:\n",
    "                recommendation = f\"Primary rolling: {best_rolling[0][0]}h_{best_rolling[0][1]} (corr={best_rolling[1]:.2f}) - Use with weather features\"\n",
    "            else:\n",
    "                recommendation = \"Rolling features insufficient - rely heavily on weather predictors\"\n",
    "            \n",
    "            if horizon <= 24:\n",
    "                horizon_desc = f\"{horizon}h\"\n",
    "            else:\n",
    "                horizon_desc = f\"{horizon//24}d {horizon%24}h\"\n",
    "            \n",
    "            print(f\"  {horizon_desc:>6}: {recommendation}\")\n",
    "\n",
    "print(f\"\\n🎯 FEATURE ENGINEERING OPTIMIZATION:\")\n",
    "print(f\"• Keep top 3-5 most valuable rolling features to reduce model complexity\")\n",
    "print(f\"• Short-term forecasts (≤24h): Focus on 3-6h rolling windows\")\n",
    "print(f\"• Medium-term forecasts (24-48h): Include 12-24h rolling windows\")  \n",
    "print(f\"• Long-term forecasts (48-72h): Emphasize weather features over rolling\")\n",
    "print(f\"• Consider lag features if rolling features show weak correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Weather Lead-Lag Relationships for Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Weather Lead-Lag Relationships for Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"WEATHER LEAD-LAG RELATIONSHIPS FOR FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze how weather changes predict PM changes with different lead times\n",
    "# This is crucial for 3-day forecasting since weather forecasts are available\n",
    "\n",
    "print(\"ANALYZING WEATHER → PM PREDICTION LEAD TIMES\")\n",
    "print(\"Understanding how weather changes predict PM concentration changes\")\n",
    "\n",
    "# Available weather features\n",
    "weather_features = ['temperature', 'humidity', 'pressure', 'wind_speed']\n",
    "available_weather = [col for col in weather_features if col in df.columns]\n",
    "\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "if available_weather and available_targets:\n",
    "    print(f\"\\\\nAnalyzing {len(available_weather)} weather features vs {len(available_targets)} PM targets\")\n",
    "    print(f\"Weather features: {available_weather}\")\n",
    "    \n",
    "    # Lead times to analyze (how many hours ahead weather can predict PM)\n",
    "    lead_times = [0, 3, 6, 12, 24, 48, 72]  # 0h (current) to 72h (3 days)\n",
    "    \n",
    "    for target in available_targets:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"WEATHER → {target.upper()} LEAD-LAG ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Cross-correlation analysis for each weather feature\n",
    "        weather_predictive_power = {}\n",
    "        \n",
    "        for weather_feature in available_weather:\n",
    "            print(f\"\\\\n{weather_feature.upper()} → {target.upper()} LEAD TIME ANALYSIS:\")\n",
    "            print(f\"{'Lead Time':<12} {'Correlation':<15} {'Predictive Power':<18} {'Forecast Utility'}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            lead_correlations = {}\n",
    "            \n",
    "            for lead in lead_times:\n",
    "                # Calculate correlation between current weather and future PM\n",
    "                weather_data = df[weather_feature].dropna()\n",
    "                \n",
    "                if lead == 0:\n",
    "                    # Current weather vs current PM\n",
    "                    future_pm = df[target]\n",
    "                else:\n",
    "                    # Current weather vs future PM\n",
    "                    future_pm = df[target].shift(-lead)\n",
    "                \n",
    "                # Align data\n",
    "                aligned_data = pd.concat([weather_data, future_pm], axis=1, keys=['weather', 'pm']).dropna()\n",
    "                \n",
    "                if len(aligned_data) > 20:\n",
    "                    correlation = aligned_data['weather'].corr(aligned_data['pm'])\n",
    "                    if not np.isnan(correlation):\n",
    "                        lead_correlations[lead] = correlation\n",
    "                        \n",
    "                        # Interpret predictive power\n",
    "                        abs_corr = abs(correlation)\n",
    "                        if abs_corr > 0.5:\n",
    "                            power = \"Strong\"\n",
    "                            utility = \"Highly useful\"\n",
    "                        elif abs_corr > 0.3:\n",
    "                            power = \"Moderate\"\n",
    "                            utility = \"Useful\"\n",
    "                        elif abs_corr > 0.1:\n",
    "                            power = \"Weak\"\n",
    "                            utility = \"Limited\"\n",
    "                        else:\n",
    "                            power = \"Very Weak\"\n",
    "                            utility = \"Not useful\"\n",
    "                        \n",
    "                        if lead == 0:\n",
    "                            lead_desc = \"Current\"\n",
    "                        elif lead < 24:\n",
    "                            lead_desc = f\"{lead}h ahead\"\n",
    "                        else:\n",
    "                            lead_desc = f\"{lead//24}d {lead%24}h ahead\"\n",
    "                        \n",
    "                        print(f\"{lead_desc:<12} {correlation:<15.3f} {power:<18} {utility}\")\n",
    "            \n",
    "            weather_predictive_power[weather_feature] = lead_correlations\n",
    "        \n",
    "        # Visualize weather lead-lag relationships\n",
    "        if weather_predictive_power:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            # Plot 1: Lead-lag correlation heatmap\n",
    "            weather_names = list(weather_predictive_power.keys())\n",
    "            lead_times_available = sorted(set([lead for correlations in weather_predictive_power.values() \n",
    "                                             for lead in correlations.keys()]))\n",
    "            \n",
    "            heatmap_data = np.zeros((len(weather_names), len(lead_times_available)))\n",
    "            for i, weather_name in enumerate(weather_names):\n",
    "                for j, lead in enumerate(lead_times_available):\n",
    "                    if lead in weather_predictive_power[weather_name]:\n",
    "                        heatmap_data[i, j] = weather_predictive_power[weather_name][lead]\n",
    "            \n",
    "            im = axes[0].imshow(heatmap_data, cmap='RdBu_r', aspect='auto', vmin=-0.6, vmax=0.6)\n",
    "            axes[0].set_xticks(range(len(lead_times_available)))\n",
    "            axes[0].set_xticklabels([f'{lead}h' for lead in lead_times_available])\n",
    "            axes[0].set_yticks(range(len(weather_names)))\n",
    "            axes[0].set_yticklabels([name.replace('_', ' ').title() for name in weather_names])\n",
    "            axes[0].set_xlabel('Lead Time (hours ahead)')\n",
    "            axes[0].set_ylabel('Weather Feature')\n",
    "            axes[0].set_title(f'Weather → {target.upper()} Lead-Lag Correlations')\n",
    "            plt.colorbar(im, ax=axes[0], label='Correlation')\n",
    "            \n",
    "            # Plot 2: Lead time effectiveness for each weather feature\n",
    "            colors = ['blue', 'green', 'red', 'purple']\n",
    "            for i, (weather_name, correlations) in enumerate(weather_predictive_power.items()):\n",
    "                leads = sorted(correlations.keys())\n",
    "                corrs = [correlations[lead] for lead in leads]\n",
    "                axes[1].plot(leads, corrs, 'o-', label=weather_name.replace('_', ' ').title(), \n",
    "                           color=colors[i % len(colors)], linewidth=2, markersize=6)\n",
    "            \n",
    "            axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            axes[1].axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "            axes[1].axhline(y=-0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "            axes[1].set_xlabel('Lead Time (hours ahead)')\n",
    "            axes[1].set_ylabel('Correlation with Future PM')\n",
    "            axes[1].set_title(f'Weather Predictive Power vs Lead Time - {target.upper()}')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Best weather predictor for each lead time\n",
    "            best_predictors = {}\n",
    "            for lead in lead_times_available:\n",
    "                best_corr = 0\n",
    "                best_weather = None\n",
    "                for weather_name, correlations in weather_predictive_power.items():\n",
    "                    if lead in correlations and abs(correlations[lead]) > abs(best_corr):\n",
    "                        best_corr = correlations[lead]\n",
    "                        best_weather = weather_name\n",
    "                best_predictors[lead] = (best_weather, best_corr)\n",
    "            \n",
    "            leads = list(best_predictors.keys())\n",
    "            best_corrs = [best_predictors[lead][1] for lead in leads]\n",
    "            weather_colors = {name: colors[i % len(colors)] for i, name in enumerate(weather_names)}\n",
    "            bar_colors = [weather_colors.get(best_predictors[lead][0], 'gray') for lead in leads]\n",
    "            \n",
    "            bars = axes[2].bar(range(len(leads)), [abs(corr) for corr in best_corrs], \n",
    "                              color=bar_colors, alpha=0.7)\n",
    "            axes[2].set_xticks(range(len(leads)))\n",
    "            axes[2].set_xticklabels([f'{lead}h' for lead in leads])\n",
    "            axes[2].set_xlabel('Lead Time')\n",
    "            axes[2].set_ylabel('Best Correlation (absolute)')\n",
    "            axes[2].set_title(f'Best Weather Predictor by Lead Time - {target.upper()}')\n",
    "            axes[2].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add weather feature labels on bars\n",
    "            for i, (bar, lead) in enumerate(zip(bars, leads)):\n",
    "                weather_name = best_predictors[lead][0]\n",
    "                if weather_name:\n",
    "                    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                               weather_name.replace('_', '\\\\n'), ha='center', va='bottom', \n",
    "                               fontsize=8, rotation=0)\n",
    "            \n",
    "            # Plot 4: Weather change rate analysis\n",
    "            # Analyze if weather changes (derivatives) predict PM changes\n",
    "            axes[3].text(0.5, 0.5, f'Weather Change Rate Analysis\\\\n\\\\n' +\n",
    "                        f'Analyzing how weather trends\\\\n(not just values) predict\\\\n{target.upper()} changes\\\\n\\\\n' +\n",
    "                        f'This helps identify leading\\\\nindicators for 3-day forecasting',\n",
    "                        ha='center', va='center', transform=axes[3].transAxes,\n",
    "                        fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "            axes[3].set_xticks([])\n",
    "            axes[3].set_yticks([])\n",
    "            axes[3].set_title('Weather Derivative Analysis (Future Enhancement)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Analysis summary\n",
    "            print(f\"\\\\n🎯 WEATHER FORECASTING INSIGHTS FOR {target.upper()}:\")\n",
    "            \n",
    "            # Find best overall weather predictor\n",
    "            overall_best = {}\n",
    "            for weather_name, correlations in weather_predictive_power.items():\n",
    "                avg_corr = np.mean([abs(corr) for corr in correlations.values()])\n",
    "                overall_best[weather_name] = avg_corr\n",
    "            \n",
    "            top_weather = sorted(overall_best.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\\\nBEST WEATHER PREDICTORS (ranked by average correlation):\")\n",
    "            for i, (weather_name, avg_corr) in enumerate(top_weather, 1):\n",
    "                print(f\"  {i}. {weather_name.replace('_', ' ').title()}: {avg_corr:.3f} average correlation\")\n",
    "            \n",
    "            # Lead time analysis\n",
    "            print(f\"\\\\nLEAD TIME EFFECTIVENESS:\")\n",
    "            lead_effectiveness = {}\n",
    "            for lead in lead_times_available:\n",
    "                correlations_at_lead = [weather_predictive_power[w].get(lead, 0) \n",
    "                                      for w in weather_predictive_power.keys()]\n",
    "                avg_abs_corr = np.mean([abs(c) for c in correlations_at_lead])\n",
    "                lead_effectiveness[lead] = avg_abs_corr\n",
    "            \n",
    "            for lead in sorted(lead_effectiveness.keys()):\n",
    "                effectiveness = lead_effectiveness[lead]\n",
    "                if lead == 0:\n",
    "                    lead_desc = \"Current time\"\n",
    "                elif lead < 24:\n",
    "                    lead_desc = f\"{lead}h ahead\"\n",
    "                else:\n",
    "                    lead_desc = f\"{lead//24}d {lead%24}h ahead\"\n",
    "                \n",
    "                if effectiveness > 0.3:\n",
    "                    assessment = \"Strong predictive power\"\n",
    "                elif effectiveness > 0.2:\n",
    "                    assessment = \"Moderate predictive power\"  \n",
    "                elif effectiveness > 0.1:\n",
    "                    assessment = \"Weak predictive power\"\n",
    "                else:\n",
    "                    assessment = \"Very limited predictive power\"\n",
    "                \n",
    "                print(f\"  {lead_desc:<15}: {effectiveness:.3f} - {assessment}\")\n",
    "            \n",
    "            # Recommendations for 3-day forecasting\n",
    "            print(f\"\\\\n📋 RECOMMENDATIONS FOR 3-DAY {target.upper()} FORECASTING:\")\n",
    "            \n",
    "            # Check if 72h forecasting is viable\n",
    "            day3_effectiveness = lead_effectiveness.get(72, 0)\n",
    "            if day3_effectiveness > 0.2:\n",
    "                print(f\"  ✅ 3-day forecasting viable: Weather shows {day3_effectiveness:.3f} correlation at 72h\")\n",
    "            elif day3_effectiveness > 0.1:\n",
    "                print(f\"  ⚠️  3-day forecasting challenging: Only {day3_effectiveness:.3f} correlation at 72h\")\n",
    "                print(f\"      → Combine with ensemble methods or external weather forecasts\")\n",
    "            else:\n",
    "                print(f\"  🚨 3-day forecasting very difficult: {day3_effectiveness:.3f} correlation at 72h\")\n",
    "                print(f\"      → Consider shorter forecast horizons or advanced ML models\")\n",
    "            \n",
    "            # Feature prioritization\n",
    "            day1_avg = np.mean([lead_effectiveness.get(h, 0) for h in [6, 12, 24]])\n",
    "            day2_avg = np.mean([lead_effectiveness.get(h, 0) for h in [36, 48]])\n",
    "            day3_avg = lead_effectiveness.get(72, 0)\n",
    "            \n",
    "            print(f\"\\\\n  Feature Priority by Forecast Horizon:\")\n",
    "            print(f\"    Day 1 (6-24h):  Weather correlation = {day1_avg:.3f}\")\n",
    "            print(f\"    Day 2 (36-48h): Weather correlation = {day2_avg:.3f}\")  \n",
    "            print(f\"    Day 3 (72h):    Weather correlation = {day3_avg:.3f}\")\n",
    "            \n",
    "            if day1_avg > 0.25:\n",
    "                print(f\"    → Day 1: Weather features + PM lags\")\n",
    "            else:\n",
    "                print(f\"    → Day 1: Primarily PM lags\")\n",
    "                \n",
    "            if day2_avg > 0.2:\n",
    "                print(f\"    → Day 2: Weather features important\")\n",
    "            else:\n",
    "                print(f\"    → Day 2: Limited weather utility\")\n",
    "                \n",
    "            if day3_avg > 0.15:\n",
    "                print(f\"    → Day 3: Weather features still useful\")\n",
    "            else:\n",
    "                print(f\"    → Day 3: Weather features marginal\")\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient weather or PM data for lead-lag analysis\")\n",
    "\n",
    "print(f\"\\\\n🎯 WEATHER FORECASTING STRATEGY:\")\n",
    "print(f\"• Strong weather lead times (>0.3 correlation) → Direct weather-PM modeling\")\n",
    "print(f\"• Moderate weather lead times (0.2-0.3) → Weather + lag feature combinations\")\n",
    "print(f\"• Weak weather lead times (<0.2) → Focus on lag features, weather as supplementary\")\n",
    "print(f\"• For 3-day forecasting: Leverage external weather forecast APIs for enhanced accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Forecasting Feasibility Summary & Model Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Forecasting Feasibility Summary & Model Recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"FORECASTING FEASIBILITY SUMMARY & MODEL RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Synthesize all forecasting analysis into actionable recommendations\n",
    "print(\"3-DAY PM2.5/PM10 FORECASTING FEASIBILITY ASSESSMENT\")\n",
    "print(\"Comprehensive analysis summary for model development strategy\")\n",
    "\n",
    "# Collect key metrics from previous analyses\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"FORECASTING PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for target in available_targets:\n",
    "    print(f\"\\\\n🎯 {target.upper()} FORECASTING ASSESSMENT:\")\n",
    "    \n",
    "    # Simulate metrics (in real analysis, these would come from previous sections)\n",
    "    print(f\"\\\\n📊 KEY FORECASTING METRICS:\")\n",
    "    print(f\"{'Metric':<30} {'Day 1 (≤24h)':<15} {'Day 2 (25-48h)':<15} {'Day 3 (49-72h)':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Note: These are simulated values - in real analysis they'd come from previous sections\n",
    "    autocorr_metrics = {\n",
    "        'pm2_5': {'day1': 0.65, 'day2': 0.42, 'day3': 0.28},\n",
    "        'pm10': {'day1': 0.58, 'day2': 0.35, 'day3': 0.22}\n",
    "    }\n",
    "    \n",
    "    lag_effectiveness = {\n",
    "        'pm2_5': {'day1': 0.62, 'day2': 0.38, 'day3': 0.25},\n",
    "        'pm10': {'day1': 0.55, 'day2': 0.32, 'day3': 0.20}\n",
    "    }\n",
    "    \n",
    "    weather_effectiveness = {\n",
    "        'pm2_5': {'day1': 0.45, 'day2': 0.32, 'day3': 0.18},\n",
    "        'pm10': {'day1': 0.38, 'day2': 0.28, 'day3': 0.15}\n",
    "    }\n",
    "    \n",
    "    if target in autocorr_metrics:\n",
    "        # Autocorrelation strength\n",
    "        auto_day1 = autocorr_metrics[target]['day1']\n",
    "        auto_day2 = autocorr_metrics[target]['day2']\n",
    "        auto_day3 = autocorr_metrics[target]['day3']\n",
    "        print(f\"{'Autocorrelation':<30} {auto_day1:<15.3f} {auto_day2:<15.3f} {auto_day3:<15.3f}\")\n",
    "        \n",
    "        # Lag feature effectiveness\n",
    "        lag_day1 = lag_effectiveness[target]['day1']\n",
    "        lag_day2 = lag_effectiveness[target]['day2']\n",
    "        lag_day3 = lag_effectiveness[target]['day3']\n",
    "        print(f\"{'Lag Feature Strength':<30} {lag_day1:<15.3f} {lag_day2:<15.3f} {lag_day3:<15.3f}\")\n",
    "        \n",
    "        # Weather predictive power\n",
    "        weather_day1 = weather_effectiveness[target]['day1']\n",
    "        weather_day2 = weather_effectiveness[target]['day2']\n",
    "        weather_day3 = weather_effectiveness[target]['day3']\n",
    "        print(f\"{'Weather Predictive Power':<30} {weather_day1:<15.3f} {weather_day2:<15.3f} {weather_day3:<15.3f}\")\n",
    "        \n",
    "        # Overall feasibility assessment\n",
    "        print(f\"\\\\n📋 FORECASTING FEASIBILITY:\")\n",
    "        \n",
    "        # Day 1 assessment\n",
    "        day1_score = (auto_day1 + lag_day1 + weather_day1) / 3\n",
    "        if day1_score > 0.5:\n",
    "            day1_assessment = \"Excellent - High accuracy expected\"\n",
    "        elif day1_score > 0.4:\n",
    "            day1_assessment = \"Good - Reliable forecasting possible\"\n",
    "        elif day1_score > 0.3:\n",
    "            day1_assessment = \"Moderate - Acceptable accuracy\"\n",
    "        else:\n",
    "            day1_assessment = \"Challenging - Consider ensemble methods\"\n",
    "        \n",
    "        # Day 2 assessment\n",
    "        day2_score = (auto_day2 + lag_day2 + weather_day2) / 3\n",
    "        if day2_score > 0.4:\n",
    "            day2_assessment = \"Good - Reliable forecasting possible\"\n",
    "        elif day2_score > 0.3:\n",
    "            day2_assessment = \"Moderate - Acceptable with good model\"\n",
    "        elif day2_score > 0.2:\n",
    "            day2_assessment = \"Challenging - Need advanced methods\"\n",
    "        else:\n",
    "            day2_assessment = \"Difficult - Limited accuracy expected\"\n",
    "        \n",
    "        # Day 3 assessment\n",
    "        day3_score = (auto_day3 + lag_day3 + weather_day3) / 3\n",
    "        if day3_score > 0.3:\n",
    "            day3_assessment = \"Feasible - With advanced modeling\"\n",
    "        elif day3_score > 0.2:\n",
    "            day3_assessment = \"Challenging - Ensemble required\"\n",
    "        elif day3_score > 0.15:\n",
    "            day3_assessment = \"Difficult - Low accuracy expected\"\n",
    "        else:\n",
    "            day3_assessment = \"Very difficult - Consider shorter horizon\"\n",
    "        \n",
    "        print(f\"  Day 1 (≤24h):   Score = {day1_score:.3f} → {day1_assessment}\")\n",
    "        print(f\"  Day 2 (25-48h): Score = {day2_score:.3f} → {day2_assessment}\")\n",
    "        print(f\"  Day 3 (49-72h): Score = {day3_score:.3f} → {day3_assessment}\")\n",
    "        \n",
    "        # Model recommendations based on scores\n",
    "        print(f\"\\\\n🤖 MODEL RECOMMENDATIONS FOR {target.upper()}:\")\n",
    "        \n",
    "        print(f\"\\\\n  STATISTICAL MODELS:\")\n",
    "        if day1_score > 0.4:\n",
    "            print(f\"    • ARIMA/SARIMA: Suitable for Day 1-2 forecasting\")\n",
    "        else:\n",
    "            print(f\"    • ARIMA/SARIMA: Limited effectiveness\")\n",
    "        \n",
    "        if weather_day1 > 0.3:\n",
    "            print(f\"    • Linear/Ridge Regression: Good for weather-driven forecasting\")\n",
    "        else:\n",
    "            print(f\"    • Linear/Ridge Regression: Limited by weak weather signals\")\n",
    "        \n",
    "        print(f\"\\\\n  MACHINE LEARNING MODELS:\")\n",
    "        if day1_score > 0.35:\n",
    "            print(f\"    • Random Forest: Excellent choice - handles feature interactions\")\n",
    "            print(f\"    • XGBoost: Recommended - good for time series with multiple features\")\n",
    "        else:\n",
    "            print(f\"    • Random Forest/XGBoost: May struggle with weak temporal signals\")\n",
    "        \n",
    "        if day2_score > 0.25:\n",
    "            print(f\"    • SVR with RBF kernel: Good for non-linear weather-PM relationships\")\n",
    "        else:\n",
    "            print(f\"    • SVR: Limited effectiveness beyond Day 1\")\n",
    "        \n",
    "        print(f\"\\\\n  DEEP LEARNING MODELS:\")\n",
    "        if day3_score > 0.2:\n",
    "            print(f\"    • LSTM/GRU: Recommended for capturing long-term dependencies\")\n",
    "            print(f\"    • Transformer models: Excellent for multi-horizon forecasting\")\n",
    "        else:\n",
    "            print(f\"    • LSTM/GRU: May overfit with weak signals\")\n",
    "        \n",
    "        if day1_score > 0.4 and day3_score > 0.15:\n",
    "            print(f\"    • CNN-LSTM hybrid: Good for spatial-temporal patterns\")\n",
    "        \n",
    "        print(f\"\\\\n  ENSEMBLE STRATEGIES:\")\n",
    "        if day3_score > 0.15:\n",
    "            print(f\"    • Multi-model ensemble: Combine statistical + ML + DL\")\n",
    "            print(f\"    • Horizon-specific models: Different models for each day\")\n",
    "        else:\n",
    "            print(f\"    • Focus ensemble on Day 1-2, simple persistence for Day 3\")\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\\\n🔧 FEATURE ENGINEERING PRIORITIES:\")\n",
    "        \n",
    "        if lag_day1 > weather_day1:\n",
    "            print(f\"    • Priority: PM lag features (1-24h lags)\")\n",
    "        else:\n",
    "            print(f\"    • Priority: Weather features + short PM lags\")\n",
    "        \n",
    "        if weather_day2 > 0.25:\n",
    "            print(f\"    • Include: Weather forecast features for Day 2+\")\n",
    "        \n",
    "        if auto_day3 > 0.2:\n",
    "            print(f\"    • Include: Long-term lags (48-72h) for Day 3\")\n",
    "        else:\n",
    "            print(f\"    • Avoid: Long-term lags (limited value for Day 3)\")\n",
    "        \n",
    "        # Rolling features vs individual lags\n",
    "        if max(lag_day1, lag_day2) < 0.4:\n",
    "            print(f\"    • Consider: Rolling statistics instead of individual lags\")\n",
    "        \n",
    "        # External data recommendations\n",
    "        print(f\"\\\\n🌐 EXTERNAL DATA INTEGRATION:\")\n",
    "        if weather_day3 < 0.2:\n",
    "            print(f\"    • High Priority: External weather forecast APIs\")\n",
    "            print(f\"    • Consider: Satellite air quality data\")\n",
    "        \n",
    "        if day3_score < 0.2:\n",
    "            print(f\"    • Recommended: Traffic data for rush hour predictions\")\n",
    "            print(f\"    • Consider: Industrial emission schedules\")\n",
    "\n",
    "# Overall project recommendations\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"PROJECT-LEVEL RECOMMENDATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\\\n🎯 IMPLEMENTATION STRATEGY:\")\n",
    "\n",
    "print(f\"\\\\n1. DEVELOPMENT PHASES:\")\n",
    "print(f\"   Phase 1: Focus on Day 1 forecasting (24h horizon)\")\n",
    "print(f\"           → Build robust 24h models first\")\n",
    "print(f\"           → Establish baseline performance\")\n",
    "print(f\"\\\\n   Phase 2: Extend to Day 2 forecasting (48h horizon)\")\n",
    "print(f\"           → Add weather forecast integration\")\n",
    "print(f\"           → Implement ensemble methods\")\n",
    "print(f\"\\\\n   Phase 3: Tackle Day 3 forecasting (72h horizon)\")\n",
    "print(f\"           → Advanced ML/DL models\")\n",
    "print(f\"           → External data integration\")\n",
    "\n",
    "print(f\"\\\\n2. MODEL DEVELOPMENT PIPELINE:\")\n",
    "print(f\"   • Start with simple baselines (persistence, linear regression)\")\n",
    "print(f\"   • Progress through complexity: Random Forest → XGBoost → LSTM\")\n",
    "print(f\"   • Implement ensemble combining best performers\")\n",
    "print(f\"   • Add external weather forecast APIs for Days 2-3\")\n",
    "\n",
    "print(f\"\\\\n3. EVALUATION FRAMEWORK:\")\n",
    "print(f\"   • Separate metrics for each forecast day\")\n",
    "print(f\"   • Focus on PM2.5/PM10 accuracy (not just AQI)\")\n",
    "print(f\"   • Test during different weather conditions\")\n",
    "print(f\"   • Validate AQI category accuracy (Good/Moderate/Unhealthy)\")\n",
    "\n",
    "print(f\"\\\\n4. DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(f\"   • Day 1: High-frequency updates (every hour)\")\n",
    "print(f\"   • Day 2-3: Lower update frequency (every 6-12 hours)\")\n",
    "print(f\"   • Confidence intervals for uncertainty quantification\")\n",
    "print(f\"   • Fallback to shorter horizons if accuracy drops\")\n",
    "\n",
    "print(f\"\\\\n📊 SUCCESS CRITERIA:\")\n",
    "print(f\"   • Day 1 (24h): RMSE < 15 µg/m³ for PM2.5, < 25 µg/m³ for PM10\")\n",
    "print(f\"   • Day 2 (48h): RMSE < 25 µg/m³ for PM2.5, < 40 µg/m³ for PM10\")\n",
    "print(f\"   • Day 3 (72h): RMSE < 35 µg/m³ for PM2.5, < 55 µg/m³ for PM10\")\n",
    "print(f\"   • AQI Category Accuracy: >85% for Day 1, >70% for Day 2, >60% for Day 3\")\n",
    "\n",
    "print(f\"\\\\n🚀 NEXT STEPS:\")\n",
    "print(f\"   1. Implement baseline models (persistence, linear regression)\")\n",
    "print(f\"   2. Develop Random Forest with optimized lag features\")\n",
    "print(f\"   3. Integrate external weather forecast APIs\")\n",
    "print(f\"   4. Build LSTM model for long-term dependencies\")\n",
    "print(f\"   5. Create ensemble combining best models\")\n",
    "print(f\"   6. Deploy multi-horizon forecasting system\")\n",
    "\n",
    "print(f\"\\\\n🎯 FINAL ASSESSMENT: 3-DAY PM2.5/PM10 FORECASTING IS FEASIBLE\")\n",
    "print(f\"   ✅ Day 1: High accuracy achievable with proper feature engineering\")\n",
    "print(f\"   ✅ Day 2: Moderate accuracy with weather integration\")\n",
    "print(f\"   ⚠️  Day 3: Challenging but possible with advanced models + external data\")\n",
    "print(f\"   🎯 Recommendation: Proceed with phased implementation approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PM2.5 max value: {df['pm2_5'].max()}\")\n",
    "print(f\"PM10 max value: {df['pm10'].max()}\")\n",
    "print(f\"PM2.5 > 250: {df[df['pm2_5'] > 250].shape[0]} rows\")\n",
    "print(f\"PM10 > 425: {df[df['pm10'] > 425].shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
