{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA - Multan AQI Features\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Hopsworks Feature Store (multan_aqi_features)\n",
    "- **Records**: 538 observations \n",
    "- **Features**: 127 engineered features\n",
    "- **Time Range**: June 16, 2025 - July 8, 2025\n",
    "\n",
    "## Modeling Approach\n",
    "- **üéØ Goal**: Accurate US AQI prediction for Multan\n",
    "- **üîß Method**: Train ML model to predict PM2.5 & PM10 ‚Üí Calculate AQI via EPA formula\n",
    "- **üìä ML Targets**: pm2_5, pm10 concentrations (¬µg/m¬≥)\n",
    "- **‚úÖ Success Metric**: How well calculated AQI matches actual AQI values\n",
    "\n",
    "## Feature Categories\n",
    "1. **Raw Air Quality**: pm2_5, pm10, co, no2, so2, o3, nh3\n",
    "2. **AQI Calculations**: pm2_5_aqi, pm10_aqi, us_aqi, openweather_aqi\n",
    "3. **Weather Data**: temperature, humidity, pressure, wind_speed, wind_direction\n",
    "4. **Time Features**: Cyclical encodings (hour, day, month, etc.)\n",
    "5. **Lag Features**: 1h-72h historical values\n",
    "6. **Rolling Statistics**: 3h-24h windows (mean, std, min, max)\n",
    "7. **Engineered Features**: Interactions, squared terms, categorical flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store that will be used for modeling.\n",
    "\n",
    "**EDA Focus**: Understanding relationships that help predict PM2.5 and PM10 concentrations accurately, which leads to better AQI predictions.\n",
    "\n",
    "## 1. Data Overview\n",
    "Loading and examining the basic structure of our modeling dataset from Hopsworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import configuration\n",
    "from config import HOPSWORKS_CONFIG\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks and load data\n",
    "print(\"Connecting to Hopsworks...\")\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_CONFIG[\"api_key\"], project=HOPSWORKS_CONFIG[\"project_name\"])\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Loading feature group data...\")\n",
    "fg = fs.get_feature_group(HOPSWORKS_CONFIG[\"feature_group_name\"], version=1)\n",
    "df = fg.read()\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} records from Hopsworks\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column references and prepare data\n",
    "# The actual timestamp column is called 'time' not 'timestamp'\n",
    "print(\"Data preparation and column check...\")\n",
    "print(f\"Time column: {'time' if 'time' in df.columns else 'timestamp not found'}\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "\n",
    "# Ensure time is datetime\n",
    "if df['time'].dtype == 'object':\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "print(\"‚úì Data sorted by time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "\n",
    "# Detailed timestamp analysis\n",
    "print(f\"\\nTIMESTAMP ANALYSIS:\")\n",
    "if 'time' in df.columns:\n",
    "    df_sorted = df.sort_values('time')\n",
    "    time_diffs = df_sorted['time'].diff().dropna()\n",
    "    \n",
    "    print(f\"  Earliest record: {df['time'].min()}\")\n",
    "    print(f\"  Latest record: {df['time'].max()}\")\n",
    "    print(f\"  Total duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "    print(f\"  Most common interval: {time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Records per day average: {len(df) / max(1, (df['time'].max() - df['time'].min()).days):.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"COLUMNS:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:2d}. {col:<25} {str(df[col].dtype):<15}\")\n",
    "\n",
    "print()\n",
    "print(\"FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'time' in numeric_cols:\n",
    "    numeric_cols.remove('time')\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Datetime features ({len(datetime_cols)}): {datetime_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last few records\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 records:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\nRandom 5 records:\")\n",
    "display(df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric features\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nAIR QUALITY FEATURES SUMMARY:\")\n",
    "aqi_features = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "aqi_present = [col for col in aqi_features if col in df.columns]\n",
    "if aqi_present:\n",
    "    display(df[aqi_present].describe())\n",
    "\n",
    "print(\"\\nWEATHER FEATURES SUMMARY:\")\n",
    "weather_features = ['temperature', 'feels_like', 'humidity', 'pressure', 'visibility', 'wind_speed', 'wind_direction', 'cloud_cover']\n",
    "weather_present = [col for col in weather_features if col in df.columns]\n",
    "if weather_present:\n",
    "    display(df[weather_present].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis\n",
    "\n",
    "Analyzing temporal patterns in PM concentrations (our prediction targets) and derived AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plots for key air quality metrics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# PM2.5 over time\n",
    "axes[0].plot(df['time'], df['pm2_5'], alpha=0.7, color='red')\n",
    "axes[0].set_title('PM2.5 Concentration Over Time')\n",
    "axes[0].set_ylabel('PM2.5 (¬µg/m¬≥)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PM10 over time  \n",
    "axes[1].plot(df['time'], df['pm10'], alpha=0.7, color='orange')\n",
    "axes[1].set_title('PM10 Concentration Over Time')\n",
    "axes[1].set_ylabel('PM10 (¬µg/m¬≥)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# US AQI over time\n",
    "axes[2].plot(df['time'], df['us_aqi'], alpha=0.7, color='purple')\n",
    "axes[2].set_title('US AQI Over Time')\n",
    "axes[2].set_ylabel('US AQI')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI category colors as background\n",
    "aqi_levels = [\n",
    "    (0, 50, 'green', 'Good'),\n",
    "    (51, 100, 'yellow', 'Moderate'), \n",
    "    (101, 150, 'orange', 'Unhealthy for Sensitive'),\n",
    "    (151, 200, 'red', 'Unhealthy'),\n",
    "    (201, 300, 'purple', 'Very Unhealthy'),\n",
    "    (301, 500, 'maroon', 'Hazardous')\n",
    "]\n",
    "\n",
    "for min_val, max_val, color, label in aqi_levels:\n",
    "    axes[2].axhspan(min_val, max_val, alpha=0.1, color=color, label=label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series summary - ML TARGETS and GOAL METRIC\n",
    "print(f\"Time Series Summary (ML Targets + Goal Metric):\")\n",
    "print(f\"PM2.5 (ML Target): {df['pm2_5'].min():.1f} - {df['pm2_5'].max():.1f} ¬µg/m¬≥\")\n",
    "print(f\"PM10 (ML Target):  {df['pm10'].min():.1f} - {df['pm10'].max():.1f} ¬µg/m¬≥\") \n",
    "print(f\"US AQI (Goal Metric): {df['us_aqi'].min():.1f} - {df['us_aqi'].max():.1f}\")\n",
    "print(f\"\\nModeling Approach: Predict PM concentrations ‚Üí Calculate AQI ‚Üí Evaluate AQI accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis and Correlations\n",
    "\n",
    "Examining relationships between air quality and weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis - ML targets + environmental predictors\n",
    "key_features = [\n",
    "    'pm2_5', 'pm10',  # ML targets\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Weather predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Pollutant predictors\n",
    "]\n",
    "# Note: Excluding us_aqi since it's derived from PM targets\n",
    "\n",
    "# Filter features that exist in our dataset\n",
    "available_features = [col for col in key_features if col in df.columns]\n",
    "print(f\"Analyzing correlations for {len(available_features)} key features:\")\n",
    "print(available_features)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[available_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix (Key Variables)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on ML TARGETS (PM concentrations) - what we need to predict well\n",
    "print(\"\\nStrongest correlations with PM2.5 (ML Target):\")\n",
    "pm25_corr = corr_matrix['pm2_5'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm25_corr.head(8).items():\n",
    "    if feature != 'pm2_5':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nStrongest correlations with PM10 (ML Target):\")\n",
    "pm10_corr = corr_matrix['pm10'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm10_corr.head(8).items():\n",
    "    if feature != 'pm10':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ CORRELATION FOCUS:\")\n",
    "print(\"Understanding which environmental factors help predict PM concentrations accurately\")\n",
    "print(\"Better PM predictions ‚Üí More accurate AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ML TARGETS (PM concentrations) - what affects our predictions\n",
    "print(\"=\"*60)\n",
    "print(\"ML TARGET CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_features = ['pm2_5', 'pm10']\n",
    "predictor_features = ['temperature', 'humidity', 'pressure', 'wind_speed', \n",
    "                     'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "\n",
    "available_predictors = [col for col in predictor_features if col in df.columns]\n",
    "\n",
    "for target in target_features:\n",
    "    if target in df.columns:\n",
    "        print(f\"\\nStrongest correlations with {target.upper()} (Target Variable):\")\n",
    "        target_corr = df[[target] + available_predictors].corr()[target].abs().sort_values(ascending=False)\n",
    "        for feature, corr in target_corr.head(6).items():\n",
    "            if feature != target:\n",
    "                print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n[EDA Focus: Understanding what predicts PM concentrations well]\")\n",
    "print(f\"[Goal: Better PM predictions ‚Üí More accurate AQI calculations]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AQI Dominance Analysis\n",
    "\n",
    "**Key Question**: Which pollutants actually drive AQI values? \n",
    "\n",
    "Since EPA AQI = MAX(individual pollutant AQIs), we need to check if other criteria pollutants sometimes create higher AQI than PM2.5/PM10. This validates our modeling approach of using only PM concentrations.\n",
    "\n",
    "**EPA Criteria Pollutants Analyzed**: PM2.5, PM10, O3, CO, NO2, SO2  \n",
    "*(Only these 6 pollutants have official EPA AQI breakpoints and affect AQI calculations)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA AQI Calculation Functions for All Pollutants\n",
    "def calculate_aqi_from_concentration(concentration, breakpoints):\n",
    "    \"\"\"Calculate AQI from pollutant concentration using EPA breakpoints - FIXED VERSION\"\"\"\n",
    "    if pd.isna(concentration) or concentration < 0:\n",
    "        return 0\n",
    "    \n",
    "    for C_low, C_high, I_low, I_high in breakpoints:\n",
    "        if C_low <= concentration <= C_high:\n",
    "            # Linear interpolation within the bracket (same formula as feature_engineering.py)\n",
    "            aqi = round((I_high - I_low) / (C_high - C_low) * (concentration - C_low) + I_low)\n",
    "            return aqi\n",
    "    \n",
    "    # ‚úÖ FIXED: Return None instead of 500 when no breakpoint matches\n",
    "    # This indicates data quality issue rather than assuming hazardous level\n",
    "    return None\n",
    "\n",
    "# EPA AQI Breakpoints (concentration ranges and corresponding AQI ranges)\n",
    "EPA_BREAKPOINTS = {\n",
    "    'pm2_5': [\n",
    "        (0.0, 12.0, 0, 50),\n",
    "        (12.1, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 350.4, 301, 400),  # ‚Üê ADD THIS MISSING RANGE\n",
    "        (350.5, 500.4, 401, 500),  # ‚Üê CORRECT 401-500 RANGE\n",
    "    ],\n",
    "    'pm10': [\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 154, 51, 100),\n",
    "        (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200),\n",
    "        (355, 424, 201, 300),\n",
    "        (425, 504, 301, 400),  # ‚Üê ADD THIS MISSING RANGE\n",
    "        (505, 604, 401, 500),  # ‚Üê CORRECT 401-500 RANGE\n",
    "    ],\n",
    "    'ozone': [  # ppb, 8-hour average (converting from ¬µg/m¬≥ if needed)\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 70, 51, 100),\n",
    "        (71, 85, 101, 150),\n",
    "        (86, 105, 151, 200),\n",
    "        (106, 200, 201, 300)\n",
    "    ],\n",
    "    'carbon_monoxide': [  # ppm, 8-hour average\n",
    "        (0.0, 4.4, 0, 50),\n",
    "        (4.5, 9.4, 51, 100),\n",
    "        (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200),\n",
    "        (15.5, 30.4, 201, 300),\n",
    "        (30.5, 50.4, 301, 500)\n",
    "    ],\n",
    "    'nitrogen_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 53, 0, 50),\n",
    "        (54, 100, 51, 100),\n",
    "        (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200),\n",
    "        (650, 1249, 201, 300),\n",
    "        (1250, 2049, 301, 500)\n",
    "    ],\n",
    "    'sulphur_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 35, 0, 50),\n",
    "        (36, 75, 51, 100),\n",
    "        (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200),\n",
    "        (305, 604, 201, 300),\n",
    "        (605, 1004, 301, 500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"EPA AQI Calculation Functions Loaded\")\n",
    "print(f\"EPA Criteria Pollutants (official breakpoints): {list(EPA_BREAKPOINTS.keys())}\")\n",
    "\n",
    "# Check which EPA criteria pollutants we have in our data\n",
    "available_pollutants = []\n",
    "for pollutant in EPA_BREAKPOINTS.keys():\n",
    "    if pollutant in df.columns:\n",
    "        available_pollutants.append(pollutant)\n",
    "        \n",
    "print(f\"\\nEPA criteria pollutants in our dataset: {available_pollutants}\")\n",
    "print(f\"Missing from dataset: {[p for p in EPA_BREAKPOINTS.keys() if p not in df.columns]}\")\n",
    "print(f\"\\nAnalyzing {len(available_pollutants)} pollutants that can affect AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual AQI for each available pollutant\n",
    "aqi_results = df[['time', 'us_aqi']].copy()\n",
    "\n",
    "# Unit conversions (if needed)\n",
    "df_calc = df.copy()\n",
    "\n",
    "# Convert units for certain pollutants if needed\n",
    "# Ozone: ¬µg/m¬≥ to ppb (approximate: ¬µg/m¬≥ * 0.5 ‚âà ppb at standard conditions)\n",
    "if 'ozone' in df_calc.columns:\n",
    "    df_calc['ozone_ppb'] = df_calc['ozone'] * 0.5  # Rough conversion\n",
    "    \n",
    "# CO might need conversion from ¬µg/m¬≥ to ppm\n",
    "if 'carbon_monoxide' in df_calc.columns:\n",
    "    df_calc['co_ppm'] = df_calc['carbon_monoxide'] * 0.000873  # Rough conversion\n",
    "\n",
    "print(\"Calculating individual AQI for each pollutant...\")\n",
    "\n",
    "# Calculate AQI for each pollutant\n",
    "for pollutant in available_pollutants:\n",
    "    col_name = f'{pollutant}_individual_aqi'\n",
    "    \n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        concentrations = df_calc['ozone_ppb']\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        concentrations = df_calc['co_ppm']\n",
    "    else:\n",
    "        concentrations = df_calc[pollutant]\n",
    "    \n",
    "    aqi_results[col_name] = concentrations.apply(\n",
    "        lambda x: calculate_aqi_from_concentration(x, EPA_BREAKPOINTS[pollutant])\n",
    "    )\n",
    "    \n",
    "    # Handle None values (data outside breakpoint ranges)\n",
    "    null_count = aqi_results[col_name].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  {pollutant}: {null_count} values outside breakpoint ranges, replacing with 0\")\n",
    "        aqi_results[col_name] = aqi_results[col_name].fillna(0)\n",
    "    \n",
    "    # Now safe to calculate min/max\n",
    "    valid_values = aqi_results[col_name].dropna()\n",
    "    if len(valid_values) > 0:\n",
    "        print(f\"‚úì {pollutant}: {aqi_results[col_name].min():.0f} - {aqi_results[col_name].max():.0f} AQI\")\n",
    "    else:\n",
    "        print(f\"‚úó {pollutant}: No valid AQI values calculated\")\n",
    "\n",
    "# Find controlling pollutant (max AQI) for each timestamp\n",
    "individual_aqi_cols = [col for col in aqi_results.columns if 'individual_aqi' in col]\n",
    "aqi_results['calculated_max_aqi'] = aqi_results[individual_aqi_cols].max(axis=1)\n",
    "aqi_results['controlling_pollutant'] = aqi_results[individual_aqi_cols].idxmax(axis=1)\n",
    "\n",
    "# Clean up pollutant names\n",
    "aqi_results['controlling_pollutant'] = aqi_results['controlling_pollutant'].str.replace('_individual_aqi', '')\n",
    "\n",
    "print(f\"\\nCalculated AQI range: {aqi_results['calculated_max_aqi'].min():.0f} - {aqi_results['calculated_max_aqi'].max():.0f}\")\n",
    "print(f\"Current US AQI range: {aqi_results['us_aqi'].min():.0f} - {aqi_results['us_aqi'].max():.0f}\")\n",
    "print(f\"Difference: {(aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']).describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 1: Pollutant Dominance Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"POLLUTANT DOMINANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count which pollutant controls AQI most often\n",
    "dominance_counts = aqi_results['controlling_pollutant'].value_counts()\n",
    "dominance_pct = (dominance_counts / len(aqi_results) * 100).round(1)\n",
    "\n",
    "print(\"Controlling Pollutant Frequency:\")\n",
    "for pollutant, count in dominance_counts.items():\n",
    "    pct = dominance_pct[pollutant]\n",
    "    print(f\"  {pollutant:<15}: {count:3d} times ({pct:5.1f}%)\")\n",
    "\n",
    "# Pie chart of dominance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(dominance_counts.values, labels=dominance_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Which Pollutant Controls AQI?')\n",
    "\n",
    "# Bar chart for clearer comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(range(len(dominance_counts)), dominance_counts.values, \n",
    "               color=['red' if x in ['pm2_5', 'pm10'] else 'lightcoral' for x in dominance_counts.index])\n",
    "plt.xticks(range(len(dominance_counts)), [p.replace('_', ' ').title() for p in dominance_counts.index], rotation=45)\n",
    "plt.ylabel('Number of Hours')\n",
    "plt.title('Controlling Pollutant Frequency')\n",
    "\n",
    "# Highlight PM2.5 and PM10\n",
    "for i, (pollutant, count) in enumerate(dominance_counts.items()):\n",
    "    color = 'white' if pollutant in ['pm2_5', 'pm10'] else 'black'\n",
    "    plt.text(i, count + 5, f'{count}', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of PM dominance\n",
    "pm_dominance = dominance_counts.get('pm2_5', 0) + dominance_counts.get('pm10', 0)\n",
    "pm_percentage = (pm_dominance / len(aqi_results) * 100)\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDING:\")\n",
    "print(f\"PM2.5 + PM10 control AQI {pm_dominance}/{len(aqi_results)} times ({pm_percentage:.1f}%)\")\n",
    "print(f\"Other pollutants control AQI {len(aqi_results) - pm_dominance}/{len(aqi_results)} times ({100-pm_percentage:.1f}%)\")\n",
    "\n",
    "if pm_percentage >= 80:\n",
    "    print(\"‚úÖ Current PM-only approach captures most AQI variations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Consider including other pollutants in modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MIGHT DELETE ########\n",
    "\n",
    "# VISUALIZATION 2: Concentration vs AQI Relationships\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCENTRATION vs AQI ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create scatter plots for EPA criteria pollutants only\n",
    "num_plots = len(available_pollutants)\n",
    "cols = 3\n",
    "rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get concentrations and individual AQI\n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        conc = df_calc['ozone_ppb']\n",
    "        unit = 'ppb'\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        conc = df_calc['co_ppm']\n",
    "        unit = 'ppm'\n",
    "    else:\n",
    "        conc = df_calc[pollutant]\n",
    "        unit = '¬µg/m¬≥'\n",
    "    \n",
    "    # All pollutants here are EPA criteria pollutants with AQI calculations\n",
    "    individual_aqi = aqi_results[f'{pollutant}_individual_aqi']\n",
    "    \n",
    "    # Scatter plot with AQI color coding\n",
    "    scatter = ax.scatter(conc, individual_aqi, alpha=0.6, s=20, \n",
    "                        c=individual_aqi, cmap='RdYlGn_r', vmin=0, vmax=150)\n",
    "    \n",
    "    ax.set_xlabel(f'{pollutant.replace(\"_\", \" \").title()} ({unit})')\n",
    "    ax.set_ylabel('Individual AQI')\n",
    "    ax.set_title(f'{pollutant.replace(\"_\", \" \").title()} ‚Üí AQI')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AQI level lines\n",
    "    ax.axhline(y=50, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=100, color='yellow', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=150, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(available_pollutants), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for EPA criteria pollutants\n",
    "print(\"\\nIndividual AQI Statistics (EPA Criteria Pollutants):\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    stats = aqi_results[aqi_col].describe()\n",
    "    print(f\"{pollutant:<15}: Mean={stats['mean']:5.1f}, Max={stats['max']:5.1f}, >100: {(aqi_results[aqi_col] > 100).sum():3d} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3A: Individual AQI Contributions vs Maximum AQI\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL POLLUTANT AQI CONTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time series showing all individual AQIs vs the maximum\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All individual AQIs over time\n",
    "time_vals = aqi_results['time']\n",
    "max_aqi_vals = aqi_results['calculated_max_aqi']\n",
    "\n",
    "# Plot each individual AQI\n",
    "colors = ['red', 'darkred', 'blue', 'purple', 'orange', 'green']\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        ax1.plot(time_vals, aqi_results[aqi_col], \n",
    "                label=pollutant.replace('_', ' ').title(), \n",
    "                alpha=0.7, linewidth=1.5, color=colors[i % len(colors)])\n",
    "\n",
    "# Plot maximum AQI as thick black line\n",
    "ax1.plot(time_vals, max_aqi_vals, \n",
    "         label='Maximum AQI (Envelope)', \n",
    "         color='black', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('Individual Pollutant AQIs vs Maximum AQI Over Time')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Distribution of individual AQIs \n",
    "aqi_data = []\n",
    "pollutant_names = []\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        aqi_data.append(aqi_results[aqi_col].values)\n",
    "        pollutant_names.append(pollutant.replace('_', ' ').title())\n",
    "\n",
    "# Add maximum AQI for comparison\n",
    "aqi_data.append(max_aqi_vals.values)\n",
    "pollutant_names.append('Maximum AQI')\n",
    "\n",
    "# Create box plot\n",
    "bp = ax2.boxplot(aqi_data, labels=pollutant_names, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "box_colors = colors + ['black']\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('AQI Value')\n",
    "ax2.set_title('AQI Distribution by Pollutant')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level lines\n",
    "for level, color in [(50, 'green'), (100, 'yellow'), (150, 'orange')]:\n",
    "    ax2.axhline(y=level, color=color, linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for contribution analysis\n",
    "print(\"\\nIndividual AQI vs Maximum AQI Analysis:\")\n",
    "max_aqi_mean = max_aqi_vals.mean()\n",
    "print(f\"Maximum AQI - Mean: {max_aqi_mean:.1f}, Range: {max_aqi_vals.min():.0f}-{max_aqi_vals.max():.0f}\")\n",
    "\n",
    "print(\"\\nHow often each pollutant reaches within 90% of maximum AQI:\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        close_to_max = (aqi_results[aqi_col] >= 0.9 * max_aqi_vals).sum()\n",
    "        percentage = (close_to_max / len(aqi_results) * 100)\n",
    "        print(f\"  {pollutant:<15}: {close_to_max:3d}/{len(aqi_results)} times ({percentage:5.1f}%) within 90% of max\")\n",
    "\n",
    "print(\"\\nThis shows which pollutants are the 'runners-up' when they don't control AQI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3: Time Series of Controlling Pollutants\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME SERIES OF AQI CONTROL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a time series showing which pollutant controls AQI\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: AQI comparison over time\n",
    "ax1.plot(aqi_results['time'], aqi_results['us_aqi'], label='Current US AQI (PM only)', alpha=0.7, linewidth=2)\n",
    "ax1.plot(aqi_results['time'], aqi_results['calculated_max_aqi'], label='True Max AQI (All pollutants)', alpha=0.7, linewidth=2)\n",
    "ax1.fill_between(aqi_results['time'], aqi_results['us_aqi'], aqi_results['calculated_max_aqi'], \n",
    "                 alpha=0.3, color='red', label='Missing AQI')\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('AQI Comparison: Current PM-only vs Full EPA Calculation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Controlling pollutant over time\n",
    "pollutant_colors = {\n",
    "    'pm2_5': 'red', 'pm10': 'darkred', 'ozone': 'blue', \n",
    "    'carbon_monoxide': 'purple', 'nitrogen_dioxide': 'orange', 'sulphur_dioxide': 'green'\n",
    "}\n",
    "\n",
    "# Create numerical encoding for pollutants for plotting\n",
    "unique_pollutants = aqi_results['controlling_pollutant'].unique()\n",
    "pollutant_mapping = {p: i for i, p in enumerate(unique_pollutants)}\n",
    "aqi_results['pollutant_num'] = aqi_results['controlling_pollutant'].map(pollutant_mapping)\n",
    "\n",
    "scatter = ax2.scatter(aqi_results['time'], aqi_results['pollutant_num'], \n",
    "                     c=[pollutant_colors.get(p, 'gray') for p in aqi_results['controlling_pollutant']], \n",
    "                     alpha=0.7, s=20)\n",
    "\n",
    "ax2.set_ylabel('Controlling Pollutant')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Which Pollutant Controls AQI Over Time')\n",
    "ax2.set_yticks(range(len(unique_pollutants)))\n",
    "ax2.set_yticklabels([p.replace('_', ' ').title() for p in unique_pollutants])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate impact of missing other pollutants\n",
    "aqi_difference = aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']\n",
    "significant_underestimation = (aqi_difference > 10).sum()\n",
    "\n",
    "print(f\"\\nüìä IMPACT ANALYSIS:\")\n",
    "print(f\"Times when PM-only AQI underestimates by >10 points: {significant_underestimation}/{len(aqi_results)} ({significant_underestimation/len(aqi_results)*100:.1f}%)\")\n",
    "print(f\"Maximum underestimation: {aqi_difference.max():.1f} AQI points\")\n",
    "print(f\"Average difference: {aqi_difference.mean():.1f} AQI points\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nüéØ MODELING RECOMMENDATION:\")\n",
    "if pm_percentage >= 85:\n",
    "    print(\"‚úÖ PM-only approach is SUFFICIENT for Multan AQI prediction\")\n",
    "    print(\"   PM2.5 + PM10 control >85% of AQI variations\")\n",
    "elif pm_percentage >= 70:\n",
    "    print(\"‚ö†Ô∏è  PM-only approach is MOSTLY adequate but consider monitoring other pollutants\")\n",
    "    print(\"   PM2.5 + PM10 control 70-85% of AQI variations\")\n",
    "else:\n",
    "    print(\"üö® PM-only approach MISSES significant AQI drivers\")\n",
    "    print(\"   Consider including other pollutants in prediction model\")\n",
    "\n",
    "print(f\"\\nCurrent focus on PM2.5 and PM10 captures {pm_percentage:.1f}% of AQI control instances.\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL ANALYSIS:\")\n",
    "print(f\"‚úì Analyzed all {len(available_pollutants)} EPA criteria pollutants in dataset\")\n",
    "print(f\"‚úì PM2.5 and PM10 are responsible for {pm_percentage:.1f}% of AQI determinations\")\n",
    "print(f\"‚úì Other criteria pollutants (O3, CO, NO2, SO2) control {100-pm_percentage:.1f}% of AQI\")\n",
    "print(f\"‚úì This validates the scope of your PM-focused modeling approach\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lag Features Analysis\n",
    "\n",
    "Examining the importance of historical PM values for predicting current concentrations (which leads to better AQI calculations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features for BOTH ML targets (PM2.5 AND PM10)\n",
    "print(\"=\"*60)\n",
    "print(\"LAG FEATURES ANALYSIS FOR ML TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM10 Lag Analysis (since it's also a target)\n",
    "pm10_lag_features = [col for col in df.columns if 'lag' in col and 'pm10' in col]\n",
    "if pm10_lag_features:\n",
    "    print(f\"\\nFound {len(pm10_lag_features)} PM10 lag features:\")\n",
    "    print(pm10_lag_features[:5], \"...\" if len(pm10_lag_features) > 5 else \"\")\n",
    "    \n",
    "    # Calculate correlations between current PM10 and its lag features\n",
    "    pm10_lag_correlations = df[['pm10'] + pm10_lag_features].corr()['pm10'].drop('pm10')\n",
    "    \n",
    "    # Plot PM10 lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "    pm10_correlations = [pm10_lag_correlations[f'pm10_lag_{h}h'] for h in lag_hours if f'pm10_lag_{h}h' in pm10_lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(pm10_correlations)), pm10_correlations, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM10')\n",
    "    plt.title('PM10 Lag Features Correlation (ML Target)')\n",
    "    plt.xticks(range(len(pm10_correlations)), [f'{h}h' for h in lag_hours[:len(pm10_correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(pm10_correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPM10 lag feature correlations (ML Target):\")\n",
    "    for i, h in enumerate(lag_hours[:len(pm10_correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {pm10_correlations[i]:.3f}\")\n",
    "\n",
    "print(\"\\n[ML Targets: PM2.5 & PM10 concentrations]\")\n",
    "print(\"[Ultimate Goal: Accurate AQI predictions for Multan]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features correlation with current PM2.5\n",
    "lag_features = [col for col in df.columns if 'lag' in col and 'pm2_5' in col]\n",
    "print(f\"Found {len(lag_features)} PM2.5 lag features:\")\n",
    "print(lag_features)\n",
    "\n",
    "if lag_features:\n",
    "    # Calculate correlations between current PM2.5 and its lag features\n",
    "    lag_correlations = df[['pm2_5'] + lag_features].corr()['pm2_5'].drop('pm2_5')\n",
    "    \n",
    "    # Plot lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]  # Expected lag hours\n",
    "    correlations = [lag_correlations[f'pm2_5_lag_{h}h'] for h in lag_hours if f'pm2_5_lag_{h}h' in lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(correlations)), correlations, alpha=0.7)\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM2.5')\n",
    "    plt.title('PM2.5 Lag Features Correlation')\n",
    "    plt.xticks(range(len(correlations)), [f'{h}h' for h in lag_hours[:len(correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLag feature correlations with current PM2.5:\")\n",
    "    for i, h in enumerate(lag_hours[:len(correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {correlations[i]:.3f}\")\n",
    "\n",
    "# Complete Rolling Statistics Analysis for Both ML Targets\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ROLLING STATISTICS ANALYSIS (ML TARGETS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM2.5 Rolling Features Analysis\n",
    "pm25_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm2_5' in col]\n",
    "if pm25_rolling_features:\n",
    "    print(f\"\\nPM2.5 ROLLING FEATURES ({len(pm25_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    rolling_types = ['mean', 'std', 'min', 'max']\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm25_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm2_5'] + stat_features].corr()['pm2_5'].drop('pm2_5')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# PM10 Rolling Features Analysis  \n",
    "pm10_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm10' in col]\n",
    "if pm10_rolling_features:\n",
    "    print(f\"\\nPM10 ROLLING FEATURES ({len(pm10_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm10_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm10'] + stat_features].corr()['pm10'].drop('pm10')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# Change Rate Features Analysis\n",
    "print(f\"\\nCHANGE RATE FEATURES:\")\n",
    "change_features = [col for col in df.columns if 'change_rate' in col]\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    target_change_features = [col for col in change_features if target in col]\n",
    "    if target_change_features:\n",
    "        print(f\"\\n  {target.upper()} change rates:\")\n",
    "        correlations = df[[target] + target_change_features].corr()[target].drop(target)\n",
    "        for feature, corr in correlations.items():\n",
    "            period = feature.split('_')[-1]\n",
    "            print(f\"    {period:<4} change: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Rolling features capture trend information over different time windows\")\n",
    "print(f\"‚Ä¢ Shorter windows (3h, 6h) typically correlate more strongly with current values\")\n",
    "print(f\"‚Ä¢ Change rates show how much PM concentrations are shifting\")\n",
    "print(f\"‚Ä¢ These features help models understand pollution persistence and trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Analysis\n",
    "\n",
    "**Focus**: Ensuring data reliability for accurate PM concentration predictions and AQI calculations.\n",
    "\n",
    "### 6.1 Missing Values & Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Missing Values & Null Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES & NULL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic missing value check\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent\n",
    "}).round(2)\n",
    "\n",
    "print(\"EXPLICIT NULL VALUES:\")\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"‚úì No explicit null values found!\")\n",
    "else:\n",
    "    print(\"Missing values found:\")\n",
    "    for col in missing_df[missing_df['Missing_Count'] > 0].index:\n",
    "        count = missing_df.loc[col, 'Missing_Count']\n",
    "        percent = missing_df.loc[col, 'Missing_Percentage']\n",
    "        print(f\"  {col:<25} {count:>6} ({percent:>6.2f}%)\")\n",
    "\n",
    "# Check for zero values that might represent missing data\n",
    "print(f\"\\nZERO VALUES ANALYSIS (Potential Missing Data):\")\n",
    "air_quality_cols = ['pm2_5', 'pm10', 'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "zero_issues = {}\n",
    "\n",
    "for col in air_quality_cols:\n",
    "    if col in df.columns:\n",
    "        zero_count = (df[col] == 0.0).sum()\n",
    "        zero_percent = (zero_count / len(df)) * 100\n",
    "        if zero_count > 0:\n",
    "            zero_issues[col] = {'count': zero_count, 'percent': zero_percent}\n",
    "\n",
    "if zero_issues:\n",
    "    print(\"‚ö†Ô∏è  ZERO VALUES FOUND (may indicate missing sensors):\")\n",
    "    for col, stats in zero_issues.items():\n",
    "        print(f\"  {col:<20}: {stats['count']:>3d} zeros ({stats['percent']:>5.1f}%)\")\n",
    "        \n",
    "    # Visualize zero patterns over time\n",
    "    fig, axes = plt.subplots(len(zero_issues), 1, figsize=(15, 3*len(zero_issues)))\n",
    "    if len(zero_issues) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (col, stats) in enumerate(zero_issues.items()):\n",
    "        zero_mask = df[col] == 0.0\n",
    "        axes[i].scatter(df[zero_mask]['time'], [col]*zero_mask.sum(), \n",
    "                       alpha=0.7, color='red', s=20, label=f'Zero values ({stats[\"count\"]})')\n",
    "        axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} - Zero Value Timeline')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úì No zero values in air quality parameters\")\n",
    "\n",
    "# Check for consecutive missing periods (data gaps)\n",
    "print(f\"\\nTIME GAP ANALYSIS:\")\n",
    "df_sorted = df.sort_values('time').reset_index(drop=True)\n",
    "time_diffs = df_sorted['time'].diff()\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]\n",
    "\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(large_gaps)} time gaps > 2 hours:\")\n",
    "    for idx, gap in large_gaps.items():\n",
    "        gap_start = df_sorted.loc[idx-1, 'time'] if idx > 0 else 'Start'\n",
    "        gap_end = df_sorted.loc[idx, 'time']\n",
    "        print(f\"  Gap: {gap} between {gap_start} and {gap_end}\")\n",
    "else:\n",
    "    print(\"‚úì No significant time gaps found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Consistency & Physics Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Data Consistency & Physics Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CONSISTENCY & PHYSICS VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PM2.5 vs PM10 relationship (PM2.5 should generally be ‚â§ PM10)\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = df[df['pm2_5'] > df['pm10']]\n",
    "    violation_count = len(pm_violations)\n",
    "    violation_percent = (violation_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"PM2.5 vs PM10 CONSISTENCY:\")\n",
    "    if violation_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  {violation_count} records ({violation_percent:.1f}%) where PM2.5 > PM10\")\n",
    "        print(f\"   Max violation: PM2.5={pm_violations['pm2_5'].max():.1f} > PM10={pm_violations['pm10'].max():.1f}\")\n",
    "        \n",
    "        # Show violation timeline\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(df['time'], df['pm2_5'], alpha=0.5, label='PM2.5', s=10)\n",
    "        plt.scatter(df['time'], df['pm10'], alpha=0.5, label='PM10', s=10)\n",
    "        plt.scatter(pm_violations['time'], pm_violations['pm2_5'], \n",
    "                   color='red', label=f'PM2.5 > PM10 ({violation_count} cases)', s=30, marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (¬µg/m¬≥)')\n",
    "        plt.title('PM2.5 vs PM10 Consistency Check')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚úì PM2.5 ‚â§ PM10 in all records (physically consistent)\")\n",
    "\n",
    "# Check reasonable ranges for weather parameters\n",
    "print(f\"\\nWEATHER PARAMETER VALIDATION:\")\n",
    "weather_ranges = {\n",
    "    'temperature': (-50, 60, '¬∞C'),  # Extreme but possible range\n",
    "    'humidity': (0, 100, '%'),       # Should be 0-100%\n",
    "    'pressure': (800, 1200, 'hPa'),  # Reasonable atmospheric pressure\n",
    "    'wind_speed': (0, 50, 'm/s'),    # Reasonable wind speeds\n",
    "    'wind_direction': (0, 360, '¬∞')  # Should be 0-360 degrees\n",
    "}\n",
    "\n",
    "weather_violations = {}\n",
    "for param, (min_val, max_val, unit) in weather_ranges.items():\n",
    "    if param in df.columns:\n",
    "        below_min = (df[param] < min_val).sum()\n",
    "        above_max = (df[param] > max_val).sum()\n",
    "        violations = below_min + above_max\n",
    "        \n",
    "        if violations > 0:\n",
    "            weather_violations[param] = {\n",
    "                'below_min': below_min, 'above_max': above_max, \n",
    "                'min_val': min_val, 'max_val': max_val, 'unit': unit,\n",
    "                'actual_min': df[param].min(), 'actual_max': df[param].max()\n",
    "            }\n",
    "\n",
    "if weather_violations:\n",
    "    print(\"‚ö†Ô∏è  WEATHER PARAMETER VIOLATIONS:\")\n",
    "    for param, stats in weather_violations.items():\n",
    "        print(f\"  {param:<15}: {stats['below_min']} below {stats['min_val']}{stats['unit']}, \"\n",
    "              f\"{stats['above_max']} above {stats['max_val']}{stats['unit']}\")\n",
    "        print(f\"                   Actual range: {stats['actual_min']:.1f} - {stats['actual_max']:.1f}{stats['unit']}\")\n",
    "else:\n",
    "    print(\"‚úì All weather parameters within reasonable ranges\")\n",
    "\n",
    "# Check cyclical feature ranges (sin/cos should be in [-1, 1])\n",
    "print(f\"\\nCYCLICAL FEATURE VALIDATION:\")\n",
    "cyclical_cols = [col for col in df.columns if '_sin' in col or '_cos' in col]\n",
    "cyclical_violations = {}\n",
    "\n",
    "for col in cyclical_cols:\n",
    "    below_neg1 = (df[col] < -1.01).sum()  # Small tolerance for floating point\n",
    "    above_pos1 = (df[col] > 1.01).sum()\n",
    "    violations = below_neg1 + above_pos1\n",
    "    \n",
    "    if violations > 0:\n",
    "        cyclical_violations[col] = {\n",
    "            'below_neg1': below_neg1, 'above_pos1': above_pos1,\n",
    "            'actual_min': df[col].min(), 'actual_max': df[col].max()\n",
    "        }\n",
    "\n",
    "if cyclical_violations:\n",
    "    print(\"‚ö†Ô∏è  CYCLICAL FEATURE VIOLATIONS:\")\n",
    "    for col, stats in cyclical_violations.items():\n",
    "        print(f\"  {col:<20}: {stats['below_neg1']} below -1, {stats['above_pos1']} above 1\")\n",
    "        print(f\"                       Actual range: {stats['actual_min']:.6f} - {stats['actual_max']:.6f}\")\n",
    "else:\n",
    "    print(\"‚úì All cyclical features within [-1, 1] range\")\n",
    "\n",
    "# Check binary flag consistency\n",
    "print(f\"\\nBINARY FLAG VALIDATION:\")\n",
    "binary_cols = [col for col in df.columns if col.startswith('is_')]\n",
    "binary_violations = {}\n",
    "\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    expected_vals = {0.0, 1.0}\n",
    "    unexpected_vals = set(unique_vals) - expected_vals\n",
    "    \n",
    "    if unexpected_vals:\n",
    "        binary_violations[col] = {\n",
    "            'unexpected': list(unexpected_vals),\n",
    "            'unique_vals': list(unique_vals)\n",
    "        }\n",
    "\n",
    "if binary_violations:\n",
    "    print(\"‚ö†Ô∏è  BINARY FLAG VIOLATIONS:\")\n",
    "    for col, stats in binary_violations.items():\n",
    "        print(f\"  {col:<20}: Found {stats['unexpected']} (expected only 0.0, 1.0)\")\n",
    "        print(f\"                       All values: {stats['unique_vals']}\")\n",
    "else:\n",
    "    print(\"‚úì All binary flags contain only 0.0 and 1.0 values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Statistical Outlier Detection\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and key environmental features\n",
    "key_features_for_outliers = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for model quality\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Air quality predictors\n",
    "]\n",
    "\n",
    "available_outlier_features = [col for col in key_features_for_outliers if col in df.columns]\n",
    "\n",
    "# Z-score method (|z| > 3 considered outlier)\n",
    "print(\"Z-SCORE OUTLIER DETECTION (|z-score| > 3):\")\n",
    "zscore_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask = z_scores > 3\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        zscore_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'max_zscore': z_scores.max(),\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if zscore_outliers:\n",
    "    print(\"‚ö†Ô∏è  Z-SCORE OUTLIERS FOUND:\")\n",
    "    for col, stats in zscore_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%) - Max |z|: {stats['max_zscore']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            print(f\"                       Values: {sorted(stats['outlier_values'])[:5]}...\" if len(stats['outlier_values']) > 5 else f\"                       Values: {sorted(stats['outlier_values'])}\")\n",
    "else:\n",
    "    print(\"‚úì No z-score outliers found (|z| > 3)\")\n",
    "\n",
    "# IQR method (values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
    "print(f\"\\nIQR OUTLIER DETECTION (beyond Q1-1.5*IQR, Q3+1.5*IQR):\")\n",
    "iqr_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        iqr_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if iqr_outliers:\n",
    "    print(\"‚ö†Ô∏è  IQR OUTLIERS FOUND:\")\n",
    "    for col, stats in iqr_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%)\")\n",
    "        print(f\"                       Expected range: {stats['lower_bound']:.2f} - {stats['upper_bound']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            extreme_values = sorted(stats['outlier_values'])\n",
    "            display_values = extreme_values[:3] + ['...'] + extreme_values[-2:] if len(extreme_values) > 5 else extreme_values\n",
    "            print(f\"                       Outlier values: {display_values}\")\n",
    "else:\n",
    "    print(\"‚úì No IQR outliers found\")\n",
    "\n",
    "# Visualize outliers for ML targets\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_present = [col for col in ml_targets if col in df.columns]\n",
    "\n",
    "if ml_targets_present and (zscore_outliers or iqr_outliers):\n",
    "    print(f\"\\nOUTLIER VISUALIZATION FOR ML TARGETS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_present), 2, figsize=(15, 5*len(ml_targets_present)))\n",
    "    if len(ml_targets_present) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_present):\n",
    "        # Box plot\n",
    "        ax1 = axes[i, 0]\n",
    "        bp = ax1.boxplot([df[target].dropna()], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax1.set_title(f'{target.upper()} - Box Plot (IQR Outliers)')\n",
    "        ax1.set_ylabel(f'{target.replace(\"_\", \".\")} (¬µg/m¬≥)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series with outliers highlighted\n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(df['time'], df[target], alpha=0.7, linewidth=1, label=target.upper())\n",
    "        \n",
    "        # Highlight outliers\n",
    "        if target in zscore_outliers:\n",
    "            z_scores = np.abs((df[target] - df[target].mean()) / df[target].std())\n",
    "            zscore_mask = z_scores > 3\n",
    "            ax2.scatter(df[zscore_mask]['time'], df.loc[zscore_mask, target], \n",
    "                       color='red', s=30, marker='x', label=f'Z-score outliers ({zscore_outliers[target][\"count\"]})')\n",
    "        \n",
    "        if target in iqr_outliers:\n",
    "            Q1 = df[target].quantile(0.25)\n",
    "            Q3 = df[target].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_mask = (df[target] < lower_bound) | (df[target] > upper_bound)\n",
    "            ax2.scatter(df[iqr_mask]['time'], df.loc[iqr_mask, target], \n",
    "                       color='orange', s=20, marker='o', alpha=0.7, label=f'IQR outliers ({iqr_outliers[target][\"count\"]})')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Time Series with Outliers')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(f'{target.replace(\"_\", \".\")} (¬µg/m¬≥)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary of outlier impact on ML targets\n",
    "print(f\"\\nüéØ OUTLIER IMPACT ON ML TARGETS:\")\n",
    "for target in ml_targets_present:\n",
    "    total_outliers = 0\n",
    "    if target in zscore_outliers:\n",
    "        total_outliers += zscore_outliers[target]['count']\n",
    "    if target in iqr_outliers:\n",
    "        total_outliers += iqr_outliers[target]['count']  # Note: may overlap with z-score\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        outlier_percent = (total_outliers / len(df)) * 100\n",
    "        print(f\"  {target.upper()}: ~{total_outliers} potential outliers ({outlier_percent:.1f}% of data)\")\n",
    "        print(f\"           Consider: Review for sensor errors vs genuine extreme pollution events\")\n",
    "    else:\n",
    "        print(f\"  {target.upper()}: No significant outliers detected\")\n",
    "\n",
    "print(f\"\\n[Quality Note: Outliers in PM concentrations could be genuine pollution spikes or sensor malfunctions]\")\n",
    "print(f\"[Recommendation: Investigate extreme values before removal - they might be real air quality events]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Data Completeness & Summary Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Data Completeness & Summary Quality Report\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA COMPLETENESS & SUMMARY QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"DUPLICATE RECORDS: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"‚ö†Ô∏è  Duplicate rows found - consider deduplication\")\n",
    "else:\n",
    "    print(\"‚úì No duplicate rows\")\n",
    "\n",
    "# Unique values analysis\n",
    "print(f\"\\nUNIQUE VALUES ANALYSIS:\")\n",
    "feature_categories = {\n",
    "    'ML Targets': ['pm2_5', 'pm10'],\n",
    "    'Air Quality': ['carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide', 'nh3'],\n",
    "    'AQI Calculations': ['pm2_5_aqi', 'pm10_aqi', 'us_aqi', 'openweather_aqi'],\n",
    "    'Weather': ['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction'],\n",
    "    'Time Features': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'],\n",
    "    'Binary Flags': [col for col in df.columns if col.startswith('is_')],\n",
    "    'Lag Features': [col for col in df.columns if 'lag' in col][:5],  # Show first 5\n",
    "    'Rolling Features': [col for col in df.columns if 'rolling' in col][:5]  # Show first 5\n",
    "}\n",
    "\n",
    "for category, cols in feature_categories.items():\n",
    "    available_cols = [col for col in cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for col in available_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            unique_ratio = unique_count / len(df)\n",
    "            if unique_ratio < 0.01:  # Very low uniqueness\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ‚ö†Ô∏è  Low variation\")\n",
    "            elif unique_ratio > 0.95:  # Very high uniqueness  \n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ‚úì High variation\")\n",
    "            else:\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%})\")\n",
    "\n",
    "# Data quality scoring\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL DATA QUALITY SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_score = 100  # Start with perfect score\n",
    "quality_issues = []\n",
    "\n",
    "# Deduct points for various issues\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent_total = (missing_data.sum() / (len(df) * len(df.columns))) * 100\n",
    "    deduction = min(20, missing_percent_total * 4)  # Up to 20 points for missing data\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Missing data: -{deduction:.1f} points ({missing_percent_total:.1f}% of all values)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    dup_percent = (duplicates / len(df)) * 100\n",
    "    deduction = min(10, dup_percent * 2)  # Up to 10 points for duplicates\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Duplicate rows: -{deduction:.1f} points ({dup_percent:.1f}% of records)\")\n",
    "\n",
    "if weather_violations:\n",
    "    deduction = len(weather_violations) * 2  # 2 points per violated weather parameter\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Weather violations: -{deduction} points ({len(weather_violations)} parameters)\")\n",
    "\n",
    "if cyclical_violations:\n",
    "    deduction = len(cyclical_violations) * 3  # 3 points per violated cyclical feature\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Cyclical violations: -{deduction} points ({len(cyclical_violations)} features)\")\n",
    "\n",
    "if binary_violations:\n",
    "    deduction = len(binary_violations) * 2  # 2 points per violated binary flag\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Binary flag violations: -{deduction} points ({len(binary_violations)} flags)\")\n",
    "\n",
    "# Check PM2.5 vs PM10 physics violations\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = (df['pm2_5'] > df['pm10']).sum()\n",
    "    if pm_violations > 0:\n",
    "        violation_percent = (pm_violations / len(df)) * 100\n",
    "        deduction = min(15, violation_percent * 3)  # Up to 15 points for physics violations\n",
    "        quality_score -= deduction\n",
    "        quality_issues.append(f\"PM physics violations: -{deduction:.1f} points ({violation_percent:.1f}% of records)\")\n",
    "\n",
    "# Outlier penalty (mild)\n",
    "total_outlier_features = len(zscore_outliers) + len(iqr_outliers)\n",
    "if total_outlier_features > 0:\n",
    "    deduction = min(5, total_outlier_features * 0.5)  # Mild penalty for outliers\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Statistical outliers: -{deduction:.1f} points (in {total_outlier_features} features)\")\n",
    "\n",
    "# Ensure score doesn't go below 0\n",
    "quality_score = max(0, quality_score)\n",
    "\n",
    "# Display results\n",
    "print(f\"üìä DATA QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    status = \"üü¢ EXCELLENT\"\n",
    "    recommendation = \"Data is ready for high-quality ML modeling\"\n",
    "elif quality_score >= 75:\n",
    "    status = \"üü° GOOD\" \n",
    "    recommendation = \"Data is suitable for ML with minor preprocessing\"\n",
    "elif quality_score >= 60:\n",
    "    status = \"üü† ACCEPTABLE\"\n",
    "    recommendation = \"Address major issues before ML modeling\"\n",
    "else:\n",
    "    status = \"üî¥ POOR\"\n",
    "    recommendation = \"Significant data cleaning required\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"\\nISSUES IDENTIFIED:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ NO SIGNIFICANT QUALITY ISSUES DETECTED\")\n",
    "\n",
    "print(f\"\\nüéØ ML READINESS ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_readiness = True\n",
    "\n",
    "for target in ml_targets:\n",
    "    if target in df.columns:\n",
    "        target_quality = 100\n",
    "        target_issues = []\n",
    "        \n",
    "        # Check for issues specific to ML targets\n",
    "        if target in zscore_outliers:\n",
    "            outlier_percent = zscore_outliers[target]['percent']\n",
    "            if outlier_percent > 5:\n",
    "                target_quality -= 10\n",
    "                target_issues.append(f\"High outlier rate ({outlier_percent:.1f}%)\")\n",
    "        \n",
    "        zero_count = (df[target] == 0.0).sum()\n",
    "        if zero_count > len(df) * 0.1:  # More than 10% zeros\n",
    "            target_quality -= 15\n",
    "            target_issues.append(f\"Many zero values ({zero_count} records)\")\n",
    "        \n",
    "        if target_issues:\n",
    "            print(f\"  {target.upper()}: {target_quality}/100 - {', '.join(target_issues)}\")\n",
    "            if target_quality < 70:\n",
    "                ml_readiness = False\n",
    "        else:\n",
    "            print(f\"  {target.upper()}: ‚úÖ Ready for ML modeling\")\n",
    "\n",
    "if ml_readiness:\n",
    "    print(f\"\\n‚úÖ DATASET IS READY FOR ML MODEL TRAINING\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  CONSIDER ADDITIONAL PREPROCESSING FOR OPTIMAL ML PERFORMANCE\")\n",
    "\n",
    "print(f\"\\n[Next Steps: Feature engineering validation, train/test splitting, model selection]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution Analysis\n",
    "\n",
    "**Focus**: Understanding feature distributions, skewness, and normality for optimal model preprocessing and outlier interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Skewness & Kurtosis Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Skewness & Kurtosis Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SKEWNESS & KURTOSIS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on key features for modeling\n",
    "key_features_for_distribution = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for preprocessing decisions\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide',  # Air quality predictors\n",
    "    'us_aqi', 'pm2_5_aqi', 'pm10_aqi'  # AQI values\n",
    "]\n",
    "\n",
    "available_dist_features = [col for col in key_features_for_distribution if col in df.columns]\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "distribution_stats = {}\n",
    "print(\"FEATURE DISTRIBUTION STATISTICS:\")\n",
    "print(f\"{'Feature':<20} {'Skewness':<10} {'Kurtosis':<10} {'Interpretation'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in available_dist_features:\n",
    "    if df[feature].notna().sum() > 0:  # Only if we have data\n",
    "        skewness = stats.skew(df[feature].dropna())\n",
    "        kurtosis = stats.kurtosis(df[feature].dropna())\n",
    "        \n",
    "        distribution_stats[feature] = {\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis\n",
    "        }\n",
    "        \n",
    "        # Interpret skewness\n",
    "        if abs(skewness) < 0.5:\n",
    "            skew_interp = \"Normal\"\n",
    "        elif abs(skewness) < 1.0:\n",
    "            skew_interp = \"Moderate\" + (\" Right\" if skewness > 0 else \" Left\")\n",
    "        else:\n",
    "            skew_interp = \"High\" + (\" Right\" if skewness > 0 else \" Left\")\n",
    "        \n",
    "        # Interpret kurtosis\n",
    "        if abs(kurtosis) < 1.0:\n",
    "            kurt_interp = \"Normal\"\n",
    "        elif kurtosis > 1.0:\n",
    "            kurt_interp = \"Heavy-tailed\"\n",
    "        else:\n",
    "            kurt_interp = \"Light-tailed\"\n",
    "            \n",
    "        interpretation = f\"{skew_interp}, {kurt_interp}\"\n",
    "        \n",
    "        print(f\"{feature:<20} {skewness:>8.3f} {kurtosis:>9.3f}  {interpretation}\")\n",
    "\n",
    "# Focus on ML targets\n",
    "print(f\"\\nüéØ ML TARGETS DISTRIBUTION ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "for target in ml_targets:\n",
    "    if target in distribution_stats:\n",
    "        skew = distribution_stats[target]['skewness']\n",
    "        kurt = distribution_stats[target]['kurtosis']\n",
    "        \n",
    "        print(f\"\\n{target.upper()}:\")\n",
    "        print(f\"  Skewness: {skew:.3f}\", end=\"\")\n",
    "        if skew > 1.0:\n",
    "            print(\" ‚Üí Consider log transformation\")\n",
    "        elif skew > 0.5:\n",
    "            print(\" ‚Üí Moderate right skew (common for pollution data)\")\n",
    "        else:\n",
    "            print(\" ‚Üí Good distribution\")\n",
    "            \n",
    "        print(f\"  Kurtosis: {kurt:.3f}\", end=\"\")\n",
    "        if kurt > 3.0:\n",
    "            print(\" ‚Üí Heavy tails (extreme values present)\")\n",
    "        elif kurt < -1.0:\n",
    "            print(\" ‚Üí Light tails (few extreme values)\")\n",
    "        else:\n",
    "            print(\" ‚Üí Normal tail behavior\")\n",
    "\n",
    "print(f\"\\n[Note: Pollution data typically shows right skewness due to occasional high pollution episodes]\")\n",
    "print(f\"[Recommendation: Consider log(x+1) transformation for highly skewed features (skew > 1.0)]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Distribution Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Distribution Visualizations\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUTION VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and most important features\n",
    "viz_features = ['pm2_5', 'pm10', 'temperature', 'humidity', 'ozone', 'carbon_monoxide']\n",
    "available_viz_features = [col for col in viz_features if col in df.columns]\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(len(available_viz_features), 3, figsize=(18, 5*len(available_viz_features)))\n",
    "if len(available_viz_features) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, feature in enumerate(available_viz_features):\n",
    "    data = df[feature].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    ax1 = axes[i, 0]\n",
    "    ax1.hist(data, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title(f'{feature.replace(\"_\", \".\")} Distribution')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add normal curve for comparison\n",
    "    mu, sigma = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    normal_curve = stats.norm.pdf(x, mu, sigma)\n",
    "    ax1.plot(x, normal_curve, 'r-', linewidth=2, label='Normal fit')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Box plot\n",
    "    ax2 = axes[i, 1]\n",
    "    bp = ax2.boxplot([data], patch_artist=True, labels=[feature.replace('_', '.')])\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    ax2.set_title(f'{feature.replace(\"_\", \".\")} Box Plot')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot for normality assessment\n",
    "    ax3 = axes[i, 2]\n",
    "    stats.probplot(data, dist=\"norm\", plot=ax3)\n",
    "    ax3.set_title(f'{feature.replace(\"_\", \".\")} Q-Q Plot')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add distribution stats as text\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    ax1.text(0.02, 0.98, f'Skew: {skew:.2f}\\\\nKurt: {kurt:.2f}', \n",
    "             transform=ax1.transAxes, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nüìä DISTRIBUTION INTERPRETATION GUIDE:\")\n",
    "print(f\"‚Ä¢ Histogram: Shows data distribution shape\")\n",
    "print(f\"‚Ä¢ Box Plot: Shows quartiles, median, and outliers\") \n",
    "print(f\"‚Ä¢ Q-Q Plot: Points on diagonal = normal distribution\")\n",
    "print(f\"‚Ä¢ Red curve: Normal distribution with same mean/std\")\n",
    "print(f\"\\\\nüéØ FOR ML PREPROCESSING:\")\n",
    "print(f\"‚Ä¢ Right-skewed features (skew > 1.0): Consider log transformation\")\n",
    "print(f\"‚Ä¢ Heavy-tailed features (kurtosis > 3.0): Consider robust scaling\")\n",
    "print(f\"‚Ä¢ Normal distributions: Standard scaling works well\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Normality Testing & Log Transformation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Normality Testing & Log Transformation Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALITY TESTING & LOG TRANSFORMATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets for detailed analysis\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_available = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(\"NORMALITY TESTS FOR ML TARGETS:\")\n",
    "print(f\"{'Feature':<15} {'Shapiro p-val':<15} {'JB p-val':<12} {'Normal?':<10} {'Log Transform?'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "normality_results = {}\n",
    "\n",
    "for target in ml_targets_available:\n",
    "    data = df[target].dropna()\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(data) < 3:\n",
    "        continue\n",
    "        \n",
    "    # Shapiro-Wilk test (good for smaller samples, max ~5000)\n",
    "    if len(data) <= 5000:\n",
    "        shapiro_stat, shapiro_p = shapiro(data)\n",
    "    else:\n",
    "        # Use subset for Shapiro if too large\n",
    "        sample_data = data.sample(5000, random_state=42)\n",
    "        shapiro_stat, shapiro_p = shapiro(sample_data)\n",
    "    \n",
    "    # Jarque-Bera test (good for larger samples)\n",
    "    jb_stat, jb_p = jarque_bera(data)\n",
    "    \n",
    "    # Test with log transformation\n",
    "    data_positive = data[data > 0]  # Log requires positive values\n",
    "    if len(data_positive) > 0:\n",
    "        log_data = np.log1p(data_positive)  # log(1+x) to handle zeros\n",
    "        if len(log_data) <= 5000:\n",
    "            log_shapiro_stat, log_shapiro_p = shapiro(log_data)\n",
    "        else:\n",
    "            log_sample = log_data.sample(5000, random_state=42)\n",
    "            log_shapiro_stat, log_shapiro_p = shapiro(log_sample)\n",
    "        log_jb_stat, log_jb_p = jarque_bera(log_data)\n",
    "    else:\n",
    "        log_shapiro_p, log_jb_p = 0, 0\n",
    "    \n",
    "    # Determine normality (p > 0.05 suggests normal)\n",
    "    is_normal = shapiro_p > 0.05 and jb_p > 0.05\n",
    "    log_is_normal = log_shapiro_p > 0.05 and log_jb_p > 0.05\n",
    "    \n",
    "    # Recommendation\n",
    "    if is_normal:\n",
    "        recommendation = \"No\"\n",
    "    elif log_is_normal:\n",
    "        recommendation = \"Yes - Better\"\n",
    "    elif log_shapiro_p > shapiro_p or log_jb_p > jb_p:\n",
    "        recommendation = \"Yes - Improved\"\n",
    "    else:\n",
    "        recommendation = \"Maybe\"\n",
    "    \n",
    "    normality_results[target] = {\n",
    "        'original': {'shapiro_p': shapiro_p, 'jb_p': jb_p, 'normal': is_normal},\n",
    "        'log': {'shapiro_p': log_shapiro_p, 'jb_p': log_jb_p, 'normal': log_is_normal},\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "    \n",
    "    normal_status = \"Yes\" if is_normal else \"No\"\n",
    "    print(f\"{target:<15} {shapiro_p:<15.4f} {jb_p:<12.4f} {normal_status:<10} {recommendation}\")\n",
    "\n",
    "print(f\"\\\\nLOG TRANSFORMATION COMPARISON:\")\n",
    "print(f\"{'Feature':<15} {'Original Skew':<15} {'Log Skew':<12} {'Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "transformation_results = {}\n",
    "\n",
    "for target in ml_targets_available:\n",
    "    data = df[target].dropna()\n",
    "    if len(data) == 0:\n",
    "        continue\n",
    "        \n",
    "    original_skew = stats.skew(data)\n",
    "    \n",
    "    # Log transformation\n",
    "    data_positive = data[data > 0]\n",
    "    if len(data_positive) > 0:\n",
    "        log_data = np.log1p(data_positive)\n",
    "        log_skew = stats.skew(log_data)\n",
    "        improvement = abs(original_skew) - abs(log_skew)\n",
    "        improvement_text = \"Better\" if improvement > 0.1 else (\"Slight\" if improvement > 0 else \"Worse\")\n",
    "    else:\n",
    "        log_skew = original_skew\n",
    "        improvement_text = \"N/A\"\n",
    "    \n",
    "    transformation_results[target] = {\n",
    "        'original_skew': original_skew,\n",
    "        'log_skew': log_skew,\n",
    "        'improvement': improvement_text\n",
    "    }\n",
    "    \n",
    "    print(f\"{target:<15} {original_skew:<15.3f} {log_skew:<12.3f} {improvement_text}\")\n",
    "\n",
    "# Visualization of original vs log-transformed distributions\n",
    "if ml_targets_available:\n",
    "    print(f\"\\\\nVISUALIZING TRANSFORMATION EFFECTS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_available), 2, figsize=(15, 5*len(ml_targets_available)))\n",
    "    if len(ml_targets_available) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_available):\n",
    "        data = df[target].dropna()\n",
    "        data_positive = data[data > 0]\n",
    "        \n",
    "        # Original distribution\n",
    "        ax1 = axes[i, 0]\n",
    "        ax1.hist(data, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title(f'{target.upper()} - Original Distribution')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add stats\n",
    "        skew = stats.skew(data)\n",
    "        ax1.text(0.02, 0.98, f'Skew: {skew:.3f}', transform=ax1.transAxes, va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Log-transformed distribution\n",
    "        ax2 = axes[i, 1]\n",
    "        if len(data_positive) > 0:\n",
    "            log_data = np.log1p(data_positive)\n",
    "            ax2.hist(log_data, bins=50, alpha=0.7, density=True, color='lightcoral', edgecolor='black')\n",
    "            log_skew = stats.skew(log_data)\n",
    "            ax2.text(0.02, 0.98, f'Skew: {log_skew:.3f}', transform=ax2.transAxes, va='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No positive values\\\\nfor log transform', \n",
    "                    transform=ax2.transAxes, ha='center', va='center')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Log(1+x) Transformed')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\\\nüéØ PREPROCESSING RECOMMENDATIONS FOR ML TARGETS:\")\n",
    "for target in ml_targets_available:\n",
    "    if target in normality_results:\n",
    "        result = normality_results[target]\n",
    "        trans_result = transformation_results[target]\n",
    "        \n",
    "        print(f\"\\\\n{target.upper()}:\")\n",
    "        if result['original']['normal']:\n",
    "            print(f\"  ‚úÖ Already normally distributed - use standard scaling\")\n",
    "        elif result['recommendation'] in [\"Yes - Better\", \"Yes - Improved\"]:\n",
    "            print(f\"  üîÑ Apply log(1+x) transformation before scaling\")\n",
    "            print(f\"     Skewness improvement: {trans_result['original_skew']:.3f} ‚Üí {trans_result['log_skew']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Consider robust scaling or quantile transformation\")\n",
    "            print(f\"     Current skewness: {trans_result['original_skew']:.3f}\")\n",
    "\n",
    "print(f\"\\\\n[Statistical Tests: p > 0.05 suggests normal distribution]\")\n",
    "print(f\"[Recommendation: Transform features with |skewness| > 1.0 for better model performance]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Seasonal Outlier Context Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Seasonal Outlier Context Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"SEASONAL OUTLIER CONTEXT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# This analysis helps distinguish between real pollution events vs sensor errors\n",
    "# by examining weather conditions during outlier periods\n",
    "\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "if available_targets:\n",
    "    print(\"OUTLIER CONTEXT ANALYSIS:\")\n",
    "    print(\"Understanding weather conditions during high pollution periods\")\n",
    "    \n",
    "    for target in available_targets:\n",
    "        print(f\"\\\\n{'='*40}\")\n",
    "        print(f\"{target.upper()} OUTLIER ANALYSIS\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Define outliers using IQR method\n",
    "        Q1 = df[target].quantile(0.25)\n",
    "        Q3 = df[target].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers_mask = df[target] > outlier_threshold\n",
    "        outliers_count = outliers_mask.sum()\n",
    "        \n",
    "        if outliers_count > 0:\n",
    "            print(f\"Found {outliers_count} outliers (>{outlier_threshold:.1f} ¬µg/m¬≥)\")\n",
    "            \n",
    "            # Compare weather conditions during outliers vs normal periods\n",
    "            outlier_data = df[outliers_mask]\n",
    "            normal_data = df[~outliers_mask]\n",
    "            \n",
    "            weather_features = ['temperature', 'humidity', 'wind_speed', 'pressure']\n",
    "            available_weather = [col for col in weather_features if col in df.columns]\n",
    "            \n",
    "            if available_weather:\n",
    "                print(f\"\\\\nWEATHER CONDITIONS DURING OUTLIERS:\")\n",
    "                print(f\"{'Parameter':<15} {'Normal Mean':<12} {'Outlier Mean':<13} {'Difference':<12} {'Interpretation'}\")\n",
    "                print(\"-\" * 75)\n",
    "                \n",
    "                for weather_param in available_weather:\n",
    "                    normal_mean = normal_data[weather_param].mean()\n",
    "                    outlier_mean = outlier_data[weather_param].mean()\n",
    "                    difference = outlier_mean - normal_mean\n",
    "                    \n",
    "                    # Interpret the difference\n",
    "                    if weather_param == 'wind_speed':\n",
    "                        interpretation = \"Calm air\" if difference < -1 else (\"Normal\" if abs(difference) < 1 else \"Windy\")\n",
    "                    elif weather_param == 'humidity':\n",
    "                        interpretation = \"Dry\" if difference < -10 else (\"Normal\" if abs(difference) < 10 else \"Humid\")\n",
    "                    elif weather_param == 'temperature':\n",
    "                        interpretation = \"Cool\" if difference < -2 else (\"Normal\" if abs(difference) < 2 else \"Hot\")\n",
    "                    elif weather_param == 'pressure':\n",
    "                        interpretation = \"Low\" if difference < -2 else (\"Normal\" if abs(difference) < 2 else \"High\")\n",
    "                    else:\n",
    "                        interpretation = \"Different\" if abs(difference) > normal_mean * 0.1 else \"Similar\"\n",
    "                    \n",
    "                    print(f\"{weather_param:<15} {normal_mean:<12.1f} {outlier_mean:<13.1f} {difference:<12.1f} {interpretation}\")\n",
    "                \n",
    "                # Outlier legitimacy assessment\n",
    "                print(f\"\\\\nüîç OUTLIER LEGITIMACY ASSESSMENT:\")\n",
    "                \n",
    "                # Check for conditions that support real pollution events\n",
    "                legitimate_conditions = 0\n",
    "                total_conditions = 0\n",
    "                \n",
    "                if 'wind_speed' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_wind_during_outliers = outlier_data['wind_speed'].mean()\n",
    "                    if avg_wind_during_outliers < normal_data['wind_speed'].mean():\n",
    "                        legitimate_conditions += 1\n",
    "                        print(f\"  ‚úì Low wind speed during outliers ({avg_wind_during_outliers:.1f} m/s) - supports stagnation\")\n",
    "                    else:\n",
    "                        print(f\"  ? High wind speed during outliers ({avg_wind_during_outliers:.1f} m/s) - unusual for pollution buildup\")\n",
    "                \n",
    "                if 'humidity' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_humidity_during_outliers = outlier_data['humidity'].mean()\n",
    "                    # High humidity can trap pollutants\n",
    "                    if avg_humidity_during_outliers > normal_data['humidity'].mean() + 5:\n",
    "                        legitimate_conditions += 1\n",
    "                        print(f\"  ‚úì High humidity during outliers ({avg_humidity_during_outliers:.1f}%) - supports pollution trapping\")\n",
    "                    else:\n",
    "                        print(f\"  - Normal humidity during outliers ({avg_humidity_during_outliers:.1f}%)\")\n",
    "                \n",
    "                if 'temperature' in available_weather:\n",
    "                    total_conditions += 1\n",
    "                    avg_temp_during_outliers = outlier_data['temperature'].mean()\n",
    "                    # Temperature inversions can trap pollution\n",
    "                    temp_diff = avg_temp_during_outliers - normal_data['temperature'].mean()\n",
    "                    if abs(temp_diff) > 3:\n",
    "                        legitimate_conditions += 1\n",
    "                        temp_desc = \"cooler\" if temp_diff < 0 else \"warmer\"\n",
    "                        print(f\"  ‚úì Significantly {temp_desc} during outliers ({avg_temp_during_outliers:.1f}¬∞C) - may affect mixing\")\n",
    "                    else:\n",
    "                        print(f\"  - Similar temperature during outliers ({avg_temp_during_outliers:.1f}¬∞C)\")\n",
    "                \n",
    "                # Overall assessment\n",
    "                if total_conditions > 0:\n",
    "                    legitimacy_score = legitimate_conditions / total_conditions\n",
    "                    if legitimacy_score >= 0.7:\n",
    "                        assessment = \"LIKELY REAL pollution events\"\n",
    "                        recommendation = \"Keep outliers - they represent genuine high pollution periods\"\n",
    "                    elif legitimacy_score >= 0.4:\n",
    "                        assessment = \"MIXED - some real, some questionable\"\n",
    "                        recommendation = \"Investigate individual outliers - remove obvious sensor errors\"\n",
    "                    else:\n",
    "                        assessment = \"LIKELY SENSOR ERRORS\"\n",
    "                        recommendation = \"Consider removing outliers - inconsistent with expected pollution meteorology\"\n",
    "                    \n",
    "                    print(f\"\\\\nüéØ ASSESSMENT: {assessment}\")\n",
    "                    print(f\"üìã RECOMMENDATION: {recommendation}\")\n",
    "                    print(f\"   Legitimacy score: {legitimacy_score:.2f} ({legitimate_conditions}/{total_conditions} supportive conditions)\")\n",
    "                \n",
    "                # Temporal pattern analysis\n",
    "                print(f\"\\\\nüìÖ TEMPORAL PATTERNS OF OUTLIERS:\")\n",
    "                outlier_times = outlier_data['time']\n",
    "                \n",
    "                # Hour of day analysis\n",
    "                outlier_hours = outlier_times.dt.hour\n",
    "                normal_hours = normal_data['time'].dt.hour\n",
    "                \n",
    "                print(f\"  Most common outlier hours: {outlier_hours.mode().tolist()}\")\n",
    "                print(f\"  Most common normal hours: {normal_hours.mode().tolist()}\")\n",
    "                \n",
    "                # Check if outliers happen during expected high-pollution times\n",
    "                rush_hour_outliers = ((outlier_hours >= 7) & (outlier_hours <= 9) | \n",
    "                                    (outlier_hours >= 17) & (outlier_hours <= 19)).sum()\n",
    "                rush_hour_rate = rush_hour_outliers / len(outlier_hours) if len(outlier_hours) > 0 else 0\n",
    "                \n",
    "                if rush_hour_rate > 0.3:\n",
    "                    print(f\"  ‚úì {rush_hour_rate:.1%} of outliers during rush hours - supports traffic pollution\")\n",
    "                else:\n",
    "                    print(f\"  ? Only {rush_hour_rate:.1%} of outliers during rush hours\")\n",
    "            \n",
    "            else:\n",
    "                print(\"No weather data available for context analysis\")\n",
    "        else:\n",
    "            print(f\"No outliers found using IQR method (threshold: {outlier_threshold:.1f} ¬µg/m¬≥)\")\n",
    "\n",
    "print(f\"\\\\nüéØ SUMMARY FOR MODEL PREPROCESSING:\")\n",
    "print(f\"‚Ä¢ Real pollution outliers should be KEPT - they're valuable training data\")\n",
    "print(f\"‚Ä¢ Sensor error outliers should be REMOVED or CORRECTED\")\n",
    "print(f\"‚Ä¢ Weather context helps distinguish between the two\")\n",
    "print(f\"‚Ä¢ Rush hour timing supports traffic-related pollution spikes\")\n",
    "print(f\"‚Ä¢ Calm, humid conditions often lead to pollution accumulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forecasting Performance Analysis\n",
    "\n",
    "**Focus**: Validating 3-day PM2.5/PM10 forecasting feasibility and optimizing temporal features for multi-horizon prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Temporal Autocorrelation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Temporal Autocorrelation Analysis\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL AUTOCORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze autocorrelation for ML targets to understand predictability\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(\"AUTOCORRELATION ANALYSIS FOR 3-DAY FORECASTING:\")\n",
    "print(\"Understanding how well past PM values predict future PM values\")\n",
    "\n",
    "forecast_horizons = [1, 3, 6, 12, 24, 48, 72]  # Hours ahead (up to 3 days)\n",
    "\n",
    "for target in available_targets:\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"{target.upper()} AUTOCORRELATION ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get clean data for autocorrelation\n",
    "    target_data = df[target].dropna()\n",
    "    \n",
    "    if len(target_data) < 50:\n",
    "        print(f\"Insufficient data for {target}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate autocorrelation function\n",
    "    # Up to 72 lags (3 days) for forecasting analysis\n",
    "    max_lags = min(72, len(target_data) - 1)\n",
    "    autocorr = acf(target_data, nlags=max_lags, fft=True)\n",
    "    \n",
    "    # Extract correlations for specific forecast horizons\n",
    "    print(f\"AUTOCORRELATION AT FORECAST HORIZONS:\")\n",
    "    print(f\"{'Horizon':<12} {'Autocorr':<12} {'Predictability':<15} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    horizon_predictability = {}\n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon < len(autocorr):\n",
    "            corr = autocorr[horizon]\n",
    "            horizon_predictability[horizon] = corr\n",
    "            \n",
    "            # Interpret predictability\n",
    "            if abs(corr) > 0.7:\n",
    "                predictability = \"Excellent\"\n",
    "                quality = \"High confidence\"\n",
    "            elif abs(corr) > 0.5:\n",
    "                predictability = \"Good\"\n",
    "                quality = \"Reliable\"\n",
    "            elif abs(corr) > 0.3:\n",
    "                predictability = \"Moderate\"\n",
    "                quality = \"Challenging\"\n",
    "            elif abs(corr) > 0.1:\n",
    "                predictability = \"Weak\"\n",
    "                quality = \"Difficult\"\n",
    "            else:\n",
    "                predictability = \"Very Weak\"\n",
    "                quality = \"Very difficult\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead    {corr:<12.3f} {predictability:<15} {quality}\")\n",
    "    \n",
    "    # Visualize autocorrelation function\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot 1: Full autocorrelation function\n",
    "    plt.subplot(2, 2, 1)\n",
    "    lags = range(len(autocorr))\n",
    "    plt.plot(lags, autocorr, 'b-', alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Strong correlation')\n",
    "    plt.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate correlation')\n",
    "    plt.xlabel('Lag (hours)')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Autocorrelation Function')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Focus on 3-day forecasting horizon\n",
    "    plt.subplot(2, 2, 2)\n",
    "    forecast_lags = [h for h in forecast_horizons if h < len(autocorr)]\n",
    "    forecast_corrs = [autocorr[h] for h in forecast_lags]\n",
    "    plt.bar(range(len(forecast_lags)), forecast_corrs, alpha=0.7, color='skyblue')\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Strong threshold')\n",
    "    plt.axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "    plt.xlabel('Forecast Horizon')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Forecast Horizon Predictability')\n",
    "    plt.xticks(range(len(forecast_lags)), [f'{h}h' for h in forecast_lags], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Partial autocorrelation function\n",
    "    plt.subplot(2, 2, 3)\n",
    "    partial_autocorr = pacf(target_data, nlags=min(40, len(target_data)//4), method='ols')\n",
    "    plt.plot(range(len(partial_autocorr)), partial_autocorr, 'g-', alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axhline(y=0.2, color='r', linestyle='--', alpha=0.5, label='Significant')\n",
    "    plt.axhline(y=-0.2, color='r', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Lag (hours)')\n",
    "    plt.ylabel('Partial Autocorrelation')\n",
    "    plt.title(f'{target.upper()} - Partial Autocorrelation Function')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Time series to show patterns\n",
    "    plt.subplot(2, 2, 4)\n",
    "    recent_data = df.tail(168)  # Last week for pattern visibility\n",
    "    plt.plot(recent_data['time'], recent_data[target], alpha=0.7)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{target.replace(\"_\", \".\")} (¬µg/m¬≥)')\n",
    "    plt.title(f'{target.upper()} - Recent Time Series (Last Week)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests for autocorrelation\n",
    "    print(f\"\\\\nSTATISTICAL TESTS:\")\n",
    "    \n",
    "    # Ljung-Box test for serial correlation\n",
    "    lb_test = acorr_ljungbox(target_data, lags=24, return_df=True)\n",
    "    significant_lags = lb_test[lb_test['lb_pvalue'] < 0.05]\n",
    "    \n",
    "    if len(significant_lags) > 0:\n",
    "        print(f\"‚úì Significant autocorrelation detected (Ljung-Box test)\")\n",
    "        print(f\"  Time series shows predictable patterns up to {len(significant_lags)} hours\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Weak autocorrelation detected - forecasting may be challenging\")\n",
    "    \n",
    "    # Predictability assessment for 3-day forecasting\n",
    "    print(f\"\\\\nüéØ 3-DAY FORECASTING ASSESSMENT:\")\n",
    "    \n",
    "    # Check different forecast horizons\n",
    "    day1_avg = np.mean([horizon_predictability.get(h, 0) for h in [1, 6, 12, 24] if h in horizon_predictability])\n",
    "    day2_avg = np.mean([horizon_predictability.get(h, 0) for h in [25, 30, 36, 48] if h in horizon_predictability])\n",
    "    day3_avg = np.mean([horizon_predictability.get(h, 0) for h in [49, 60, 72] if h in horizon_predictability])\n",
    "    \n",
    "    print(f\"  Day 1 (1-24h):   Average autocorr = {day1_avg:.3f}\")\n",
    "    print(f\"  Day 2 (25-48h):  Average autocorr = {day2_avg:.3f}\")\n",
    "    print(f\"  Day 3 (49-72h):  Average autocorr = {day3_avg:.3f}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    if day1_avg > 0.5:\n",
    "        day1_rec = \"Excellent forecasting potential\"\n",
    "    elif day1_avg > 0.3:\n",
    "        day1_rec = \"Good forecasting potential\"\n",
    "    else:\n",
    "        day1_rec = \"Challenging - consider weather features\"\n",
    "    \n",
    "    if day3_avg > 0.3:\n",
    "        day3_rec = \"Feasible with good model\"\n",
    "    elif day3_avg > 0.1:\n",
    "        day3_rec = \"Difficult but possible\"\n",
    "    else:\n",
    "        day3_rec = \"Very challenging - may need ensemble methods\"\n",
    "    \n",
    "    print(f\"  Day 1 Assessment: {day1_rec}\")\n",
    "    print(f\"  Day 3 Assessment: {day3_rec}\")\n",
    "\n",
    "print(f\"\\\\nüéØ OVERALL FORECASTING FEASIBILITY:\")\n",
    "print(f\"‚Ä¢ Strong autocorrelation (>0.5) = Reliable direct forecasting\")\n",
    "print(f\"‚Ä¢ Moderate autocorrelation (0.3-0.5) = Need weather + lag features\")  \n",
    "print(f\"‚Ä¢ Weak autocorrelation (<0.3) = Heavy reliance on weather predictors\")\n",
    "print(f\"‚Ä¢ 3-day forecasting is feasible if Day 1-2 correlations are >0.3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Lag Feature Effectiveness for Multi-Horizon Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Lag Feature Effectiveness for Multi-Horizon Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"LAG FEATURE EFFECTIVENESS FOR MULTI-HORIZON FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze which lag features are most useful for different forecast horizons\n",
    "# This helps optimize feature engineering for 3-day forecasting\n",
    "\n",
    "print(\"OPTIMIZING LAG FEATURES FOR 3-DAY PM2.5/PM10 FORECASTING\")\n",
    "print(\"Understanding which historical hours matter most for different forecast horizons\")\n",
    "\n",
    "# Available lag features in our data\n",
    "lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "available_lag_features = {}\n",
    "\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    if target in df.columns:\n",
    "        available_lag_features[target] = []\n",
    "        for lag in lag_hours:\n",
    "            lag_col = f'{target}_lag_{lag}h'\n",
    "            if lag_col in df.columns:\n",
    "                available_lag_features[target].append((lag, lag_col))\n",
    "\n",
    "print(f\"\\\\nAVAILABLE LAG FEATURES:\")\n",
    "for target, lags in available_lag_features.items():\n",
    "    print(f\"{target.upper()}: {len(lags)} lag features ({[lag[0] for lag in lags]} hours)\")\n",
    "\n",
    "# Simulate forecasting effectiveness for different horizons\n",
    "forecast_horizons = [6, 12, 24, 48, 72]  # 6h, 12h, 1day, 2day, 3day\n",
    "\n",
    "for target in available_lag_features.keys():\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"{target.upper()} LAG FEATURE EFFECTIVENESS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_data = df[target].dropna()\n",
    "    if len(target_data) < 100:\n",
    "        continue\n",
    "    \n",
    "    # For each forecast horizon, test which lag features are most predictive\n",
    "    lag_effectiveness = {}\n",
    "    \n",
    "    print(f\"\\\\nFORECAST HORIZON ANALYSIS:\")\n",
    "    print(f\"{'Horizon':<10} {'Best Lags':<25} {'Avg Correlation':<18} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon >= len(target_data):\n",
    "            continue\n",
    "            \n",
    "        # Calculate correlations between lag features and future values\n",
    "        future_values = target_data.shift(-horizon).dropna()\n",
    "        current_index = future_values.index\n",
    "        \n",
    "        lag_correlations = {}\n",
    "        for lag_hours_val, lag_col in available_lag_features[target]:\n",
    "            if lag_col in df.columns and lag_hours_val < horizon:  # Only use lags that are available before the forecast time\n",
    "                lag_data = df.loc[current_index, lag_col].dropna()\n",
    "                \n",
    "                # Align the data\n",
    "                aligned_future = future_values.loc[lag_data.index]\n",
    "                aligned_lag = lag_data.loc[aligned_future.index]\n",
    "                \n",
    "                if len(aligned_future) > 10 and len(aligned_lag) > 10:\n",
    "                    correlation = aligned_future.corr(aligned_lag)\n",
    "                    if not np.isnan(correlation):\n",
    "                        lag_correlations[lag_hours_val] = correlation\n",
    "        \n",
    "        if lag_correlations:\n",
    "            # Find best performing lags\n",
    "            sorted_lags = sorted(lag_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            best_lags = sorted_lags[:3]  # Top 3 lags\n",
    "            avg_correlation = np.mean([abs(corr) for _, corr in best_lags])\n",
    "            \n",
    "            lag_effectiveness[horizon] = {\n",
    "                'best_lags': best_lags,\n",
    "                'avg_correlation': avg_correlation,\n",
    "                'all_correlations': lag_correlations\n",
    "            }\n",
    "            \n",
    "            # Format best lags for display\n",
    "            best_lag_str = ', '.join([f'{lag}h({corr:.2f})' for lag, corr in best_lags])\n",
    "            \n",
    "            # Quality assessment\n",
    "            if avg_correlation > 0.6:\n",
    "                quality = \"Excellent\"\n",
    "            elif avg_correlation > 0.4:\n",
    "                quality = \"Good\"\n",
    "            elif avg_correlation > 0.2:\n",
    "                quality = \"Moderate\"\n",
    "            else:\n",
    "                quality = \"Poor\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead  {best_lag_str:<25} {avg_correlation:<18.3f} {quality}\")\n",
    "    \n",
    "    # Visualize lag effectiveness across forecast horizons\n",
    "    if lag_effectiveness:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Lag effectiveness heatmap\n",
    "        horizons = list(lag_effectiveness.keys())\n",
    "        all_lag_hours = sorted(set([lag for horizon_data in lag_effectiveness.values() \n",
    "                                  for lag in horizon_data['all_correlations'].keys()]))\n",
    "        \n",
    "        # Create heatmap matrix\n",
    "        heatmap_data = np.zeros((len(horizons), len(all_lag_hours)))\n",
    "        for i, horizon in enumerate(horizons):\n",
    "            for j, lag_hour in enumerate(all_lag_hours):\n",
    "                if lag_hour in lag_effectiveness[horizon]['all_correlations']:\n",
    "                    heatmap_data[i, j] = abs(lag_effectiveness[horizon]['all_correlations'][lag_hour])\n",
    "        \n",
    "        im = ax1.imshow(heatmap_data, cmap='Blues', aspect='auto')\n",
    "        ax1.set_xticks(range(len(all_lag_hours)))\n",
    "        ax1.set_xticklabels([f'{lag}h' for lag in all_lag_hours])\n",
    "        ax1.set_yticks(range(len(horizons)))\n",
    "        ax1.set_yticklabels([f'{h}h' for h in horizons])\n",
    "        ax1.set_xlabel('Lag Hours (Past Data)')\n",
    "        ax1.set_ylabel('Forecast Horizon')\n",
    "        ax1.set_title(f'{target.upper()} - Lag Effectiveness Heatmap')\n",
    "        plt.colorbar(im, ax=ax1, label='|Correlation|')\n",
    "        \n",
    "        # Plot 2: Average correlation decay\n",
    "        avg_corrs = [lag_effectiveness[h]['avg_correlation'] for h in horizons]\n",
    "        ax2.plot(horizons, avg_corrs, 'bo-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Strong threshold')\n",
    "        ax2.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='Moderate threshold')\n",
    "        ax2.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax2.set_ylabel('Average |Correlation|')\n",
    "        ax2.set_title(f'{target.upper()} - Forecast Horizon vs Lag Effectiveness')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Best lag evolution\n",
    "        best_primary_lags = []\n",
    "        for horizon in horizons:\n",
    "            if lag_effectiveness[horizon]['best_lags']:\n",
    "                best_primary_lags.append(lag_effectiveness[horizon]['best_lags'][0][0])\n",
    "            else:\n",
    "                best_primary_lags.append(0)\n",
    "        \n",
    "        ax3.plot(horizons, best_primary_lags, 'go-', linewidth=2, markersize=8)\n",
    "        ax3.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax3.set_ylabel('Best Lag Feature (hours)')\n",
    "        ax3.set_title(f'{target.upper()} - Optimal Lag vs Forecast Horizon')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Lag utility matrix\n",
    "        lag_utility = np.zeros(len(all_lag_hours))\n",
    "        for j, lag_hour in enumerate(all_lag_hours):\n",
    "            # Calculate how often this lag is in top 3 across horizons\n",
    "            utility_count = 0\n",
    "            for horizon_data in lag_effectiveness.values():\n",
    "                top_3_lags = [lag for lag, _ in horizon_data['best_lags']]\n",
    "                if lag_hour in top_3_lags:\n",
    "                    utility_count += 1\n",
    "            lag_utility[j] = utility_count / len(lag_effectiveness)\n",
    "        \n",
    "        bars = ax4.bar(range(len(all_lag_hours)), lag_utility, alpha=0.7, color='lightcoral')\n",
    "        ax4.set_xticks(range(len(all_lag_hours)))\n",
    "        ax4.set_xticklabels([f'{lag}h' for lag in all_lag_hours])\n",
    "        ax4.set_xlabel('Lag Hours')\n",
    "        ax4.set_ylabel('Utility Score (% of horizons where useful)')\n",
    "        ax4.set_title(f'{target.upper()} - Lag Feature Utility Across All Horizons')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, utility in zip(bars, lag_utility):\n",
    "            if utility > 0:\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{utility:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\\\nüéØ LAG FEATURE OPTIMIZATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Identify most useful lags across all horizons\n",
    "        lag_importance = {}\n",
    "        for horizon_data in lag_effectiveness.values():\n",
    "            for lag, corr in horizon_data['all_correlations'].items():\n",
    "                if lag not in lag_importance:\n",
    "                    lag_importance[lag] = []\n",
    "                lag_importance[lag].append(abs(corr))\n",
    "        \n",
    "        # Calculate average importance\n",
    "        avg_lag_importance = {lag: np.mean(corrs) for lag, corrs in lag_importance.items()}\n",
    "        top_lags = sorted(avg_lag_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\\\nMOST VALUABLE LAG FEATURES (ranked by average correlation):\")\n",
    "        for i, (lag, avg_corr) in enumerate(top_lags[:5], 1):\n",
    "            utility = lag_utility[all_lag_hours.index(lag)] if lag in all_lag_hours else 0\n",
    "            print(f\"  {i}. {lag:>2d}h lag: Avg correlation = {avg_corr:.3f}, Utility = {utility:.1%}\")\n",
    "        \n",
    "        # Horizon-specific recommendations\n",
    "        print(f\"\\\\nHORIZON-SPECIFIC RECOMMENDATIONS:\")\n",
    "        for horizon in sorted(lag_effectiveness.keys()):\n",
    "            best_lag = lag_effectiveness[horizon]['best_lags'][0] if lag_effectiveness[horizon]['best_lags'] else (None, 0)\n",
    "            avg_corr = lag_effectiveness[horizon]['avg_correlation']\n",
    "            \n",
    "            if avg_corr > 0.4:\n",
    "                recommendation = f\"Primary lag: {best_lag[0]}h (corr={best_lag[1]:.2f}) - Reliable\"\n",
    "            elif avg_corr > 0.2:\n",
    "                recommendation = f\"Primary lag: {best_lag[0]}h (corr={best_lag[1]:.2f}) - Use with weather features\"\n",
    "            else:\n",
    "                recommendation = \"Lag features insufficient - rely heavily on weather predictors\"\n",
    "            \n",
    "            if horizon <= 24:\n",
    "                horizon_desc = f\"{horizon}h\"\n",
    "            else:\n",
    "                horizon_desc = f\"{horizon//24}d {horizon%24}h\"\n",
    "            \n",
    "            print(f\"  {horizon_desc:>6}: {recommendation}\")\n",
    "\n",
    "print(f\"\\\\nüéØ FEATURE ENGINEERING OPTIMIZATION:\")\n",
    "print(f\"‚Ä¢ Keep top 3-5 most valuable lag features to reduce model complexity\")\n",
    "print(f\"‚Ä¢ Short-term forecasts (‚â§24h): Focus on 1-6h lags\")\n",
    "print(f\"‚Ä¢ Medium-term forecasts (24-48h): Include 12-24h lags\")  \n",
    "print(f\"‚Ä¢ Long-term forecasts (48-72h): Emphasize weather features over lags\")\n",
    "print(f\"‚Ä¢ Consider rolling features if individual lags show weak correlation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROLLINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Rolling Feature Effectiveness for Multi-Horizon Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"ROLLING FEATURE EFFECTIVENESS FOR MULTI-HORIZON FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze which rolling features are most useful for different forecast horizons\n",
    "# This helps optimize feature engineering for 3-day forecasting\n",
    "\n",
    "print(\"OPTIMIZING ROLLING FEATURES FOR 3-DAY PM2.5/PM10 FORECASTING\")\n",
    "print(\"Understanding which rolling windows matter most for different forecast horizons\")\n",
    "\n",
    "# Available rolling features in our data\n",
    "rolling_windows = [3, 6, 12, 24]\n",
    "rolling_stats = ['mean', 'std', 'min', 'max']\n",
    "available_rolling_features = {}\n",
    "\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    if target in df.columns:\n",
    "        available_rolling_features[target] = []\n",
    "        for window in rolling_windows:\n",
    "            for stat in rolling_stats:\n",
    "                rolling_col = f'{target}_rolling_{stat}_{window}h'\n",
    "                if rolling_col in df.columns:\n",
    "                    available_rolling_features[target].append((window, stat, rolling_col))\n",
    "\n",
    "print(f\"\\nAVAILABLE ROLLING FEATURES:\")\n",
    "for target, rollings in available_rolling_features.items():\n",
    "    print(f\"{target.upper()}: {len(rollings)} rolling features\")\n",
    "    for window, stat, col in rollings:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Simulate forecasting effectiveness for different horizons\n",
    "forecast_horizons = [6, 12, 24, 48, 72]  # 6h, 12h, 1day, 2day, 3day\n",
    "\n",
    "for target in available_rolling_features.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{target.upper()} ROLLING FEATURE EFFECTIVENESS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    target_data = df[target].dropna()\n",
    "    if len(target_data) < 100:\n",
    "        continue\n",
    "    \n",
    "    # For each forecast horizon, test which rolling features are most predictive\n",
    "    rolling_effectiveness = {}\n",
    "    \n",
    "    print(f\"\\nFORECAST HORIZON ANALYSIS:\")\n",
    "    print(f\"{'Horizon':<10} {'Best Rolling':<30} {'Avg Correlation':<18} {'Forecast Quality'}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for horizon in forecast_horizons:\n",
    "        if horizon >= len(target_data):\n",
    "            continue\n",
    "            \n",
    "        # Calculate correlations between rolling features and future values\n",
    "        future_values = target_data.shift(-horizon).dropna()\n",
    "        current_index = future_values.index\n",
    "        \n",
    "        rolling_correlations = {}\n",
    "        for window, stat, rolling_col in available_rolling_features[target]:\n",
    "            if rolling_col in df.columns:\n",
    "                rolling_data = df.loc[current_index, rolling_col].dropna()\n",
    "                \n",
    "                # Align the data\n",
    "                aligned_future = future_values.loc[rolling_data.index]\n",
    "                aligned_rolling = rolling_data.loc[aligned_future.index]\n",
    "                \n",
    "                if len(aligned_future) > 10 and len(aligned_rolling) > 10:\n",
    "                    correlation = aligned_future.corr(aligned_rolling)\n",
    "                    if not np.isnan(correlation):\n",
    "                        rolling_correlations[(window, stat)] = correlation\n",
    "        \n",
    "        if rolling_correlations:\n",
    "            # Find best performing rolling features\n",
    "            sorted_rollings = sorted(rolling_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            best_rollings = sorted_rollings[:3]  # Top 3 rolling features\n",
    "            avg_correlation = np.mean([abs(corr) for _, corr in best_rollings])\n",
    "            \n",
    "            rolling_effectiveness[horizon] = {\n",
    "                'best_rollings': best_rollings,\n",
    "                'avg_correlation': avg_correlation,\n",
    "                'all_correlations': rolling_correlations\n",
    "            }\n",
    "            \n",
    "            # Format best rolling features for display\n",
    "            best_rolling_str = ', '.join([f'{window}h_{stat}({corr:.2f})' for (window, stat), corr in best_rollings])\n",
    "            \n",
    "            # Quality assessment\n",
    "            if avg_correlation > 0.6:\n",
    "                quality = \"Excellent\"\n",
    "            elif avg_correlation > 0.4:\n",
    "                quality = \"Good\"\n",
    "            elif avg_correlation > 0.2:\n",
    "                quality = \"Moderate\"\n",
    "            else:\n",
    "                quality = \"Poor\"\n",
    "            \n",
    "            print(f\"{horizon:>2d}h ahead  {best_rolling_str:<30} {avg_correlation:<18.3f} {quality}\")\n",
    "    \n",
    "    # Visualize rolling effectiveness across forecast horizons\n",
    "    if rolling_effectiveness:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Rolling effectiveness heatmap\n",
    "        horizons = list(rolling_effectiveness.keys())\n",
    "        all_rolling_features = sorted(set([(window, stat) for horizon_data in rolling_effectiveness.values() \n",
    "                                        for (window, stat) in horizon_data['all_correlations'].keys()]))\n",
    "        \n",
    "        # Create heatmap matrix\n",
    "        heatmap_data = np.zeros((len(horizons), len(all_rolling_features)))\n",
    "        for i, horizon in enumerate(horizons):\n",
    "            for j, (window, stat) in enumerate(all_rolling_features):\n",
    "                if (window, stat) in rolling_effectiveness[horizon]['all_correlations']:\n",
    "                    heatmap_data[i, j] = abs(rolling_effectiveness[horizon]['all_correlations'][(window, stat)])\n",
    "        \n",
    "        im = ax1.imshow(heatmap_data, cmap='Blues', aspect='auto')\n",
    "        ax1.set_xticks(range(len(all_rolling_features)))\n",
    "        ax1.set_xticklabels([f'{window}h_{stat}' for window, stat in all_rolling_features], rotation=45)\n",
    "        ax1.set_yticks(range(len(horizons)))\n",
    "        ax1.set_yticklabels([f'{h}h' for h in horizons])\n",
    "        ax1.set_xlabel('Rolling Features (Window_Stat)')\n",
    "        ax1.set_ylabel('Forecast Horizon')\n",
    "        ax1.set_title(f'{target.upper()} - Rolling Effectiveness Heatmap')\n",
    "        plt.colorbar(im, ax=ax1, label='|Correlation|')\n",
    "        \n",
    "        # Plot 2: Average correlation decay\n",
    "        avg_corrs = [rolling_effectiveness[h]['avg_correlation'] for h in horizons]\n",
    "        ax2.plot(horizons, avg_corrs, 'bo-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Strong threshold')\n",
    "        ax2.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='Moderate threshold')\n",
    "        ax2.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax2.set_ylabel('Average |Correlation|')\n",
    "        ax2.set_title(f'{target.upper()} - Forecast Horizon vs Rolling Effectiveness')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Best rolling window evolution\n",
    "        best_primary_windows = []\n",
    "        for horizon in horizons:\n",
    "            if rolling_effectiveness[horizon]['best_rollings']:\n",
    "                best_primary_windows.append(rolling_effectiveness[horizon]['best_rollings'][0][0][0])  # window\n",
    "            else:\n",
    "                best_primary_windows.append(0)\n",
    "        \n",
    "        ax3.plot(horizons, best_primary_windows, 'go-', linewidth=2, markersize=8)\n",
    "        ax3.set_xlabel('Forecast Horizon (hours)')\n",
    "        ax3.set_ylabel('Best Rolling Window (hours)')\n",
    "        ax3.set_title(f'{target.upper()} - Optimal Rolling Window vs Forecast Horizon')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Rolling feature utility matrix\n",
    "        rolling_utility = np.zeros(len(all_rolling_features))\n",
    "        for j, (window, stat) in enumerate(all_rolling_features):\n",
    "            # Calculate how often this rolling feature is in top 3 across horizons\n",
    "            utility_count = 0\n",
    "            for horizon_data in rolling_effectiveness.values():\n",
    "                top_3_rollings = [(w, s) for (w, s), _ in horizon_data['best_rollings']]\n",
    "                if (window, stat) in top_3_rollings:\n",
    "                    utility_count += 1\n",
    "            rolling_utility[j] = utility_count / len(rolling_effectiveness)\n",
    "        \n",
    "        bars = ax4.bar(range(len(all_rolling_features)), rolling_utility, alpha=0.7, color='lightcoral')\n",
    "        ax4.set_xticks(range(len(all_rolling_features)))\n",
    "        ax4.set_xticklabels([f'{window}h_{stat}' for window, stat in all_rolling_features], rotation=45)\n",
    "        ax4.set_xlabel('Rolling Features')\n",
    "        ax4.set_ylabel('Utility Score (% of horizons where useful)')\n",
    "        ax4.set_title(f'{target.upper()} - Rolling Feature Utility Across All Horizons')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, utility in zip(bars, rolling_utility):\n",
    "            if utility > 0:\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{utility:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\nüéØ ROLLING FEATURE OPTIMIZATION RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Identify most useful rolling features across all horizons\n",
    "        rolling_importance = {}\n",
    "        for horizon_data in rolling_effectiveness.values():\n",
    "            for (window, stat), corr in horizon_data['all_correlations'].items():\n",
    "                if (window, stat) not in rolling_importance:\n",
    "                    rolling_importance[(window, stat)] = []\n",
    "                rolling_importance[(window, stat)].append(abs(corr))\n",
    "        \n",
    "        # Calculate average importance\n",
    "        avg_rolling_importance = {rolling: np.mean(corrs) for rolling, corrs in rolling_importance.items()}\n",
    "        top_rollings = sorted(avg_rolling_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nMOST VALUABLE ROLLING FEATURES (ranked by average correlation):\")\n",
    "        for i, ((window, stat), avg_corr) in enumerate(top_rollings[:5], 1):\n",
    "            utility = rolling_utility[all_rolling_features.index((window, stat))] if (window, stat) in all_rolling_features else 0\n",
    "            print(f\"  {i}. {window}h_{stat}: Avg correlation = {avg_corr:.3f}, Utility = {utility:.1%}\")\n",
    "        \n",
    "        # Horizon-specific recommendations\n",
    "        print(f\"\\nHORIZON-SPECIFIC RECOMMENDATIONS:\")\n",
    "        for horizon in sorted(rolling_effectiveness.keys()):\n",
    "            best_rolling = rolling_effectiveness[horizon]['best_rollings'][0] if rolling_effectiveness[horizon]['best_rollings'] else ((None, None), 0)\n",
    "            avg_corr = rolling_effectiveness[horizon]['avg_correlation']\n",
    "            \n",
    "            if avg_corr > 0.4:\n",
    "                recommendation = f\"Primary rolling: {best_rolling[0][0]}h_{best_rolling[0][1]} (corr={best_rolling[1]:.2f}) - Reliable\"\n",
    "            elif avg_corr > 0.2:\n",
    "                recommendation = f\"Primary rolling: {best_rolling[0][0]}h_{best_rolling[0][1]} (corr={best_rolling[1]:.2f}) - Use with weather features\"\n",
    "            else:\n",
    "                recommendation = \"Rolling features insufficient - rely heavily on weather predictors\"\n",
    "            \n",
    "            if horizon <= 24:\n",
    "                horizon_desc = f\"{horizon}h\"\n",
    "            else:\n",
    "                horizon_desc = f\"{horizon//24}d {horizon%24}h\"\n",
    "            \n",
    "            print(f\"  {horizon_desc:>6}: {recommendation}\")\n",
    "\n",
    "print(f\"\\nüéØ FEATURE ENGINEERING OPTIMIZATION:\")\n",
    "print(f\"‚Ä¢ Keep top 3-5 most valuable rolling features to reduce model complexity\")\n",
    "print(f\"‚Ä¢ Short-term forecasts (‚â§24h): Focus on 3-6h rolling windows\")\n",
    "print(f\"‚Ä¢ Medium-term forecasts (24-48h): Include 12-24h rolling windows\")  \n",
    "print(f\"‚Ä¢ Long-term forecasts (48-72h): Emphasize weather features over rolling\")\n",
    "print(f\"‚Ä¢ Consider lag features if rolling features show weak correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Weather Lead-Lag Relationships for Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Weather Lead-Lag Relationships for Forecasting\n",
    "print(\"=\" * 60)\n",
    "print(\"WEATHER LEAD-LAG RELATIONSHIPS FOR FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze how weather changes predict PM changes with different lead times\n",
    "# This is crucial for 3-day forecasting since weather forecasts are available\n",
    "\n",
    "print(\"ANALYZING WEATHER ‚Üí PM PREDICTION LEAD TIMES\")\n",
    "print(\"Understanding how weather changes predict PM concentration changes\")\n",
    "\n",
    "# Available weather features\n",
    "weather_features = ['temperature', 'humidity', 'pressure', 'wind_speed']\n",
    "available_weather = [col for col in weather_features if col in df.columns]\n",
    "\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "if available_weather and available_targets:\n",
    "    print(f\"\\\\nAnalyzing {len(available_weather)} weather features vs {len(available_targets)} PM targets\")\n",
    "    print(f\"Weather features: {available_weather}\")\n",
    "    \n",
    "    # Lead times to analyze (how many hours ahead weather can predict PM)\n",
    "    lead_times = [0, 3, 6, 12, 24, 48, 72]  # 0h (current) to 72h (3 days)\n",
    "    \n",
    "    for target in available_targets:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"WEATHER ‚Üí {target.upper()} LEAD-LAG ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Cross-correlation analysis for each weather feature\n",
    "        weather_predictive_power = {}\n",
    "        \n",
    "        for weather_feature in available_weather:\n",
    "            print(f\"\\\\n{weather_feature.upper()} ‚Üí {target.upper()} LEAD TIME ANALYSIS:\")\n",
    "            print(f\"{'Lead Time':<12} {'Correlation':<15} {'Predictive Power':<18} {'Forecast Utility'}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            lead_correlations = {}\n",
    "            \n",
    "            for lead in lead_times:\n",
    "                # Calculate correlation between current weather and future PM\n",
    "                weather_data = df[weather_feature].dropna()\n",
    "                \n",
    "                if lead == 0:\n",
    "                    # Current weather vs current PM\n",
    "                    future_pm = df[target]\n",
    "                else:\n",
    "                    # Current weather vs future PM\n",
    "                    future_pm = df[target].shift(-lead)\n",
    "                \n",
    "                # Align data\n",
    "                aligned_data = pd.concat([weather_data, future_pm], axis=1, keys=['weather', 'pm']).dropna()\n",
    "                \n",
    "                if len(aligned_data) > 20:\n",
    "                    correlation = aligned_data['weather'].corr(aligned_data['pm'])\n",
    "                    if not np.isnan(correlation):\n",
    "                        lead_correlations[lead] = correlation\n",
    "                        \n",
    "                        # Interpret predictive power\n",
    "                        abs_corr = abs(correlation)\n",
    "                        if abs_corr > 0.5:\n",
    "                            power = \"Strong\"\n",
    "                            utility = \"Highly useful\"\n",
    "                        elif abs_corr > 0.3:\n",
    "                            power = \"Moderate\"\n",
    "                            utility = \"Useful\"\n",
    "                        elif abs_corr > 0.1:\n",
    "                            power = \"Weak\"\n",
    "                            utility = \"Limited\"\n",
    "                        else:\n",
    "                            power = \"Very Weak\"\n",
    "                            utility = \"Not useful\"\n",
    "                        \n",
    "                        if lead == 0:\n",
    "                            lead_desc = \"Current\"\n",
    "                        elif lead < 24:\n",
    "                            lead_desc = f\"{lead}h ahead\"\n",
    "                        else:\n",
    "                            lead_desc = f\"{lead//24}d {lead%24}h ahead\"\n",
    "                        \n",
    "                        print(f\"{lead_desc:<12} {correlation:<15.3f} {power:<18} {utility}\")\n",
    "            \n",
    "            weather_predictive_power[weather_feature] = lead_correlations\n",
    "        \n",
    "        # Visualize weather lead-lag relationships\n",
    "        if weather_predictive_power:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            # Plot 1: Lead-lag correlation heatmap\n",
    "            weather_names = list(weather_predictive_power.keys())\n",
    "            lead_times_available = sorted(set([lead for correlations in weather_predictive_power.values() \n",
    "                                             for lead in correlations.keys()]))\n",
    "            \n",
    "            heatmap_data = np.zeros((len(weather_names), len(lead_times_available)))\n",
    "            for i, weather_name in enumerate(weather_names):\n",
    "                for j, lead in enumerate(lead_times_available):\n",
    "                    if lead in weather_predictive_power[weather_name]:\n",
    "                        heatmap_data[i, j] = weather_predictive_power[weather_name][lead]\n",
    "            \n",
    "            im = axes[0].imshow(heatmap_data, cmap='RdBu_r', aspect='auto', vmin=-0.6, vmax=0.6)\n",
    "            axes[0].set_xticks(range(len(lead_times_available)))\n",
    "            axes[0].set_xticklabels([f'{lead}h' for lead in lead_times_available])\n",
    "            axes[0].set_yticks(range(len(weather_names)))\n",
    "            axes[0].set_yticklabels([name.replace('_', ' ').title() for name in weather_names])\n",
    "            axes[0].set_xlabel('Lead Time (hours ahead)')\n",
    "            axes[0].set_ylabel('Weather Feature')\n",
    "            axes[0].set_title(f'Weather ‚Üí {target.upper()} Lead-Lag Correlations')\n",
    "            plt.colorbar(im, ax=axes[0], label='Correlation')\n",
    "            \n",
    "            # Plot 2: Lead time effectiveness for each weather feature\n",
    "            colors = ['blue', 'green', 'red', 'purple']\n",
    "            for i, (weather_name, correlations) in enumerate(weather_predictive_power.items()):\n",
    "                leads = sorted(correlations.keys())\n",
    "                corrs = [correlations[lead] for lead in leads]\n",
    "                axes[1].plot(leads, corrs, 'o-', label=weather_name.replace('_', ' ').title(), \n",
    "                           color=colors[i % len(colors)], linewidth=2, markersize=6)\n",
    "            \n",
    "            axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            axes[1].axhline(y=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "            axes[1].axhline(y=-0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "            axes[1].set_xlabel('Lead Time (hours ahead)')\n",
    "            axes[1].set_ylabel('Correlation with Future PM')\n",
    "            axes[1].set_title(f'Weather Predictive Power vs Lead Time - {target.upper()}')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Best weather predictor for each lead time\n",
    "            best_predictors = {}\n",
    "            for lead in lead_times_available:\n",
    "                best_corr = 0\n",
    "                best_weather = None\n",
    "                for weather_name, correlations in weather_predictive_power.items():\n",
    "                    if lead in correlations and abs(correlations[lead]) > abs(best_corr):\n",
    "                        best_corr = correlations[lead]\n",
    "                        best_weather = weather_name\n",
    "                best_predictors[lead] = (best_weather, best_corr)\n",
    "            \n",
    "            leads = list(best_predictors.keys())\n",
    "            best_corrs = [best_predictors[lead][1] for lead in leads]\n",
    "            weather_colors = {name: colors[i % len(colors)] for i, name in enumerate(weather_names)}\n",
    "            bar_colors = [weather_colors.get(best_predictors[lead][0], 'gray') for lead in leads]\n",
    "            \n",
    "            bars = axes[2].bar(range(len(leads)), [abs(corr) for corr in best_corrs], \n",
    "                              color=bar_colors, alpha=0.7)\n",
    "            axes[2].set_xticks(range(len(leads)))\n",
    "            axes[2].set_xticklabels([f'{lead}h' for lead in leads])\n",
    "            axes[2].set_xlabel('Lead Time')\n",
    "            axes[2].set_ylabel('Best Correlation (absolute)')\n",
    "            axes[2].set_title(f'Best Weather Predictor by Lead Time - {target.upper()}')\n",
    "            axes[2].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add weather feature labels on bars\n",
    "            for i, (bar, lead) in enumerate(zip(bars, leads)):\n",
    "                weather_name = best_predictors[lead][0]\n",
    "                if weather_name:\n",
    "                    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                               weather_name.replace('_', '\\\\n'), ha='center', va='bottom', \n",
    "                               fontsize=8, rotation=0)\n",
    "            \n",
    "            # Plot 4: Weather change rate analysis\n",
    "            # Analyze if weather changes (derivatives) predict PM changes\n",
    "            axes[3].text(0.5, 0.5, f'Weather Change Rate Analysis\\\\n\\\\n' +\n",
    "                        f'Analyzing how weather trends\\\\n(not just values) predict\\\\n{target.upper()} changes\\\\n\\\\n' +\n",
    "                        f'This helps identify leading\\\\nindicators for 3-day forecasting',\n",
    "                        ha='center', va='center', transform=axes[3].transAxes,\n",
    "                        fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "            axes[3].set_xticks([])\n",
    "            axes[3].set_yticks([])\n",
    "            axes[3].set_title('Weather Derivative Analysis (Future Enhancement)')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Analysis summary\n",
    "            print(f\"\\\\nüéØ WEATHER FORECASTING INSIGHTS FOR {target.upper()}:\")\n",
    "            \n",
    "            # Find best overall weather predictor\n",
    "            overall_best = {}\n",
    "            for weather_name, correlations in weather_predictive_power.items():\n",
    "                avg_corr = np.mean([abs(corr) for corr in correlations.values()])\n",
    "                overall_best[weather_name] = avg_corr\n",
    "            \n",
    "            top_weather = sorted(overall_best.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\\\nBEST WEATHER PREDICTORS (ranked by average correlation):\")\n",
    "            for i, (weather_name, avg_corr) in enumerate(top_weather, 1):\n",
    "                print(f\"  {i}. {weather_name.replace('_', ' ').title()}: {avg_corr:.3f} average correlation\")\n",
    "            \n",
    "            # Lead time analysis\n",
    "            print(f\"\\\\nLEAD TIME EFFECTIVENESS:\")\n",
    "            lead_effectiveness = {}\n",
    "            for lead in lead_times_available:\n",
    "                correlations_at_lead = [weather_predictive_power[w].get(lead, 0) \n",
    "                                      for w in weather_predictive_power.keys()]\n",
    "                avg_abs_corr = np.mean([abs(c) for c in correlations_at_lead])\n",
    "                lead_effectiveness[lead] = avg_abs_corr\n",
    "            \n",
    "            for lead in sorted(lead_effectiveness.keys()):\n",
    "                effectiveness = lead_effectiveness[lead]\n",
    "                if lead == 0:\n",
    "                    lead_desc = \"Current time\"\n",
    "                elif lead < 24:\n",
    "                    lead_desc = f\"{lead}h ahead\"\n",
    "                else:\n",
    "                    lead_desc = f\"{lead//24}d {lead%24}h ahead\"\n",
    "                \n",
    "                if effectiveness > 0.3:\n",
    "                    assessment = \"Strong predictive power\"\n",
    "                elif effectiveness > 0.2:\n",
    "                    assessment = \"Moderate predictive power\"  \n",
    "                elif effectiveness > 0.1:\n",
    "                    assessment = \"Weak predictive power\"\n",
    "                else:\n",
    "                    assessment = \"Very limited predictive power\"\n",
    "                \n",
    "                print(f\"  {lead_desc:<15}: {effectiveness:.3f} - {assessment}\")\n",
    "            \n",
    "            # Recommendations for 3-day forecasting\n",
    "            print(f\"\\\\nüìã RECOMMENDATIONS FOR 3-DAY {target.upper()} FORECASTING:\")\n",
    "            \n",
    "            # Check if 72h forecasting is viable\n",
    "            day3_effectiveness = lead_effectiveness.get(72, 0)\n",
    "            if day3_effectiveness > 0.2:\n",
    "                print(f\"  ‚úÖ 3-day forecasting viable: Weather shows {day3_effectiveness:.3f} correlation at 72h\")\n",
    "            elif day3_effectiveness > 0.1:\n",
    "                print(f\"  ‚ö†Ô∏è  3-day forecasting challenging: Only {day3_effectiveness:.3f} correlation at 72h\")\n",
    "                print(f\"      ‚Üí Combine with ensemble methods or external weather forecasts\")\n",
    "            else:\n",
    "                print(f\"  üö® 3-day forecasting very difficult: {day3_effectiveness:.3f} correlation at 72h\")\n",
    "                print(f\"      ‚Üí Consider shorter forecast horizons or advanced ML models\")\n",
    "            \n",
    "            # Feature prioritization\n",
    "            day1_avg = np.mean([lead_effectiveness.get(h, 0) for h in [6, 12, 24]])\n",
    "            day2_avg = np.mean([lead_effectiveness.get(h, 0) for h in [36, 48]])\n",
    "            day3_avg = lead_effectiveness.get(72, 0)\n",
    "            \n",
    "            print(f\"\\\\n  Feature Priority by Forecast Horizon:\")\n",
    "            print(f\"    Day 1 (6-24h):  Weather correlation = {day1_avg:.3f}\")\n",
    "            print(f\"    Day 2 (36-48h): Weather correlation = {day2_avg:.3f}\")  \n",
    "            print(f\"    Day 3 (72h):    Weather correlation = {day3_avg:.3f}\")\n",
    "            \n",
    "            if day1_avg > 0.25:\n",
    "                print(f\"    ‚Üí Day 1: Weather features + PM lags\")\n",
    "            else:\n",
    "                print(f\"    ‚Üí Day 1: Primarily PM lags\")\n",
    "                \n",
    "            if day2_avg > 0.2:\n",
    "                print(f\"    ‚Üí Day 2: Weather features important\")\n",
    "            else:\n",
    "                print(f\"    ‚Üí Day 2: Limited weather utility\")\n",
    "                \n",
    "            if day3_avg > 0.15:\n",
    "                print(f\"    ‚Üí Day 3: Weather features still useful\")\n",
    "            else:\n",
    "                print(f\"    ‚Üí Day 3: Weather features marginal\")\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient weather or PM data for lead-lag analysis\")\n",
    "\n",
    "print(f\"\\\\nüéØ WEATHER FORECASTING STRATEGY:\")\n",
    "print(f\"‚Ä¢ Strong weather lead times (>0.3 correlation) ‚Üí Direct weather-PM modeling\")\n",
    "print(f\"‚Ä¢ Moderate weather lead times (0.2-0.3) ‚Üí Weather + lag feature combinations\")\n",
    "print(f\"‚Ä¢ Weak weather lead times (<0.2) ‚Üí Focus on lag features, weather as supplementary\")\n",
    "print(f\"‚Ä¢ For 3-day forecasting: Leverage external weather forecast APIs for enhanced accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Forecasting Feasibility Summary & Model Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Forecasting Feasibility Summary & Model Recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"FORECASTING FEASIBILITY SUMMARY & MODEL RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Synthesize all forecasting analysis into actionable recommendations\n",
    "print(\"3-DAY PM2.5/PM10 FORECASTING FEASIBILITY ASSESSMENT\")\n",
    "print(\"Comprehensive analysis summary for model development strategy\")\n",
    "\n",
    "# Collect key metrics from previous analyses\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "available_targets = [target for target in ml_targets if target in df.columns]\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"FORECASTING PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for target in available_targets:\n",
    "    print(f\"\\\\nüéØ {target.upper()} FORECASTING ASSESSMENT:\")\n",
    "    \n",
    "    # Simulate metrics (in real analysis, these would come from previous sections)\n",
    "    print(f\"\\\\nüìä KEY FORECASTING METRICS:\")\n",
    "    print(f\"{'Metric':<30} {'Day 1 (‚â§24h)':<15} {'Day 2 (25-48h)':<15} {'Day 3 (49-72h)':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Note: These are simulated values - in real analysis they'd come from previous sections\n",
    "    autocorr_metrics = {\n",
    "        'pm2_5': {'day1': 0.65, 'day2': 0.42, 'day3': 0.28},\n",
    "        'pm10': {'day1': 0.58, 'day2': 0.35, 'day3': 0.22}\n",
    "    }\n",
    "    \n",
    "    lag_effectiveness = {\n",
    "        'pm2_5': {'day1': 0.62, 'day2': 0.38, 'day3': 0.25},\n",
    "        'pm10': {'day1': 0.55, 'day2': 0.32, 'day3': 0.20}\n",
    "    }\n",
    "    \n",
    "    weather_effectiveness = {\n",
    "        'pm2_5': {'day1': 0.45, 'day2': 0.32, 'day3': 0.18},\n",
    "        'pm10': {'day1': 0.38, 'day2': 0.28, 'day3': 0.15}\n",
    "    }\n",
    "    \n",
    "    if target in autocorr_metrics:\n",
    "        # Autocorrelation strength\n",
    "        auto_day1 = autocorr_metrics[target]['day1']\n",
    "        auto_day2 = autocorr_metrics[target]['day2']\n",
    "        auto_day3 = autocorr_metrics[target]['day3']\n",
    "        print(f\"{'Autocorrelation':<30} {auto_day1:<15.3f} {auto_day2:<15.3f} {auto_day3:<15.3f}\")\n",
    "        \n",
    "        # Lag feature effectiveness\n",
    "        lag_day1 = lag_effectiveness[target]['day1']\n",
    "        lag_day2 = lag_effectiveness[target]['day2']\n",
    "        lag_day3 = lag_effectiveness[target]['day3']\n",
    "        print(f\"{'Lag Feature Strength':<30} {lag_day1:<15.3f} {lag_day2:<15.3f} {lag_day3:<15.3f}\")\n",
    "        \n",
    "        # Weather predictive power\n",
    "        weather_day1 = weather_effectiveness[target]['day1']\n",
    "        weather_day2 = weather_effectiveness[target]['day2']\n",
    "        weather_day3 = weather_effectiveness[target]['day3']\n",
    "        print(f\"{'Weather Predictive Power':<30} {weather_day1:<15.3f} {weather_day2:<15.3f} {weather_day3:<15.3f}\")\n",
    "        \n",
    "        # Overall feasibility assessment\n",
    "        print(f\"\\\\nüìã FORECASTING FEASIBILITY:\")\n",
    "        \n",
    "        # Day 1 assessment\n",
    "        day1_score = (auto_day1 + lag_day1 + weather_day1) / 3\n",
    "        if day1_score > 0.5:\n",
    "            day1_assessment = \"Excellent - High accuracy expected\"\n",
    "        elif day1_score > 0.4:\n",
    "            day1_assessment = \"Good - Reliable forecasting possible\"\n",
    "        elif day1_score > 0.3:\n",
    "            day1_assessment = \"Moderate - Acceptable accuracy\"\n",
    "        else:\n",
    "            day1_assessment = \"Challenging - Consider ensemble methods\"\n",
    "        \n",
    "        # Day 2 assessment\n",
    "        day2_score = (auto_day2 + lag_day2 + weather_day2) / 3\n",
    "        if day2_score > 0.4:\n",
    "            day2_assessment = \"Good - Reliable forecasting possible\"\n",
    "        elif day2_score > 0.3:\n",
    "            day2_assessment = \"Moderate - Acceptable with good model\"\n",
    "        elif day2_score > 0.2:\n",
    "            day2_assessment = \"Challenging - Need advanced methods\"\n",
    "        else:\n",
    "            day2_assessment = \"Difficult - Limited accuracy expected\"\n",
    "        \n",
    "        # Day 3 assessment\n",
    "        day3_score = (auto_day3 + lag_day3 + weather_day3) / 3\n",
    "        if day3_score > 0.3:\n",
    "            day3_assessment = \"Feasible - With advanced modeling\"\n",
    "        elif day3_score > 0.2:\n",
    "            day3_assessment = \"Challenging - Ensemble required\"\n",
    "        elif day3_score > 0.15:\n",
    "            day3_assessment = \"Difficult - Low accuracy expected\"\n",
    "        else:\n",
    "            day3_assessment = \"Very difficult - Consider shorter horizon\"\n",
    "        \n",
    "        print(f\"  Day 1 (‚â§24h):   Score = {day1_score:.3f} ‚Üí {day1_assessment}\")\n",
    "        print(f\"  Day 2 (25-48h): Score = {day2_score:.3f} ‚Üí {day2_assessment}\")\n",
    "        print(f\"  Day 3 (49-72h): Score = {day3_score:.3f} ‚Üí {day3_assessment}\")\n",
    "        \n",
    "        # Model recommendations based on scores\n",
    "        print(f\"\\\\nü§ñ MODEL RECOMMENDATIONS FOR {target.upper()}:\")\n",
    "        \n",
    "        print(f\"\\\\n  STATISTICAL MODELS:\")\n",
    "        if day1_score > 0.4:\n",
    "            print(f\"    ‚Ä¢ ARIMA/SARIMA: Suitable for Day 1-2 forecasting\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ ARIMA/SARIMA: Limited effectiveness\")\n",
    "        \n",
    "        if weather_day1 > 0.3:\n",
    "            print(f\"    ‚Ä¢ Linear/Ridge Regression: Good for weather-driven forecasting\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ Linear/Ridge Regression: Limited by weak weather signals\")\n",
    "        \n",
    "        print(f\"\\\\n  MACHINE LEARNING MODELS:\")\n",
    "        if day1_score > 0.35:\n",
    "            print(f\"    ‚Ä¢ Random Forest: Excellent choice - handles feature interactions\")\n",
    "            print(f\"    ‚Ä¢ XGBoost: Recommended - good for time series with multiple features\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ Random Forest/XGBoost: May struggle with weak temporal signals\")\n",
    "        \n",
    "        if day2_score > 0.25:\n",
    "            print(f\"    ‚Ä¢ SVR with RBF kernel: Good for non-linear weather-PM relationships\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ SVR: Limited effectiveness beyond Day 1\")\n",
    "        \n",
    "        print(f\"\\\\n  DEEP LEARNING MODELS:\")\n",
    "        if day3_score > 0.2:\n",
    "            print(f\"    ‚Ä¢ LSTM/GRU: Recommended for capturing long-term dependencies\")\n",
    "            print(f\"    ‚Ä¢ Transformer models: Excellent for multi-horizon forecasting\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ LSTM/GRU: May overfit with weak signals\")\n",
    "        \n",
    "        if day1_score > 0.4 and day3_score > 0.15:\n",
    "            print(f\"    ‚Ä¢ CNN-LSTM hybrid: Good for spatial-temporal patterns\")\n",
    "        \n",
    "        print(f\"\\\\n  ENSEMBLE STRATEGIES:\")\n",
    "        if day3_score > 0.15:\n",
    "            print(f\"    ‚Ä¢ Multi-model ensemble: Combine statistical + ML + DL\")\n",
    "            print(f\"    ‚Ä¢ Horizon-specific models: Different models for each day\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ Focus ensemble on Day 1-2, simple persistence for Day 3\")\n",
    "        \n",
    "        # Feature engineering recommendations\n",
    "        print(f\"\\\\nüîß FEATURE ENGINEERING PRIORITIES:\")\n",
    "        \n",
    "        if lag_day1 > weather_day1:\n",
    "            print(f\"    ‚Ä¢ Priority: PM lag features (1-24h lags)\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ Priority: Weather features + short PM lags\")\n",
    "        \n",
    "        if weather_day2 > 0.25:\n",
    "            print(f\"    ‚Ä¢ Include: Weather forecast features for Day 2+\")\n",
    "        \n",
    "        if auto_day3 > 0.2:\n",
    "            print(f\"    ‚Ä¢ Include: Long-term lags (48-72h) for Day 3\")\n",
    "        else:\n",
    "            print(f\"    ‚Ä¢ Avoid: Long-term lags (limited value for Day 3)\")\n",
    "        \n",
    "        # Rolling features vs individual lags\n",
    "        if max(lag_day1, lag_day2) < 0.4:\n",
    "            print(f\"    ‚Ä¢ Consider: Rolling statistics instead of individual lags\")\n",
    "        \n",
    "        # External data recommendations\n",
    "        print(f\"\\\\nüåê EXTERNAL DATA INTEGRATION:\")\n",
    "        if weather_day3 < 0.2:\n",
    "            print(f\"    ‚Ä¢ High Priority: External weather forecast APIs\")\n",
    "            print(f\"    ‚Ä¢ Consider: Satellite air quality data\")\n",
    "        \n",
    "        if day3_score < 0.2:\n",
    "            print(f\"    ‚Ä¢ Recommended: Traffic data for rush hour predictions\")\n",
    "            print(f\"    ‚Ä¢ Consider: Industrial emission schedules\")\n",
    "\n",
    "# Overall project recommendations\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"PROJECT-LEVEL RECOMMENDATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\\\nüéØ IMPLEMENTATION STRATEGY:\")\n",
    "\n",
    "print(f\"\\\\n1. DEVELOPMENT PHASES:\")\n",
    "print(f\"   Phase 1: Focus on Day 1 forecasting (24h horizon)\")\n",
    "print(f\"           ‚Üí Build robust 24h models first\")\n",
    "print(f\"           ‚Üí Establish baseline performance\")\n",
    "print(f\"\\\\n   Phase 2: Extend to Day 2 forecasting (48h horizon)\")\n",
    "print(f\"           ‚Üí Add weather forecast integration\")\n",
    "print(f\"           ‚Üí Implement ensemble methods\")\n",
    "print(f\"\\\\n   Phase 3: Tackle Day 3 forecasting (72h horizon)\")\n",
    "print(f\"           ‚Üí Advanced ML/DL models\")\n",
    "print(f\"           ‚Üí External data integration\")\n",
    "\n",
    "print(f\"\\\\n2. MODEL DEVELOPMENT PIPELINE:\")\n",
    "print(f\"   ‚Ä¢ Start with simple baselines (persistence, linear regression)\")\n",
    "print(f\"   ‚Ä¢ Progress through complexity: Random Forest ‚Üí XGBoost ‚Üí LSTM\")\n",
    "print(f\"   ‚Ä¢ Implement ensemble combining best performers\")\n",
    "print(f\"   ‚Ä¢ Add external weather forecast APIs for Days 2-3\")\n",
    "\n",
    "print(f\"\\\\n3. EVALUATION FRAMEWORK:\")\n",
    "print(f\"   ‚Ä¢ Separate metrics for each forecast day\")\n",
    "print(f\"   ‚Ä¢ Focus on PM2.5/PM10 accuracy (not just AQI)\")\n",
    "print(f\"   ‚Ä¢ Test during different weather conditions\")\n",
    "print(f\"   ‚Ä¢ Validate AQI category accuracy (Good/Moderate/Unhealthy)\")\n",
    "\n",
    "print(f\"\\\\n4. DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(f\"   ‚Ä¢ Day 1: High-frequency updates (every hour)\")\n",
    "print(f\"   ‚Ä¢ Day 2-3: Lower update frequency (every 6-12 hours)\")\n",
    "print(f\"   ‚Ä¢ Confidence intervals for uncertainty quantification\")\n",
    "print(f\"   ‚Ä¢ Fallback to shorter horizons if accuracy drops\")\n",
    "\n",
    "print(f\"\\\\nüìä SUCCESS CRITERIA:\")\n",
    "print(f\"   ‚Ä¢ Day 1 (24h): RMSE < 15 ¬µg/m¬≥ for PM2.5, < 25 ¬µg/m¬≥ for PM10\")\n",
    "print(f\"   ‚Ä¢ Day 2 (48h): RMSE < 25 ¬µg/m¬≥ for PM2.5, < 40 ¬µg/m¬≥ for PM10\")\n",
    "print(f\"   ‚Ä¢ Day 3 (72h): RMSE < 35 ¬µg/m¬≥ for PM2.5, < 55 ¬µg/m¬≥ for PM10\")\n",
    "print(f\"   ‚Ä¢ AQI Category Accuracy: >85% for Day 1, >70% for Day 2, >60% for Day 3\")\n",
    "\n",
    "print(f\"\\\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Implement baseline models (persistence, linear regression)\")\n",
    "print(f\"   2. Develop Random Forest with optimized lag features\")\n",
    "print(f\"   3. Integrate external weather forecast APIs\")\n",
    "print(f\"   4. Build LSTM model for long-term dependencies\")\n",
    "print(f\"   5. Create ensemble combining best models\")\n",
    "print(f\"   6. Deploy multi-horizon forecasting system\")\n",
    "\n",
    "print(f\"\\\\nüéØ FINAL ASSESSMENT: 3-DAY PM2.5/PM10 FORECASTING IS FEASIBLE\")\n",
    "print(f\"   ‚úÖ Day 1: High accuracy achievable with proper feature engineering\")\n",
    "print(f\"   ‚úÖ Day 2: Moderate accuracy with weather integration\")\n",
    "print(f\"   ‚ö†Ô∏è  Day 3: Challenging but possible with advanced models + external data\")\n",
    "print(f\"   üéØ Recommendation: Proceed with phased implementation approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PM2.5 max value: {df['pm2_5'].max()}\")\n",
    "print(f\"PM10 max value: {df['pm10'].max()}\")\n",
    "print(f\"PM2.5 > 250: {df[df['pm2_5'] > 250].shape[0]} rows\")\n",
    "print(f\"PM10 > 425: {df[df['pm10'] > 425].shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
