{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA - Multan AQI Features\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Hopsworks Feature Store (multan_aqi_features)\n",
    "- **Records**: 538 observations \n",
    "- **Features**: 127 engineered features\n",
    "- **Time Range**: June 16, 2025 - July 8, 2025\n",
    "\n",
    "## Modeling Approach\n",
    "- **üéØ Goal**: Accurate US AQI prediction for Multan\n",
    "- **üîß Method**: Train ML model to predict PM2.5 & PM10 ‚Üí Calculate AQI via EPA formula\n",
    "- **üìä ML Targets**: pm2_5, pm10 concentrations (¬µg/m¬≥)\n",
    "- **‚úÖ Success Metric**: How well calculated AQI matches actual AQI values\n",
    "\n",
    "## Feature Categories\n",
    "1. **Raw Air Quality**: pm2_5, pm10, co, no2, so2, o3, nh3\n",
    "2. **AQI Calculations**: pm2_5_aqi, pm10_aqi, us_aqi, openweather_aqi\n",
    "3. **Weather Data**: temperature, humidity, pressure, wind_speed, wind_direction\n",
    "4. **Time Features**: Cyclical encodings (hour, day, month, etc.)\n",
    "5. **Lag Features**: 1h-72h historical values\n",
    "6. **Rolling Statistics**: 3h-24h windows (mean, std, min, max)\n",
    "7. **Engineered Features**: Interactions, squared terms, categorical flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Data EDA\n",
    "\n",
    "This notebook analyzes the engineered air quality and weather data from Hopsworks feature store that will be used for modeling.\n",
    "\n",
    "**EDA Focus**: Understanding relationships that help predict PM2.5 and PM10 concentrations accurately, which leads to better AQI predictions.\n",
    "\n",
    "## 1. Data Overview\n",
    "Loading and examining the basic structure of our modeling dataset from Hopsworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import configuration\n",
    "from config import HOPSWORKS_CONFIG\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks and load data\n",
    "print(\"Connecting to Hopsworks...\")\n",
    "project = hopsworks.login(api_key_value=HOPSWORKS_CONFIG[\"api_key\"], project=HOPSWORKS_CONFIG[\"project_name\"])\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "print(\"Loading feature group data...\")\n",
    "fg = fs.get_feature_group(HOPSWORKS_CONFIG[\"feature_group_name\"], version=1)\n",
    "df = fg.read()\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} records from Hopsworks\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column references and prepare data\n",
    "# The actual timestamp column is called 'time' not 'timestamp'\n",
    "print(\"Data preparation and column check...\")\n",
    "print(f\"Time column: {'time' if 'time' in df.columns else 'timestamp not found'}\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "\n",
    "# Ensure time is datetime\n",
    "if df['time'].dtype == 'object':\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Sort by time\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "print(\"‚úì Data sorted by time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "print()\n",
    "\n",
    "print(\"COLUMNS:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:2d}. {col:<25} {str(df[col].dtype):<15}\")\n",
    "\n",
    "print()\n",
    "print(\"FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'time' in numeric_cols:\n",
    "    numeric_cols.remove('time')\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Datetime features ({len(datetime_cols)}): {datetime_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last few records\n",
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 records:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\nRandom 5 records:\")\n",
    "display(df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric features\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nAIR QUALITY FEATURES SUMMARY:\")\n",
    "aqi_features = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "aqi_present = [col for col in aqi_features if col in df.columns]\n",
    "if aqi_present:\n",
    "    display(df[aqi_present].describe())\n",
    "\n",
    "print(\"\\nWEATHER FEATURES SUMMARY:\")\n",
    "weather_features = ['temperature', 'feels_like', 'humidity', 'pressure', 'visibility', 'wind_speed', 'wind_direction', 'cloud_cover']\n",
    "weather_present = [col for col in weather_features if col in df.columns]\n",
    "if weather_present:\n",
    "    display(df[weather_present].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis\n",
    "\n",
    "Analyzing temporal patterns in PM concentrations (our prediction targets) and derived AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plots for key air quality metrics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# PM2.5 over time\n",
    "axes[0].plot(df['time'], df['pm2_5'], alpha=0.7, color='red')\n",
    "axes[0].set_title('PM2.5 Concentration Over Time')\n",
    "axes[0].set_ylabel('PM2.5 (¬µg/m¬≥)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PM10 over time  \n",
    "axes[1].plot(df['time'], df['pm10'], alpha=0.7, color='orange')\n",
    "axes[1].set_title('PM10 Concentration Over Time')\n",
    "axes[1].set_ylabel('PM10 (¬µg/m¬≥)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# US AQI over time\n",
    "axes[2].plot(df['time'], df['us_aqi'], alpha=0.7, color='purple')\n",
    "axes[2].set_title('US AQI Over Time')\n",
    "axes[2].set_ylabel('US AQI')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI category colors as background\n",
    "aqi_levels = [\n",
    "    (0, 50, 'green', 'Good'),\n",
    "    (51, 100, 'yellow', 'Moderate'), \n",
    "    (101, 150, 'orange', 'Unhealthy for Sensitive'),\n",
    "    (151, 200, 'red', 'Unhealthy'),\n",
    "    (201, 300, 'purple', 'Very Unhealthy'),\n",
    "    (301, 500, 'maroon', 'Hazardous')\n",
    "]\n",
    "\n",
    "for min_val, max_val, color, label in aqi_levels:\n",
    "    axes[2].axhspan(min_val, max_val, alpha=0.1, color=color, label=label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series summary - ML TARGETS and GOAL METRIC\n",
    "print(f\"Time Series Summary (ML Targets + Goal Metric):\")\n",
    "print(f\"PM2.5 (ML Target): {df['pm2_5'].min():.1f} - {df['pm2_5'].max():.1f} ¬µg/m¬≥\")\n",
    "print(f\"PM10 (ML Target):  {df['pm10'].min():.1f} - {df['pm10'].max():.1f} ¬µg/m¬≥\") \n",
    "print(f\"US AQI (Goal Metric): {df['us_aqi'].min():.1f} - {df['us_aqi'].max():.1f}\")\n",
    "print(f\"\\nModeling Approach: Predict PM concentrations ‚Üí Calculate AQI ‚Üí Evaluate AQI accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis and Correlations\n",
    "\n",
    "Examining relationships between air quality and weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis - ML targets + environmental predictors\n",
    "key_features = [\n",
    "    'pm2_5', 'pm10',  # ML targets\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Weather predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Pollutant predictors\n",
    "]\n",
    "# Note: Excluding us_aqi since it's derived from PM targets\n",
    "\n",
    "# Filter features that exist in our dataset\n",
    "available_features = [col for col in key_features if col in df.columns]\n",
    "print(f\"Analyzing correlations for {len(available_features)} key features:\")\n",
    "print(available_features)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[available_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix (Key Variables)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Focus on ML TARGETS (PM concentrations) - what we need to predict well\n",
    "print(\"\\nStrongest correlations with PM2.5 (ML Target):\")\n",
    "pm25_corr = corr_matrix['pm2_5'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm25_corr.head(8).items():\n",
    "    if feature != 'pm2_5':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nStrongest correlations with PM10 (ML Target):\")\n",
    "pm10_corr = corr_matrix['pm10'].abs().sort_values(ascending=False)\n",
    "for feature, corr in pm10_corr.head(8).items():\n",
    "    if feature != 'pm10':\n",
    "        print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ CORRELATION FOCUS:\")\n",
    "print(\"Understanding which environmental factors help predict PM concentrations accurately\")\n",
    "print(\"Better PM predictions ‚Üí More accurate AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on ML TARGETS (PM concentrations) - what affects our predictions\n",
    "print(\"=\"*60)\n",
    "print(\"ML TARGET CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_features = ['pm2_5', 'pm10']\n",
    "predictor_features = ['temperature', 'humidity', 'pressure', 'wind_speed', \n",
    "                     'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "\n",
    "available_predictors = [col for col in predictor_features if col in df.columns]\n",
    "\n",
    "for target in target_features:\n",
    "    if target in df.columns:\n",
    "        print(f\"\\nStrongest correlations with {target.upper()} (Target Variable):\")\n",
    "        target_corr = df[[target] + available_predictors].corr()[target].abs().sort_values(ascending=False)\n",
    "        for feature, corr in target_corr.head(6).items():\n",
    "            if feature != target:\n",
    "                print(f\"  {feature:<20}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\n[EDA Focus: Understanding what predicts PM concentrations well]\")\n",
    "print(f\"[Goal: Better PM predictions ‚Üí More accurate AQI calculations]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AQI Dominance Analysis\n",
    "\n",
    "**Key Question**: Which pollutants actually drive AQI values? \n",
    "\n",
    "Since EPA AQI = MAX(individual pollutant AQIs), we need to check if other criteria pollutants sometimes create higher AQI than PM2.5/PM10. This validates our modeling approach of using only PM concentrations.\n",
    "\n",
    "**EPA Criteria Pollutants Analyzed**: PM2.5, PM10, O3, CO, NO2, SO2  \n",
    "*(Only these 6 pollutants have official EPA AQI breakpoints and affect AQI calculations)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPA AQI Calculation Functions for All Pollutants\n",
    "def calculate_aqi_from_concentration(concentration, breakpoints):\n",
    "    \"\"\"Calculate AQI from pollutant concentration using EPA breakpoints\"\"\"\n",
    "    if pd.isna(concentration) or concentration < 0:\n",
    "        return 0\n",
    "    \n",
    "    for i, (c_low, c_high, aqi_low, aqi_high) in enumerate(breakpoints):\n",
    "        if c_low <= concentration <= c_high:\n",
    "            # Linear interpolation within the bracket\n",
    "            aqi = ((aqi_high - aqi_low) / (c_high - c_low)) * (concentration - c_low) + aqi_low\n",
    "            return round(aqi)\n",
    "    \n",
    "    # If concentration exceeds highest breakpoint, use hazardous level\n",
    "    return 500\n",
    "\n",
    "# EPA AQI Breakpoints (concentration ranges and corresponding AQI ranges)\n",
    "EPA_BREAKPOINTS = {\n",
    "    'pm2_5': [  # ¬µg/m¬≥, 24-hour average\n",
    "        (0.0, 12.0, 0, 50),\n",
    "        (12.1, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 500.4, 301, 500)\n",
    "    ],\n",
    "    'pm10': [  # ¬µg/m¬≥, 24-hour average  \n",
    "        (0, 54, 0, 50),\n",
    "        (55, 154, 51, 100),\n",
    "        (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200),\n",
    "        (355, 424, 201, 300),\n",
    "        (425, 604, 301, 500)\n",
    "    ],\n",
    "    'ozone': [  # ppb, 8-hour average (converting from ¬µg/m¬≥ if needed)\n",
    "        (0, 54, 0, 50),\n",
    "        (55, 70, 51, 100),\n",
    "        (71, 85, 101, 150),\n",
    "        (86, 105, 151, 200),\n",
    "        (106, 200, 201, 300)\n",
    "    ],\n",
    "    'carbon_monoxide': [  # ppm, 8-hour average\n",
    "        (0.0, 4.4, 0, 50),\n",
    "        (4.5, 9.4, 51, 100),\n",
    "        (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200),\n",
    "        (15.5, 30.4, 201, 300),\n",
    "        (30.5, 50.4, 301, 500)\n",
    "    ],\n",
    "    'nitrogen_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 53, 0, 50),\n",
    "        (54, 100, 51, 100),\n",
    "        (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200),\n",
    "        (650, 1249, 201, 300),\n",
    "        (1250, 2049, 301, 500)\n",
    "    ],\n",
    "    'sulphur_dioxide': [  # ppb, 1-hour average\n",
    "        (0, 35, 0, 50),\n",
    "        (36, 75, 51, 100),\n",
    "        (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200),\n",
    "        (305, 604, 201, 300),\n",
    "        (605, 1004, 301, 500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"EPA AQI Calculation Functions Loaded\")\n",
    "print(f\"EPA Criteria Pollutants (official breakpoints): {list(EPA_BREAKPOINTS.keys())}\")\n",
    "\n",
    "# Check which EPA criteria pollutants we have in our data\n",
    "available_pollutants = []\n",
    "for pollutant in EPA_BREAKPOINTS.keys():\n",
    "    if pollutant in df.columns:\n",
    "        available_pollutants.append(pollutant)\n",
    "        \n",
    "print(f\"\\nEPA criteria pollutants in our dataset: {available_pollutants}\")\n",
    "print(f\"Missing from dataset: {[p for p in EPA_BREAKPOINTS.keys() if p not in df.columns]}\")\n",
    "print(f\"\\nAnalyzing {len(available_pollutants)} pollutants that can affect AQI calculations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual AQI for each available pollutant\n",
    "aqi_results = df[['time', 'us_aqi']].copy()\n",
    "\n",
    "# Unit conversions (if needed)\n",
    "df_calc = df.copy()\n",
    "\n",
    "# Convert units for certain pollutants if needed\n",
    "# Ozone: ¬µg/m¬≥ to ppb (approximate: ¬µg/m¬≥ * 0.5 ‚âà ppb at standard conditions)\n",
    "if 'ozone' in df_calc.columns:\n",
    "    df_calc['ozone_ppb'] = df_calc['ozone'] * 0.5  # Rough conversion\n",
    "    \n",
    "# CO might need conversion from ¬µg/m¬≥ to ppm\n",
    "if 'carbon_monoxide' in df_calc.columns:\n",
    "    df_calc['co_ppm'] = df_calc['carbon_monoxide'] * 0.000873  # Rough conversion\n",
    "\n",
    "print(\"Calculating individual AQI for each pollutant...\")\n",
    "\n",
    "# Calculate AQI for each pollutant\n",
    "for pollutant in available_pollutants:\n",
    "    col_name = f'{pollutant}_individual_aqi'\n",
    "    \n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        concentrations = df_calc['ozone_ppb']\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        concentrations = df_calc['co_ppm']\n",
    "    else:\n",
    "        concentrations = df_calc[pollutant]\n",
    "    \n",
    "    aqi_results[col_name] = concentrations.apply(\n",
    "        lambda x: calculate_aqi_from_concentration(x, EPA_BREAKPOINTS[pollutant])\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì {pollutant}: {aqi_results[col_name].min():.0f} - {aqi_results[col_name].max():.0f} AQI\")\n",
    "\n",
    "# Find controlling pollutant (max AQI) for each timestamp\n",
    "individual_aqi_cols = [col for col in aqi_results.columns if 'individual_aqi' in col]\n",
    "aqi_results['calculated_max_aqi'] = aqi_results[individual_aqi_cols].max(axis=1)\n",
    "aqi_results['controlling_pollutant'] = aqi_results[individual_aqi_cols].idxmax(axis=1)\n",
    "\n",
    "# Clean up pollutant names\n",
    "aqi_results['controlling_pollutant'] = aqi_results['controlling_pollutant'].str.replace('_individual_aqi', '')\n",
    "\n",
    "print(f\"\\nCalculated AQI range: {aqi_results['calculated_max_aqi'].min():.0f} - {aqi_results['calculated_max_aqi'].max():.0f}\")\n",
    "print(f\"Current US AQI range: {aqi_results['us_aqi'].min():.0f} - {aqi_results['us_aqi'].max():.0f}\")\n",
    "print(f\"Difference: {(aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']).describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 1: Pollutant Dominance Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"POLLUTANT DOMINANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count which pollutant controls AQI most often\n",
    "dominance_counts = aqi_results['controlling_pollutant'].value_counts()\n",
    "dominance_pct = (dominance_counts / len(aqi_results) * 100).round(1)\n",
    "\n",
    "print(\"Controlling Pollutant Frequency:\")\n",
    "for pollutant, count in dominance_counts.items():\n",
    "    pct = dominance_pct[pollutant]\n",
    "    print(f\"  {pollutant:<15}: {count:3d} times ({pct:5.1f}%)\")\n",
    "\n",
    "# Pie chart of dominance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(dominance_counts.values, labels=dominance_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Which Pollutant Controls AQI?')\n",
    "\n",
    "# Bar chart for clearer comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(range(len(dominance_counts)), dominance_counts.values, \n",
    "               color=['red' if x in ['pm2_5', 'pm10'] else 'lightcoral' for x in dominance_counts.index])\n",
    "plt.xticks(range(len(dominance_counts)), [p.replace('_', ' ').title() for p in dominance_counts.index], rotation=45)\n",
    "plt.ylabel('Number of Hours')\n",
    "plt.title('Controlling Pollutant Frequency')\n",
    "\n",
    "# Highlight PM2.5 and PM10\n",
    "for i, (pollutant, count) in enumerate(dominance_counts.items()):\n",
    "    color = 'white' if pollutant in ['pm2_5', 'pm10'] else 'black'\n",
    "    plt.text(i, count + 5, f'{count}', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of PM dominance\n",
    "pm_dominance = dominance_counts.get('pm2_5', 0) + dominance_counts.get('pm10', 0)\n",
    "pm_percentage = (pm_dominance / len(aqi_results) * 100)\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDING:\")\n",
    "print(f\"PM2.5 + PM10 control AQI {pm_dominance}/{len(aqi_results)} times ({pm_percentage:.1f}%)\")\n",
    "print(f\"Other pollutants control AQI {len(aqi_results) - pm_dominance}/{len(aqi_results)} times ({100-pm_percentage:.1f}%)\")\n",
    "\n",
    "if pm_percentage >= 80:\n",
    "    print(\"‚úÖ Current PM-only approach captures most AQI variations\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Consider including other pollutants in modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 2: Concentration vs AQI Relationships\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCENTRATION vs AQI ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create scatter plots for EPA criteria pollutants only\n",
    "num_plots = len(available_pollutants)\n",
    "cols = 3\n",
    "rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "if rows == 1:\n",
    "    axes = [axes]  # Make it iterable\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get concentrations and individual AQI\n",
    "    if pollutant == 'ozone' and 'ozone_ppb' in df_calc.columns:\n",
    "        conc = df_calc['ozone_ppb']\n",
    "        unit = 'ppb'\n",
    "    elif pollutant == 'carbon_monoxide' and 'co_ppm' in df_calc.columns:\n",
    "        conc = df_calc['co_ppm']\n",
    "        unit = 'ppm'\n",
    "    else:\n",
    "        conc = df_calc[pollutant]\n",
    "        unit = '¬µg/m¬≥'\n",
    "    \n",
    "    # All pollutants here are EPA criteria pollutants with AQI calculations\n",
    "    individual_aqi = aqi_results[f'{pollutant}_individual_aqi']\n",
    "    \n",
    "    # Scatter plot with AQI color coding\n",
    "    scatter = ax.scatter(conc, individual_aqi, alpha=0.6, s=20, \n",
    "                        c=individual_aqi, cmap='RdYlGn_r', vmin=0, vmax=150)\n",
    "    \n",
    "    ax.set_xlabel(f'{pollutant.replace(\"_\", \" \").title()} ({unit})')\n",
    "    ax.set_ylabel('Individual AQI')\n",
    "    ax.set_title(f'{pollutant.replace(\"_\", \" \").title()} ‚Üí AQI')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AQI level lines\n",
    "    ax.axhline(y=50, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=100, color='yellow', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(y=150, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(available_pollutants), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for EPA criteria pollutants\n",
    "print(\"\\nIndividual AQI Statistics (EPA Criteria Pollutants):\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    stats = aqi_results[aqi_col].describe()\n",
    "    print(f\"{pollutant:<15}: Mean={stats['mean']:5.1f}, Max={stats['max']:5.1f}, >100: {(aqi_results[aqi_col] > 100).sum():3d} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3A: Individual AQI Contributions vs Maximum AQI\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL POLLUTANT AQI CONTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Time series showing all individual AQIs vs the maximum\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All individual AQIs over time\n",
    "time_vals = aqi_results['time']\n",
    "max_aqi_vals = aqi_results['calculated_max_aqi']\n",
    "\n",
    "# Plot each individual AQI\n",
    "colors = ['red', 'darkred', 'blue', 'purple', 'orange', 'green']\n",
    "for i, pollutant in enumerate(available_pollutants):\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        ax1.plot(time_vals, aqi_results[aqi_col], \n",
    "                label=pollutant.replace('_', ' ').title(), \n",
    "                alpha=0.7, linewidth=1.5, color=colors[i % len(colors)])\n",
    "\n",
    "# Plot maximum AQI as thick black line\n",
    "ax1.plot(time_vals, max_aqi_vals, \n",
    "         label='Maximum AQI (Envelope)', \n",
    "         color='black', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('Individual Pollutant AQIs vs Maximum AQI Over Time')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Distribution of individual AQIs \n",
    "aqi_data = []\n",
    "pollutant_names = []\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        aqi_data.append(aqi_results[aqi_col].values)\n",
    "        pollutant_names.append(pollutant.replace('_', ' ').title())\n",
    "\n",
    "# Add maximum AQI for comparison\n",
    "aqi_data.append(max_aqi_vals.values)\n",
    "pollutant_names.append('Maximum AQI')\n",
    "\n",
    "# Create box plot\n",
    "bp = ax2.boxplot(aqi_data, labels=pollutant_names, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "box_colors = colors + ['black']\n",
    "for patch, color in zip(bp['boxes'], box_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('AQI Value')\n",
    "ax2.set_title('AQI Distribution by Pollutant')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level lines\n",
    "for level, color in [(50, 'green'), (100, 'yellow'), (150, 'orange')]:\n",
    "    ax2.axhline(y=level, color=color, linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics for contribution analysis\n",
    "print(\"\\nIndividual AQI vs Maximum AQI Analysis:\")\n",
    "max_aqi_mean = max_aqi_vals.mean()\n",
    "print(f\"Maximum AQI - Mean: {max_aqi_mean:.1f}, Range: {max_aqi_vals.min():.0f}-{max_aqi_vals.max():.0f}\")\n",
    "\n",
    "print(\"\\nHow often each pollutant reaches within 90% of maximum AQI:\")\n",
    "for pollutant in available_pollutants:\n",
    "    aqi_col = f'{pollutant}_individual_aqi'\n",
    "    if aqi_col in aqi_results.columns:\n",
    "        close_to_max = (aqi_results[aqi_col] >= 0.9 * max_aqi_vals).sum()\n",
    "        percentage = (close_to_max / len(aqi_results) * 100)\n",
    "        print(f\"  {pollutant:<15}: {close_to_max:3d}/{len(aqi_results)} times ({percentage:5.1f}%) within 90% of max\")\n",
    "\n",
    "print(\"\\nThis shows which pollutants are the 'runners-up' when they don't control AQI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION 3: Time Series of Controlling Pollutants\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIME SERIES OF AQI CONTROL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a time series showing which pollutant controls AQI\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: AQI comparison over time\n",
    "ax1.plot(aqi_results['time'], aqi_results['us_aqi'], label='Current US AQI (PM only)', alpha=0.7, linewidth=2)\n",
    "ax1.plot(aqi_results['time'], aqi_results['calculated_max_aqi'], label='True Max AQI (All pollutants)', alpha=0.7, linewidth=2)\n",
    "ax1.fill_between(aqi_results['time'], aqi_results['us_aqi'], aqi_results['calculated_max_aqi'], \n",
    "                 alpha=0.3, color='red', label='Missing AQI')\n",
    "\n",
    "ax1.set_ylabel('AQI Value')\n",
    "ax1.set_title('AQI Comparison: Current PM-only vs Full EPA Calculation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add AQI level backgrounds\n",
    "aqi_levels = [(0, 50, 'green'), (50, 100, 'yellow'), (100, 150, 'orange'), (150, 200, 'red')]\n",
    "for min_val, max_val, color in aqi_levels:\n",
    "    ax1.axhspan(min_val, max_val, alpha=0.1, color=color)\n",
    "\n",
    "# Plot 2: Controlling pollutant over time\n",
    "pollutant_colors = {\n",
    "    'pm2_5': 'red', 'pm10': 'darkred', 'ozone': 'blue', \n",
    "    'carbon_monoxide': 'purple', 'nitrogen_dioxide': 'orange', 'sulphur_dioxide': 'green'\n",
    "}\n",
    "\n",
    "# Create numerical encoding for pollutants for plotting\n",
    "unique_pollutants = aqi_results['controlling_pollutant'].unique()\n",
    "pollutant_mapping = {p: i for i, p in enumerate(unique_pollutants)}\n",
    "aqi_results['pollutant_num'] = aqi_results['controlling_pollutant'].map(pollutant_mapping)\n",
    "\n",
    "scatter = ax2.scatter(aqi_results['time'], aqi_results['pollutant_num'], \n",
    "                     c=[pollutant_colors.get(p, 'gray') for p in aqi_results['controlling_pollutant']], \n",
    "                     alpha=0.7, s=20)\n",
    "\n",
    "ax2.set_ylabel('Controlling Pollutant')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Which Pollutant Controls AQI Over Time')\n",
    "ax2.set_yticks(range(len(unique_pollutants)))\n",
    "ax2.set_yticklabels([p.replace('_', ' ').title() for p in unique_pollutants])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate impact of missing other pollutants\n",
    "aqi_difference = aqi_results['calculated_max_aqi'] - aqi_results['us_aqi']\n",
    "significant_underestimation = (aqi_difference > 10).sum()\n",
    "\n",
    "print(f\"\\nüìä IMPACT ANALYSIS:\")\n",
    "print(f\"Times when PM-only AQI underestimates by >10 points: {significant_underestimation}/{len(aqi_results)} ({significant_underestimation/len(aqi_results)*100:.1f}%)\")\n",
    "print(f\"Maximum underestimation: {aqi_difference.max():.1f} AQI points\")\n",
    "print(f\"Average difference: {aqi_difference.mean():.1f} AQI points\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nüéØ MODELING RECOMMENDATION:\")\n",
    "if pm_percentage >= 85:\n",
    "    print(\"‚úÖ PM-only approach is SUFFICIENT for Multan AQI prediction\")\n",
    "    print(\"   PM2.5 + PM10 control >85% of AQI variations\")\n",
    "elif pm_percentage >= 70:\n",
    "    print(\"‚ö†Ô∏è  PM-only approach is MOSTLY adequate but consider monitoring other pollutants\")\n",
    "    print(\"   PM2.5 + PM10 control 70-85% of AQI variations\")\n",
    "else:\n",
    "    print(\"üö® PM-only approach MISSES significant AQI drivers\")\n",
    "    print(\"   Consider including other pollutants in prediction model\")\n",
    "\n",
    "print(f\"\\nCurrent focus on PM2.5 and PM10 captures {pm_percentage:.1f}% of AQI control instances.\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL ANALYSIS:\")\n",
    "print(f\"‚úì Analyzed all {len(available_pollutants)} EPA criteria pollutants in dataset\")\n",
    "print(f\"‚úì PM2.5 and PM10 are responsible for {pm_percentage:.1f}% of AQI determinations\")\n",
    "print(f\"‚úì Other criteria pollutants (O3, CO, NO2, SO2) control {100-pm_percentage:.1f}% of AQI\")\n",
    "print(f\"‚úì This validates the scope of your PM-focused modeling approach\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lag Features Analysis\n",
    "\n",
    "Examining the importance of historical PM values for predicting current concentrations (which leads to better AQI calculations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features for BOTH ML targets (PM2.5 AND PM10)\n",
    "print(\"=\"*60)\n",
    "print(\"LAG FEATURES ANALYSIS FOR ML TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM10 Lag Analysis (since it's also a target)\n",
    "pm10_lag_features = [col for col in df.columns if 'lag' in col and 'pm10' in col]\n",
    "if pm10_lag_features:\n",
    "    print(f\"\\nFound {len(pm10_lag_features)} PM10 lag features:\")\n",
    "    print(pm10_lag_features[:5], \"...\" if len(pm10_lag_features) > 5 else \"\")\n",
    "    \n",
    "    # Calculate correlations between current PM10 and its lag features\n",
    "    pm10_lag_correlations = df[['pm10'] + pm10_lag_features].corr()['pm10'].drop('pm10')\n",
    "    \n",
    "    # Plot PM10 lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]\n",
    "    pm10_correlations = [pm10_lag_correlations[f'pm10_lag_{h}h'] for h in lag_hours if f'pm10_lag_{h}h' in pm10_lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(pm10_correlations)), pm10_correlations, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM10')\n",
    "    plt.title('PM10 Lag Features Correlation (ML Target)')\n",
    "    plt.xticks(range(len(pm10_correlations)), [f'{h}h' for h in lag_hours[:len(pm10_correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(pm10_correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPM10 lag feature correlations (ML Target):\")\n",
    "    for i, h in enumerate(lag_hours[:len(pm10_correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {pm10_correlations[i]:.3f}\")\n",
    "\n",
    "print(\"\\n[ML Targets: PM2.5 & PM10 concentrations]\")\n",
    "print(\"[Ultimate Goal: Accurate AQI predictions for Multan]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lag features correlation with current PM2.5\n",
    "lag_features = [col for col in df.columns if 'lag' in col and 'pm2_5' in col]\n",
    "print(f\"Found {len(lag_features)} PM2.5 lag features:\")\n",
    "print(lag_features)\n",
    "\n",
    "if lag_features:\n",
    "    # Calculate correlations between current PM2.5 and its lag features\n",
    "    lag_correlations = df[['pm2_5'] + lag_features].corr()['pm2_5'].drop('pm2_5')\n",
    "    \n",
    "    # Plot lag correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    lag_hours = [1, 2, 3, 6, 12, 24, 48, 72]  # Expected lag hours\n",
    "    correlations = [lag_correlations[f'pm2_5_lag_{h}h'] for h in lag_hours if f'pm2_5_lag_{h}h' in lag_correlations.index]\n",
    "    \n",
    "    plt.bar(range(len(correlations)), correlations, alpha=0.7)\n",
    "    plt.xlabel('Lag Hours')\n",
    "    plt.ylabel('Correlation with Current PM2.5')\n",
    "    plt.title('PM2.5 Lag Features Correlation')\n",
    "    plt.xticks(range(len(correlations)), [f'{h}h' for h in lag_hours[:len(correlations)]])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, corr in enumerate(correlations):\n",
    "        plt.text(i, corr + 0.01, f'{corr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLag feature correlations with current PM2.5:\")\n",
    "    for i, h in enumerate(lag_hours[:len(correlations)]):\n",
    "        print(f\"  {h:2d}h lag: {correlations[i]:.3f}\")\n",
    "\n",
    "# Complete Rolling Statistics Analysis for Both ML Targets\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ROLLING STATISTICS ANALYSIS (ML TARGETS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PM2.5 Rolling Features Analysis\n",
    "pm25_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm2_5' in col]\n",
    "if pm25_rolling_features:\n",
    "    print(f\"\\nPM2.5 ROLLING FEATURES ({len(pm25_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    rolling_types = ['mean', 'std', 'min', 'max']\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm25_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm2_5'] + stat_features].corr()['pm2_5'].drop('pm2_5')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# PM10 Rolling Features Analysis  \n",
    "pm10_rolling_features = [col for col in df.columns if 'rolling' in col and 'pm10' in col]\n",
    "if pm10_rolling_features:\n",
    "    print(f\"\\nPM10 ROLLING FEATURES ({len(pm10_rolling_features)} total):\")\n",
    "    \n",
    "    # Group by statistic type\n",
    "    for stat_type in rolling_types:\n",
    "        stat_features = [col for col in pm10_rolling_features if stat_type in col]\n",
    "        if stat_features:\n",
    "            print(f\"\\n  {stat_type.upper()} features:\")\n",
    "            correlations = df[['pm10'] + stat_features].corr()['pm10'].drop('pm10')\n",
    "            for feature, corr in correlations.sort_values(ascending=False).items():\n",
    "                window = feature.split('_')[-1]\n",
    "                print(f\"    {window:<4} window: {corr:.3f}\")\n",
    "\n",
    "# Change Rate Features Analysis\n",
    "print(f\"\\nCHANGE RATE FEATURES:\")\n",
    "change_features = [col for col in df.columns if 'change_rate' in col]\n",
    "for target in ['pm2_5', 'pm10']:\n",
    "    target_change_features = [col for col in change_features if target in col]\n",
    "    if target_change_features:\n",
    "        print(f\"\\n  {target.upper()} change rates:\")\n",
    "        correlations = df[[target] + target_change_features].corr()[target].drop(target)\n",
    "        for feature, corr in correlations.items():\n",
    "            period = feature.split('_')[-1]\n",
    "            print(f\"    {period:<4} change: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Rolling features capture trend information over different time windows\")\n",
    "print(f\"‚Ä¢ Shorter windows (3h, 6h) typically correlate more strongly with current values\")\n",
    "print(f\"‚Ä¢ Change rates show how much PM concentrations are shifting\")\n",
    "print(f\"‚Ä¢ These features help models understand pollution persistence and trends\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Analysis\n",
    "\n",
    "**Focus**: Ensuring data reliability for accurate PM concentration predictions and AQI calculations.\n",
    "\n",
    "### 6.1 Missing Values & Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Missing Values & Null Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES & NULL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic missing value check\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_percent\n",
    "}).round(2)\n",
    "\n",
    "print(\"EXPLICIT NULL VALUES:\")\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"‚úì No explicit null values found!\")\n",
    "else:\n",
    "    print(\"Missing values found:\")\n",
    "    for col in missing_df[missing_df['Missing_Count'] > 0].index:\n",
    "        count = missing_df.loc[col, 'Missing_Count']\n",
    "        percent = missing_df.loc[col, 'Missing_Percentage']\n",
    "        print(f\"  {col:<25} {count:>6} ({percent:>6.2f}%)\")\n",
    "\n",
    "# Check for zero values that might represent missing data\n",
    "print(f\"\\nZERO VALUES ANALYSIS (Potential Missing Data):\")\n",
    "air_quality_cols = ['pm2_5', 'pm10', 'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide']\n",
    "zero_issues = {}\n",
    "\n",
    "for col in air_quality_cols:\n",
    "    if col in df.columns:\n",
    "        zero_count = (df[col] == 0.0).sum()\n",
    "        zero_percent = (zero_count / len(df)) * 100\n",
    "        if zero_count > 0:\n",
    "            zero_issues[col] = {'count': zero_count, 'percent': zero_percent}\n",
    "\n",
    "if zero_issues:\n",
    "    print(\"‚ö†Ô∏è  ZERO VALUES FOUND (may indicate missing sensors):\")\n",
    "    for col, stats in zero_issues.items():\n",
    "        print(f\"  {col:<20}: {stats['count']:>3d} zeros ({stats['percent']:>5.1f}%)\")\n",
    "        \n",
    "    # Visualize zero patterns over time\n",
    "    fig, axes = plt.subplots(len(zero_issues), 1, figsize=(15, 3*len(zero_issues)))\n",
    "    if len(zero_issues) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (col, stats) in enumerate(zero_issues.items()):\n",
    "        zero_mask = df[col] == 0.0\n",
    "        axes[i].scatter(df[zero_mask]['time'], [col]*zero_mask.sum(), \n",
    "                       alpha=0.7, color='red', s=20, label=f'Zero values ({stats[\"count\"]})')\n",
    "        axes[i].set_ylabel(col.replace('_', ' ').title())\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} - Zero Value Timeline')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úì No zero values in air quality parameters\")\n",
    "\n",
    "# Check for consecutive missing periods (data gaps)\n",
    "print(f\"\\nTIME GAP ANALYSIS:\")\n",
    "df_sorted = df.sort_values('time').reset_index(drop=True)\n",
    "time_diffs = df_sorted['time'].diff()\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]\n",
    "\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(large_gaps)} time gaps > 2 hours:\")\n",
    "    for idx, gap in large_gaps.items():\n",
    "        gap_start = df_sorted.loc[idx-1, 'time'] if idx > 0 else 'Start'\n",
    "        gap_end = df_sorted.loc[idx, 'time']\n",
    "        print(f\"  Gap: {gap} between {gap_start} and {gap_end}\")\n",
    "else:\n",
    "    print(\"‚úì No significant time gaps found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Consistency & Physics Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Data Consistency & Physics Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA CONSISTENCY & PHYSICS VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PM2.5 vs PM10 relationship (PM2.5 should generally be ‚â§ PM10)\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = df[df['pm2_5'] > df['pm10']]\n",
    "    violation_count = len(pm_violations)\n",
    "    violation_percent = (violation_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"PM2.5 vs PM10 CONSISTENCY:\")\n",
    "    if violation_count > 0:\n",
    "        print(f\"‚ö†Ô∏è  {violation_count} records ({violation_percent:.1f}%) where PM2.5 > PM10\")\n",
    "        print(f\"   Max violation: PM2.5={pm_violations['pm2_5'].max():.1f} > PM10={pm_violations['pm10'].max():.1f}\")\n",
    "        \n",
    "        # Show violation timeline\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.scatter(df['time'], df['pm2_5'], alpha=0.5, label='PM2.5', s=10)\n",
    "        plt.scatter(df['time'], df['pm10'], alpha=0.5, label='PM10', s=10)\n",
    "        plt.scatter(pm_violations['time'], pm_violations['pm2_5'], \n",
    "                   color='red', label=f'PM2.5 > PM10 ({violation_count} cases)', s=30, marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (¬µg/m¬≥)')\n",
    "        plt.title('PM2.5 vs PM10 Consistency Check')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚úì PM2.5 ‚â§ PM10 in all records (physically consistent)\")\n",
    "\n",
    "# Check reasonable ranges for weather parameters\n",
    "print(f\"\\nWEATHER PARAMETER VALIDATION:\")\n",
    "weather_ranges = {\n",
    "    'temperature': (-50, 60, '¬∞C'),  # Extreme but possible range\n",
    "    'humidity': (0, 100, '%'),       # Should be 0-100%\n",
    "    'pressure': (800, 1200, 'hPa'),  # Reasonable atmospheric pressure\n",
    "    'wind_speed': (0, 50, 'm/s'),    # Reasonable wind speeds\n",
    "    'wind_direction': (0, 360, '¬∞')  # Should be 0-360 degrees\n",
    "}\n",
    "\n",
    "weather_violations = {}\n",
    "for param, (min_val, max_val, unit) in weather_ranges.items():\n",
    "    if param in df.columns:\n",
    "        below_min = (df[param] < min_val).sum()\n",
    "        above_max = (df[param] > max_val).sum()\n",
    "        violations = below_min + above_max\n",
    "        \n",
    "        if violations > 0:\n",
    "            weather_violations[param] = {\n",
    "                'below_min': below_min, 'above_max': above_max, \n",
    "                'min_val': min_val, 'max_val': max_val, 'unit': unit,\n",
    "                'actual_min': df[param].min(), 'actual_max': df[param].max()\n",
    "            }\n",
    "\n",
    "if weather_violations:\n",
    "    print(\"‚ö†Ô∏è  WEATHER PARAMETER VIOLATIONS:\")\n",
    "    for param, stats in weather_violations.items():\n",
    "        print(f\"  {param:<15}: {stats['below_min']} below {stats['min_val']}{stats['unit']}, \"\n",
    "              f\"{stats['above_max']} above {stats['max_val']}{stats['unit']}\")\n",
    "        print(f\"                   Actual range: {stats['actual_min']:.1f} - {stats['actual_max']:.1f}{stats['unit']}\")\n",
    "else:\n",
    "    print(\"‚úì All weather parameters within reasonable ranges\")\n",
    "\n",
    "# Check cyclical feature ranges (sin/cos should be in [-1, 1])\n",
    "print(f\"\\nCYCLICAL FEATURE VALIDATION:\")\n",
    "cyclical_cols = [col for col in df.columns if '_sin' in col or '_cos' in col]\n",
    "cyclical_violations = {}\n",
    "\n",
    "for col in cyclical_cols:\n",
    "    below_neg1 = (df[col] < -1.01).sum()  # Small tolerance for floating point\n",
    "    above_pos1 = (df[col] > 1.01).sum()\n",
    "    violations = below_neg1 + above_pos1\n",
    "    \n",
    "    if violations > 0:\n",
    "        cyclical_violations[col] = {\n",
    "            'below_neg1': below_neg1, 'above_pos1': above_pos1,\n",
    "            'actual_min': df[col].min(), 'actual_max': df[col].max()\n",
    "        }\n",
    "\n",
    "if cyclical_violations:\n",
    "    print(\"‚ö†Ô∏è  CYCLICAL FEATURE VIOLATIONS:\")\n",
    "    for col, stats in cyclical_violations.items():\n",
    "        print(f\"  {col:<20}: {stats['below_neg1']} below -1, {stats['above_pos1']} above 1\")\n",
    "        print(f\"                       Actual range: {stats['actual_min']:.6f} - {stats['actual_max']:.6f}\")\n",
    "else:\n",
    "    print(\"‚úì All cyclical features within [-1, 1] range\")\n",
    "\n",
    "# Check binary flag consistency\n",
    "print(f\"\\nBINARY FLAG VALIDATION:\")\n",
    "binary_cols = [col for col in df.columns if col.startswith('is_')]\n",
    "binary_violations = {}\n",
    "\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    expected_vals = {0.0, 1.0}\n",
    "    unexpected_vals = set(unique_vals) - expected_vals\n",
    "    \n",
    "    if unexpected_vals:\n",
    "        binary_violations[col] = {\n",
    "            'unexpected': list(unexpected_vals),\n",
    "            'unique_vals': list(unique_vals)\n",
    "        }\n",
    "\n",
    "if binary_violations:\n",
    "    print(\"‚ö†Ô∏è  BINARY FLAG VIOLATIONS:\")\n",
    "    for col, stats in binary_violations.items():\n",
    "        print(f\"  {col:<20}: Found {stats['unexpected']} (expected only 0.0, 1.0)\")\n",
    "        print(f\"                       All values: {stats['unique_vals']}\")\n",
    "else:\n",
    "    print(\"‚úì All binary flags contain only 0.0 and 1.0 values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Statistical Outlier Detection\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Focus on ML targets and key environmental features\n",
    "key_features_for_outliers = [\n",
    "    'pm2_5', 'pm10',  # ML targets - critical for model quality\n",
    "    'temperature', 'humidity', 'pressure', 'wind_speed',  # Environmental predictors\n",
    "    'carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide'  # Air quality predictors\n",
    "]\n",
    "\n",
    "available_outlier_features = [col for col in key_features_for_outliers if col in df.columns]\n",
    "\n",
    "# Z-score method (|z| > 3 considered outlier)\n",
    "print(\"Z-SCORE OUTLIER DETECTION (|z-score| > 3):\")\n",
    "zscore_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask = z_scores > 3\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        zscore_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'max_zscore': z_scores.max(),\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if zscore_outliers:\n",
    "    print(\"‚ö†Ô∏è  Z-SCORE OUTLIERS FOUND:\")\n",
    "    for col, stats in zscore_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%) - Max |z|: {stats['max_zscore']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            print(f\"                       Values: {sorted(stats['outlier_values'])[:5]}...\" if len(stats['outlier_values']) > 5 else f\"                       Values: {sorted(stats['outlier_values'])}\")\n",
    "else:\n",
    "    print(\"‚úì No z-score outliers found (|z| > 3)\")\n",
    "\n",
    "# IQR method (values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
    "print(f\"\\nIQR OUTLIER DETECTION (beyond Q1-1.5*IQR, Q3+1.5*IQR):\")\n",
    "iqr_outliers = {}\n",
    "\n",
    "for col in available_outlier_features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    outlier_count = outlier_mask.sum()\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        iqr_outliers[col] = {\n",
    "            'count': outlier_count,\n",
    "            'percent': (outlier_count / len(df)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_values': df.loc[outlier_mask, col].tolist()\n",
    "        }\n",
    "\n",
    "if iqr_outliers:\n",
    "    print(\"‚ö†Ô∏è  IQR OUTLIERS FOUND:\")\n",
    "    for col, stats in iqr_outliers.items():\n",
    "        print(f\"  {col:<20}: {stats['count']} outliers ({stats['percent']:.1f}%)\")\n",
    "        print(f\"                       Expected range: {stats['lower_bound']:.2f} - {stats['upper_bound']:.2f}\")\n",
    "        if col in ['pm2_5', 'pm10']:  # Show details for ML targets\n",
    "            extreme_values = sorted(stats['outlier_values'])\n",
    "            display_values = extreme_values[:3] + ['...'] + extreme_values[-2:] if len(extreme_values) > 5 else extreme_values\n",
    "            print(f\"                       Outlier values: {display_values}\")\n",
    "else:\n",
    "    print(\"‚úì No IQR outliers found\")\n",
    "\n",
    "# Visualize outliers for ML targets\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_targets_present = [col for col in ml_targets if col in df.columns]\n",
    "\n",
    "if ml_targets_present and (zscore_outliers or iqr_outliers):\n",
    "    print(f\"\\nOUTLIER VISUALIZATION FOR ML TARGETS:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(ml_targets_present), 2, figsize=(15, 5*len(ml_targets_present)))\n",
    "    if len(ml_targets_present) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, target in enumerate(ml_targets_present):\n",
    "        # Box plot\n",
    "        ax1 = axes[i, 0]\n",
    "        bp = ax1.boxplot([df[target].dropna()], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax1.set_title(f'{target.upper()} - Box Plot (IQR Outliers)')\n",
    "        ax1.set_ylabel(f'{target.replace(\"_\", \".\")} (¬µg/m¬≥)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series with outliers highlighted\n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(df['time'], df[target], alpha=0.7, linewidth=1, label=target.upper())\n",
    "        \n",
    "        # Highlight outliers\n",
    "        if target in zscore_outliers:\n",
    "            z_scores = np.abs((df[target] - df[target].mean()) / df[target].std())\n",
    "            zscore_mask = z_scores > 3\n",
    "            ax2.scatter(df[zscore_mask]['time'], df.loc[zscore_mask, target], \n",
    "                       color='red', s=30, marker='x', label=f'Z-score outliers ({zscore_outliers[target][\"count\"]})')\n",
    "        \n",
    "        if target in iqr_outliers:\n",
    "            Q1 = df[target].quantile(0.25)\n",
    "            Q3 = df[target].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_mask = (df[target] < lower_bound) | (df[target] > upper_bound)\n",
    "            ax2.scatter(df[iqr_mask]['time'], df.loc[iqr_mask, target], \n",
    "                       color='orange', s=20, marker='o', alpha=0.7, label=f'IQR outliers ({iqr_outliers[target][\"count\"]})')\n",
    "        \n",
    "        ax2.set_title(f'{target.upper()} - Time Series with Outliers')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(f'{target.replace(\"_\", \".\")} (¬µg/m¬≥)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary of outlier impact on ML targets\n",
    "print(f\"\\nüéØ OUTLIER IMPACT ON ML TARGETS:\")\n",
    "for target in ml_targets_present:\n",
    "    total_outliers = 0\n",
    "    if target in zscore_outliers:\n",
    "        total_outliers += zscore_outliers[target]['count']\n",
    "    if target in iqr_outliers:\n",
    "        total_outliers += iqr_outliers[target]['count']  # Note: may overlap with z-score\n",
    "    \n",
    "    if total_outliers > 0:\n",
    "        outlier_percent = (total_outliers / len(df)) * 100\n",
    "        print(f\"  {target.upper()}: ~{total_outliers} potential outliers ({outlier_percent:.1f}% of data)\")\n",
    "        print(f\"           Consider: Review for sensor errors vs genuine extreme pollution events\")\n",
    "    else:\n",
    "        print(f\"  {target.upper()}: No significant outliers detected\")\n",
    "\n",
    "print(f\"\\n[Quality Note: Outliers in PM concentrations could be genuine pollution spikes or sensor malfunctions]\")\n",
    "print(f\"[Recommendation: Investigate extreme values before removal - they might be real air quality events]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Data Completeness & Summary Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Data Completeness & Summary Quality Report\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA COMPLETENESS & SUMMARY QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"DUPLICATE RECORDS: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"‚ö†Ô∏è  Duplicate rows found - consider deduplication\")\n",
    "else:\n",
    "    print(\"‚úì No duplicate rows\")\n",
    "\n",
    "# Unique values analysis\n",
    "print(f\"\\nUNIQUE VALUES ANALYSIS:\")\n",
    "feature_categories = {\n",
    "    'ML Targets': ['pm2_5', 'pm10'],\n",
    "    'Air Quality': ['carbon_monoxide', 'nitrogen_dioxide', 'ozone', 'sulphur_dioxide', 'nh3'],\n",
    "    'AQI Calculations': ['pm2_5_aqi', 'pm10_aqi', 'us_aqi', 'openweather_aqi'],\n",
    "    'Weather': ['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction'],\n",
    "    'Time Features': ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos'],\n",
    "    'Binary Flags': [col for col in df.columns if col.startswith('is_')],\n",
    "    'Lag Features': [col for col in df.columns if 'lag' in col][:5],  # Show first 5\n",
    "    'Rolling Features': [col for col in df.columns if 'rolling' in col][:5]  # Show first 5\n",
    "}\n",
    "\n",
    "for category, cols in feature_categories.items():\n",
    "    available_cols = [col for col in cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for col in available_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            unique_ratio = unique_count / len(df)\n",
    "            if unique_ratio < 0.01:  # Very low uniqueness\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ‚ö†Ô∏è  Low variation\")\n",
    "            elif unique_ratio > 0.95:  # Very high uniqueness  \n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%}) ‚úì High variation\")\n",
    "            else:\n",
    "                print(f\"  {col:<25} {unique_count:>6} unique ({unique_ratio:>6.2%})\")\n",
    "\n",
    "# Data quality scoring\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL DATA QUALITY SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_score = 100  # Start with perfect score\n",
    "quality_issues = []\n",
    "\n",
    "# Deduct points for various issues\n",
    "if missing_data.sum() > 0:\n",
    "    missing_percent_total = (missing_data.sum() / (len(df) * len(df.columns))) * 100\n",
    "    deduction = min(20, missing_percent_total * 4)  # Up to 20 points for missing data\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Missing data: -{deduction:.1f} points ({missing_percent_total:.1f}% of all values)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    dup_percent = (duplicates / len(df)) * 100\n",
    "    deduction = min(10, dup_percent * 2)  # Up to 10 points for duplicates\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Duplicate rows: -{deduction:.1f} points ({dup_percent:.1f}% of records)\")\n",
    "\n",
    "if weather_violations:\n",
    "    deduction = len(weather_violations) * 2  # 2 points per violated weather parameter\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Weather violations: -{deduction} points ({len(weather_violations)} parameters)\")\n",
    "\n",
    "if cyclical_violations:\n",
    "    deduction = len(cyclical_violations) * 3  # 3 points per violated cyclical feature\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Cyclical violations: -{deduction} points ({len(cyclical_violations)} features)\")\n",
    "\n",
    "if binary_violations:\n",
    "    deduction = len(binary_violations) * 2  # 2 points per violated binary flag\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Binary flag violations: -{deduction} points ({len(binary_violations)} flags)\")\n",
    "\n",
    "# Check PM2.5 vs PM10 physics violations\n",
    "if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "    pm_violations = (df['pm2_5'] > df['pm10']).sum()\n",
    "    if pm_violations > 0:\n",
    "        violation_percent = (pm_violations / len(df)) * 100\n",
    "        deduction = min(15, violation_percent * 3)  # Up to 15 points for physics violations\n",
    "        quality_score -= deduction\n",
    "        quality_issues.append(f\"PM physics violations: -{deduction:.1f} points ({violation_percent:.1f}% of records)\")\n",
    "\n",
    "# Outlier penalty (mild)\n",
    "total_outlier_features = len(zscore_outliers) + len(iqr_outliers)\n",
    "if total_outlier_features > 0:\n",
    "    deduction = min(5, total_outlier_features * 0.5)  # Mild penalty for outliers\n",
    "    quality_score -= deduction\n",
    "    quality_issues.append(f\"Statistical outliers: -{deduction:.1f} points (in {total_outlier_features} features)\")\n",
    "\n",
    "# Ensure score doesn't go below 0\n",
    "quality_score = max(0, quality_score)\n",
    "\n",
    "# Display results\n",
    "print(f\"üìä DATA QUALITY SCORE: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    status = \"üü¢ EXCELLENT\"\n",
    "    recommendation = \"Data is ready for high-quality ML modeling\"\n",
    "elif quality_score >= 75:\n",
    "    status = \"üü° GOOD\" \n",
    "    recommendation = \"Data is suitable for ML with minor preprocessing\"\n",
    "elif quality_score >= 60:\n",
    "    status = \"üü† ACCEPTABLE\"\n",
    "    recommendation = \"Address major issues before ML modeling\"\n",
    "else:\n",
    "    status = \"üî¥ POOR\"\n",
    "    recommendation = \"Significant data cleaning required\"\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Recommendation: {recommendation}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"\\nISSUES IDENTIFIED:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ NO SIGNIFICANT QUALITY ISSUES DETECTED\")\n",
    "\n",
    "print(f\"\\nüéØ ML READINESS ASSESSMENT:\")\n",
    "ml_targets = ['pm2_5', 'pm10']\n",
    "ml_readiness = True\n",
    "\n",
    "for target in ml_targets:\n",
    "    if target in df.columns:\n",
    "        target_quality = 100\n",
    "        target_issues = []\n",
    "        \n",
    "        # Check for issues specific to ML targets\n",
    "        if target in zscore_outliers:\n",
    "            outlier_percent = zscore_outliers[target]['percent']\n",
    "            if outlier_percent > 5:\n",
    "                target_quality -= 10\n",
    "                target_issues.append(f\"High outlier rate ({outlier_percent:.1f}%)\")\n",
    "        \n",
    "        zero_count = (df[target] == 0.0).sum()\n",
    "        if zero_count > len(df) * 0.1:  # More than 10% zeros\n",
    "            target_quality -= 15\n",
    "            target_issues.append(f\"Many zero values ({zero_count} records)\")\n",
    "        \n",
    "        if target_issues:\n",
    "            print(f\"  {target.upper()}: {target_quality}/100 - {', '.join(target_issues)}\")\n",
    "            if target_quality < 70:\n",
    "                ml_readiness = False\n",
    "        else:\n",
    "            print(f\"  {target.upper()}: ‚úÖ Ready for ML modeling\")\n",
    "\n",
    "if ml_readiness:\n",
    "    print(f\"\\n‚úÖ DATASET IS READY FOR ML MODEL TRAINING\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  CONSIDER ADDITIONAL PREPROCESSING FOR OPTIMAL ML PERFORMANCE\")\n",
    "\n",
    "print(f\"\\n[Next Steps: Feature engineering validation, train/test splitting, model selection]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation checks\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA VALIDATION CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for negative values in air quality parameters (shouldn't be negative)\n",
    "negative_check_cols = ['pm2_5', 'pm10', 'co', 'no2', 'so2', 'o3', 'us_aqi', 'pm2_5_aqi', 'pm10_aqi']\n",
    "negative_issues = {}\n",
    "\n",
    "for col in negative_check_cols:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            negative_issues[col] = negative_count\n",
    "\n",
    "if negative_issues:\n",
    "    print(\"‚ö†Ô∏è  NEGATIVE VALUES FOUND (unexpected):\")\n",
    "    for col, count in negative_issues.items():\n",
    "        print(f\"  {col}: {count} negative values\")\n",
    "else:\n",
    "    print(\"‚úì No unexpected negative values in air quality parameters\")\n",
    "\n",
    "# Check for infinite values\n",
    "infinite_issues = {}\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        infinite_count = np.isinf(df[col]).sum()\n",
    "        if infinite_count > 0:\n",
    "            infinite_issues[col] = infinite_count\n",
    "\n",
    "if infinite_issues:\n",
    "    print(\"\\n‚ö†Ô∏è  INFINITE VALUES FOUND:\")\n",
    "    for col, count in infinite_issues.items():\n",
    "        print(f\"  {col}: {count} infinite values\")\n",
    "else:\n",
    "    print(\"\\n‚úì No infinite values found\")\n",
    "\n",
    "# Check timestamp continuity\n",
    "print(f\"\\nTIMESTAMP ANALYSIS:\")\n",
    "if 'time' in df.columns:\n",
    "    df_sorted = df.sort_values('time')\n",
    "    time_diffs = df_sorted['time'].diff().dropna()\n",
    "    \n",
    "    print(f\"  Earliest record: {df['time'].min()}\")\n",
    "    print(f\"  Latest record: {df['time'].max()}\")\n",
    "    print(f\"  Total duration: {(df['time'].max() - df['time'].min()).days} days\")\n",
    "    print(f\"  Most common interval: {time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"  Records per day average: {len(df) / max(1, (df['time'].max() - df['time'].min()).days):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
